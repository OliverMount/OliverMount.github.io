[
  {
    "objectID": "Research/Research.html",
    "href": "Research/Research.html",
    "title": "Research",
    "section": "",
    "text": "Krylov Subspace-based Channel Estimation\n\n\n\n\n\n\n\n\n\n\n\n2010-01-01\n\n\nOliver\n\n\n\n\n\n\n\n\n\n\n\n\nWavelet Shift Keying\n\n\n\n\n\n\n\n\n\n\n\n2005-08-16\n\n\nOliver\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "Publications/Publications.html",
    "href": "Publications/Publications.html",
    "title": "List of publications",
    "section": "",
    "text": "This page contains materials related to projects",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "Simulations/Neuron/Neuron.html",
    "href": "Simulations/Neuron/Neuron.html",
    "title": "NEURON simulator",
    "section": "",
    "text": "NEURON simulator excels at simulating complex, multi-compartmental models with detailed biophysics. The NEURON simulator is widely used in the simulation of detailed neural mechanisms and networks of neurons in the neuroscience literature. The original programming language used to code the NEURON is Higher Order Calculator (HOC). NEURON may also be programmed in Python, which we will explore more in this series of writings.",
    "crumbs": [
      "Neuron",
      "Overview"
    ]
  },
  {
    "objectID": "Simulations/Neuron/Fundamentals.html",
    "href": "Simulations/Neuron/Fundamentals.html",
    "title": "Fundamentals",
    "section": "",
    "text": "A few fundamentals would help the beginners aiming to use the NEURON simulator via python interface.\n\n\nParmeters: L - L is the length of the entire section in microns),\nnseg - (L/nseg)\ndiam -\nRa (Axial resistivity in ohm-cm -\nconnectivity -\nSection vs segment",
    "crumbs": [
      "Neuron",
      "Fundamentals"
    ]
  },
  {
    "objectID": "Simulations/Python/Python.html",
    "href": "Simulations/Python/Python.html",
    "title": "Python codes",
    "section": "",
    "text": "Python codes",
    "crumbs": [
      "Python",
      "Python codes"
    ]
  },
  {
    "objectID": "Tutorials/EstimationTheory/CramerRaoBound.html",
    "href": "Tutorials/EstimationTheory/CramerRaoBound.html",
    "title": "Cramer-Rao Lower Bound (CRLB)",
    "section": "",
    "text": "Cramer-Rao Lower Bound (CRLB)\nThe Cramer-Rao Lower Bound (CRLB) is a fundamental concept in estimation theory. It provides a theoretical lower bound on the variance of any unbiased estimator of a deterministic parameter. This bound is derived using the Fisher information and is widely used to assess the efficiency of estimators.\nThe Bayesian Cramér-Rao Lower Bound (BCRLB) is an extension of the classical Cramér-Rao Lower Bound (CRLB) to Bayesian estimation problems. Unlike the classical CRLB, which applies to deterministic parameters, the BCRLB is used when the parameter being estimated is treated as a random variable with a known prior distribution.",
    "crumbs": [
      "Tutorials",
      "Estimation Theory",
      "Estimation Theory"
    ]
  },
  {
    "objectID": "Tutorials/Tutorials.html",
    "href": "Tutorials/Tutorials.html",
    "title": "List of tutorials here",
    "section": "",
    "text": "List of tutorials here",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "Tutorials/RandomProcess/MA.html",
    "href": "Tutorials/RandomProcess/MA.html",
    "title": "Moving Average (MA) process",
    "section": "",
    "text": "Moving Average (MA) process",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "MA"
    ]
  },
  {
    "objectID": "Tutorials/RandomProcess/ARMA.html",
    "href": "Tutorials/RandomProcess/ARMA.html",
    "title": "Auto-regressive moving average (ARMA) processes {#sec-auto-regressive-moving-average-(arma-processes}",
    "section": "",
    "text": "Auto-regressive moving average (ARMA) processes {#sec-auto-regressive-moving-average-(arma-processes}\nLet x(n) and v(n) are stationary random processes. If they are related by the linear constant coefficient differential equations\n\nx(n) + \\sum_{l=1}^{p} a(l) x(n-l)  = \\sum_{l=0}^q b(l)v(n-l)\n\nthen x(n) is called an ARMA(p,q) process.\n\nAutocorrelation\nOn both sides, multiplying by x^*(n-k) and taking the expectation \\mathbb{E} leads to\n\n\\begin{split}\n\\mathbb{E}[x(n)x^*(n-k)] + \\sum_{l=1}^{p} a(l) \\mathbb{E}[x(n-l) x^*(n-k)] & = \\sum_{l=0}^q b(l)\\mathbb{E}[v(n-l)x^*(n-k)] \\\\\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l) & = \\sum_{l=0}^q b(l)r_{vx}(k-l)\n\\end{split}\n\\tag{1}\nIn the present form, Equation 1 is not useful. It would be nice to relate the cross-correlation term r_{vx}(k) on the RHS of Equation 1 to the autocorrelation term r_x(k) by invoking the linear filter theory (that is convolution). Since\n\nx(n) = \\sum_{m={-\\infty}}^{\\infty} v(m) h^*(n-m)\n\nwhere h(n) is the impulse response of LTI system.\n\n\\begin{split}\nr_{vx}(k) &= \\mathbb{E}[v(n)x^*(n-k)]  \\\\\n& =  \\sum_{m=-\\infty}^{\\infty} \\mathbb{E}[v(n)v^*(m)  h^*(n-m-k)] \\\\\n  & = \\sigma_v^2 h^*(-k)\n\\end{split}\n\nThe above simplification is due to the assumption that v(n) is white noise. Now,\n\nr_{vx}(k-l) =  \\sigma_v^2 h^*(l-k)  \n\\tag{2}\nSubstituting Equation 2 in to Equation 1\n\n\\begin{split}\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l) & = \\sum_{l=0}^q b(l)r_{vx}(k-l) \\\\\n& = \\sigma_v^2\\sum_{l=0}^q b(l)  h^*(l-k) \\\\\n& = \\sigma_v^2\\sum_{l=0}^{q-k} b(l+k)  h^*(l)\n\\end{split}\n\nThe last step is due to the assumption of causal h(n).\nThe equation\n\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l)  = \\sigma_v^2\\sum_{l=0}^{q-k} b(l+k)  h^*(l)\n\\tag{3}\nis called Yule-Walker equation. Let the RHS be\n  \nC_q(k)  = \\sigma_v^2\\sum_{l=0}^{q-k} b(l+k)  h^*(l)  \n\n\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l) =\\left\\{ \\begin{array}{ c l }\n     C_q(k) & k  \\leq q \\\\\n    0                 &   k &gt;q\n  \\end{array}\\right.\n\nThe above YW equations can be written (for k=0,\\cdots,p+q) as\n\n\\begin{split}\nr_x(0) +   a(1) r_x(-1)  + a(2) r_x(-2) + \\cdots + a(p) r_x(-p) &= C_q(0)\\\\\nr_x(1) +   a(1) r_x(0)  + a(2) r_x(-1) + \\cdots + a(p) r_x(-(p-1)) &= C_q(1)\\\\\n\\vdots ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\\\\nr_x(q) +   a(1) r_x(q-1)  + a(2) r_x(q-2) + \\cdots + a(p) r_x(-(p-q)) &= C_q(q)\\\\\n---------------------------\\\\\nr_x(q+1) +   a(1) r_x(q)  + a(2) r_x(q-1) + \\cdots + a(p) r_x(-(p-(q+1)) &= 0\\\\\nr_x(q+2) +   a(1) r_x(q+1)  + a(2) r_x(q) + \\cdots + a(p) r_x(-(p-(q+2))) &= 0\\\\\n\\vdots ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\\\\nr_x(q+p) +   a(1) r_x(q+p-1)  + a(2) r_x(q-1) + \\cdots + a(p) r_x(q) &= 0\\\\\n\\end{split}\n\nIn matrix form,\n\n\\begin{bmatrix}\nr_x(0) &     r_x(-1)  &   r_x(-2) & \\cdots &  r_x(-p)  \\\\\nr_x(1) &     r_x(0)  &    r_x(-1) &  \\cdots &  r_x(-(p-1)) \\\\  \n\\vdots &     \\ddots  &    \\ddots &  \\vdots &  \\vdots  \\\\   \nr_x(q) &    r_x(q-1)  &  r_x(q-2) & \\cdots & r_x(-(p-q)) \\\\\n---- &     ----   &    ----  &  ----  & ----   \\\\   \nr_x(q+1) &    r_x(q)  &  r_x(q-1) & \\cdots &   r_x(-(p-(q+1)) \\\\\nr_x(q+2) &  r_x(q+1)  & r_x(q) & \\cdots &   r_x(-(p-(q+2)))  \\\\\n\\vdots &     \\ddots  &    \\ddots &  \\vdots &  \\vdots  \\\\   \nr_x(q+p) &    r_x(q+p-1)  & r_x(q-1) & \\cdots &  r_x(q)  \n\\end{bmatrix}\n\\begin{bmatrix}  1 \\\\ a(1) \\\\ a(2)  \\\\ \\vdots \\\\ a(p) \\\\\n\\end{bmatrix}\n= \\sigma_v^2 \\begin{bmatrix}  C_q(0) \\\\ C_q(1) \\\\ C_q(2)  \\\\ \\vdots \\\\ C_q(q)\\\\ ---- \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0\n\\end{bmatrix}\n\nThe YW equations can be used\n\nTo find the (AR) filter coefficients a(i) given autocorrelation values.\nRecurrence relationship for autocorrelation sequence for extrapolation.\nYW equations can be used to extrapolate the\n\n\n\nRecurrence for autocorrelation sequence for extrapolation\nFrom\n\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l) =\\left\\{ \\begin{array}{ c l }\n     C_q(k) & k  \\leq q \\\\\n    0                 &   k &gt;q\n  \\end{array}\\right.\n\nfor k&gt;q,\n\nr_x(k) =- \\sum_{l=1}^{p} a(l) r_x(k-l)   \n\nIn the case of ARMA(1,1) process where p=q=1, we have for k&gt;1\n\nr_x(k) =-  a(1) r_x(k-1)   \n\nIf r_x(0) and r_x(1) are known, then the r_x(k), \\forall k&gt;1 can be obtained from the above recurrence.\nIn the case of ARMA(3,1) process where p=3,q=1, we have for k&gt;1\n\nr_x(k) =-  a(1) r_x(k-1)   -  a(2) r_x(k-2) -  a(1) r_x(k-3)\n\nIf r_x(0),r_x(1) are known, then the r_x(k), \\forall k&gt;1 can be obtained from the above recurrence.\nIn the case of ARMA(1,3) process where p=1,q=3, we have for k&gt;3\n\nr_x(k) =-  a(1) r_x(k-1)     \n\nIf r_x(0),r_x(1),r_x(2),r_x(3) are known, then the r_x(k), \\forall k&gt;3 can be obtained from the above recurrence.\nAlso, if p \\geq q and if r_x(0), \\cdots, r_x(p-1) are known, the for any k \\geq p, one can obtain \nr_x(k) =- \\sum_{l=1}^{p} a(l) r_x(k-l)",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "ARMA"
    ]
  },
  {
    "objectID": "Tutorials/RandomProcess/RandomProcessIntro.html",
    "href": "Tutorials/RandomProcess/RandomProcessIntro.html",
    "title": "Random processes",
    "section": "",
    "text": "Random processes\nA list of random processes we discuss in this tutorial are\n\nARMA\nAR\nMA\nHarmonic processes\n\nExcept the harmonic processes, the random processes are obtained by filtering white noise via linear time-invariant filter with rational transfer function given by\n\nH(z) =\\frac{B(z)}{A(z)} =  \\frac{\\sum_{k=0}^qb(k)z^{-k}}{1+\\sum_{k=1}^pa(k)z^{-k}}",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "Random Processes"
    ]
  },
  {
    "objectID": "Tutorials/RandomProcess/AR.html",
    "href": "Tutorials/RandomProcess/AR.html",
    "title": "Auto-regressive (AR) processes",
    "section": "",
    "text": "Auto-regressive (AR) processes\nThe YW equation for AR(p) process can be obtained by setting q=0 in YW equation for ARMA(p,q) that results in\n\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l)  = \\sigma_v^2  b(0)  h^*(0) \\delta(k)~~;~~ k\\geq0\n\nFor a stable, causal filter with rational transfer function, we have (using intial value of z-transform, that is, h(0) = \\lim_{z-&gt;\\infty} H(z) results in h^*(0)=b(0), then, the RHS becomes \\sigma_v^2 |b(0)|^2.\nIn the matrix form, this would be for k=0,1,\\cdots,p\n\n\\begin{bmatrix}\nr_x(0) &     r_x(-1)  &   r_x(-2) & \\cdots &  r_x(-p)  \\\\\nr_x(1) &     r_x(0)  &    r_x(-1) &  \\cdots &  r_x(-(p-1)) \\\\  \n\\vdots &     \\ddots  &    \\ddots &  \\vdots &  \\vdots  \\\\   \nr_x(p) &    r_x(p-1)  &  r_x(p-2) & \\cdots & r_x(0)  \n\\end{bmatrix}\n\\begin{bmatrix}  1 \\\\ a(1) \\\\ a(2)  \\\\ \\vdots \\\\ a(p) \\\\\n\\end{bmatrix}\n= \\sigma_v^2 |b(0)|^2  \\begin{bmatrix}  1 \\\\ 0 \\\\ 0  \\\\ \\vdots \\\\ 0  \n\\end{bmatrix}\n\nThe filter coefficients are linear in autocorrelation and thence it would be easy to find them give autocorrelation sequence.\nConsider for example AR(1) process (p=1),\n\n\\begin{bmatrix}\nr_x(0) &     r_x(-1)   \\\\\nr_x(1) &     r_x(0)   \n\\end{bmatrix}\n\\begin{bmatrix}  1 \\\\ a(1)   \n\\end{bmatrix}\n= \\sigma_v^2 |b(0)|^2\\begin{bmatrix}  1 \\\\ 0  \n\\end{bmatrix}\n\nSince, r_x(k)=r_x(-k), we have\n\n\\begin{split}\nr_x(0) +     r_x(1)   & =\\sigma_v^2 |b(0)|^2 \\\\\nr_x(1) +     a(1) r_x(0) &= 0  \n\\end{split}\n\nthat leads to",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "AR"
    ]
  },
  {
    "objectID": "Tutorials/EstimationTheory/PointEstimation.html",
    "href": "Tutorials/EstimationTheory/PointEstimation.html",
    "title": "Point Estimation",
    "section": "",
    "text": "Point Estimation",
    "crumbs": [
      "Tutorials",
      "Estimation Theory",
      "Estimation Theory"
    ]
  },
  {
    "objectID": "Tutorials/EstimationTheory/EstimationTheoryIntro.html",
    "href": "Tutorials/EstimationTheory/EstimationTheoryIntro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nGiven a set of observations (scalar or vector) \\bm{x}_1, x_2,\\cdots , x_N, find the parameter \\theta (scalar or vector) hidden in the observations.",
    "crumbs": [
      "Tutorials",
      "Estimation Theory",
      "Estimation Theory"
    ]
  },
  {
    "objectID": "Simulations/Brian2/Brian2.html",
    "href": "Simulations/Brian2/Brian2.html",
    "title": "Brian2",
    "section": "",
    "text": "Brian2\nBrian2 allows scientists to simulate spiking neural network models using simple and concise high-level descriptions. This makes it more accessible to researchers who may not have extensive programming experience.",
    "crumbs": [
      "Brian2",
      "Brian2"
    ]
  },
  {
    "objectID": "Simulations/Neuron/Introduction.html",
    "href": "Simulations/Neuron/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Recently, NEURON simulator has python interface, with excellent beginners tutorials.\n\nBall and stick 1: Basic cell\nLoad the necessary modules\n\nfrom neuron import h\nfrom neuron.units import ms, mV, µm \n\nimport matplotlib.pyplot as plt   # For plotting using matplotlib\n\n  \nh.load_file(\"stdrun.hoc\") #load the standard run library to give us high-level simulation control functions (e.g. running a simulation for a given period of time):\n\n1.0\n\n\nDefine one cell with soma and dendrite (with necessary parameters, morphology) using python class\n\nclass BallAndStick:\n    def __init__(self, gid):\n        self._gid = gid\n        self._setup_morphology()\n        self._setup_biophysics()\n\n    def _setup_morphology(self):\n        self.soma = h.Section(name=\"soma\", cell=self)\n        self.dend = h.Section(name=\"dend\", cell=self)\n        self.dend.connect(self.soma)\n        self.all = self.soma.wholetree()\n        self.soma.L = self.soma.diam = 12.6157 * µm\n        self.dend.L = 200 * µm\n        self.dend.diam = 1 * µm\n\n    def _setup_biophysics(self):\n        for sec in self.all:\n            sec.Ra = 100  # Axial resistance in Ohm * cm\n            sec.cm = 1  # Membrane capacitance in micro Farads / cm^2\n        self.soma.insert(\"hh\")\n        for seg in self.soma:\n            seg.hh.gnabar = 0.12  # Sodium conductance in S/cm2\n            seg.hh.gkbar = 0.036  # Potassium conductance in S/cm2\n            seg.hh.gl = 0.0003  # Leak conductance in S/cm2\n            seg.hh.el = -54.3 * mV  # Reversal potential \n        self.dend.insert(\"pas\")  # # Insert passive current in the dendrite  \n        for seg in self.dend:   \n            seg.pas.g = 0.001  # Passive conductance in S/cm2        \n            seg.pas.e = -65 * mV  # Leak reversal potential             \n\n    def __repr__(self):\n        return \"BallAndStick[{}]\".format(self._gid)\n\n\nmy_cell = BallAndStick(0)\n\n\n# Make sure soma is hh and dendtrite is passive membrane\nfor sec in h.allsec():\n    print(\"%s: %s\" % (sec, \", \".join(sec.psection()[\"density_mechs\"].keys())))\n\nBallAndStick[0].soma: hh\nBallAndStick[0].dend: pas\n\n\n\nStimulus\n\nstim = h.IClamp(my_cell.dend(1))  # at the origin of the dentrite\nstim.delay = 5 #ms\nstim.dur = 1 #ms\nstim.amp = 0.1  # nA\n\n\n\nRecording soma voltage\n\nsoma_v = h.Vector().record(my_cell.soma(0.5)._ref_v)\nt = h.Vector().record(h._ref_t)\n\n\n\nInitialization\n\nh.finitialize(-65 * mV) # initialize membrane potential \n\nh.continuerun(25 * ms) # run until time 25 ms:\n\n0.0\n\n\n\nplt.figure()\nplt.plot(t, soma_v)\nplt.xlabel(\"t (ms)\")\nplt.ylabel(\"v (mV)\")\nplt.show()",
    "crumbs": [
      "Neuron",
      "Introduction"
    ]
  },
  {
    "objectID": "Simulations/R/R.html",
    "href": "Simulations/R/R.html",
    "title": "R",
    "section": "",
    "text": "R\nA few R packages",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I am a computational neuroscience enthusiast with signal processing expertise, currently studying about brain. A passionate explorer of computational neuroscience methods, my diverse interests span from analyzing neural signals (spikes/fMRI/EEG/DTI) to developing cutting-edge dynamic models for neuroscientific applications.\nWhile acknowledging the value of established methodologies in neuroscience, I am driven by an insatiable curiosity to push beyond the boundaries of current knowledge. My ambition is to forge new pathways of discovery, developing innovative techniques (rather than recycling techniques) and paradigms that will revolutionize our understanding of the brain.\nIf you would like to collaborate with my explorations, please contact me."
  },
  {
    "objectID": "index.html#current-research-interests",
    "href": "index.html#current-research-interests",
    "title": "Welcome!",
    "section": "Current Research Interests",
    "text": "Current Research Interests\n\nComputational Neuroscience\nSignal Processing\nStatistics and Probability"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Welcome!",
    "section": "Education",
    "text": "Education\nPhD in Signal Processing\nMasters in Communication Engineering\nBachelors in Electronics and Communication Engineering"
  },
  {
    "objectID": "Simulations/Neuron/Fundamentals.html#section",
    "href": "Simulations/Neuron/Fundamentals.html#section",
    "title": "Fundamentals",
    "section": "",
    "text": "Parmeters: L - L is the length of the entire section in microns),\nnseg - (L/nseg)\ndiam -\nRa (Axial resistivity in ohm-cm -\nconnectivity -\nSection vs segment",
    "crumbs": [
      "Neuron",
      "Fundamentals"
    ]
  },
  {
    "objectID": "Simulations/Neuron/Neuron.html#installation-of-neuron-with-gpu-support-coreneuron",
    "href": "Simulations/Neuron/Neuron.html#installation-of-neuron-with-gpu-support-coreneuron",
    "title": "NEURON simulator",
    "section": "Installation of NEURON with GPU support (CoreNEURON)",
    "text": "Installation of NEURON with GPU support (CoreNEURON)\n\n1. Clone the latest version of NEURON\ngit clone https://github.com/neuronsimulator/nrn\ncd nrn\n\n\n2. Build\nmkdir build\ncd build\n\n\n3. Load the modules\nIn normal Linux systems, the modules are in the path /usr/share/modules/modulefiles. If the directory does not exists, then install using\n\nsudo apt-get install environment-modules\n\n# and configure the ~/.bashrc with \nsource /etc/profile.d/modules.sh\n\nLoad the following modules\nmodule load  openmpi python cmake nvidia-hpc-sdk/25.1 cuda  \nIf they are not loading make sure the following files are available in /usr/share/modules/modulefiles\n\nopenmpi\npython\ncmake\nnvidia-hpc-sdk/25.1\ncuda\n\nFor example, the content of the file in nvidia-hpc-sdk/25.1 should read as\n#%Module1.0 \n\nproc ModulesHelp { } { puts stderr \"NVIDIA HPC SDK 25.1\" } \nmodule-whatis \"NVIDIA HPC SDK 25.1\"\n\nset HPC_SDK /opt/nvidia/hpc_sdk/Linux_x86_64/25.1 \nprepend-path PATH $HPC_SDK/compilers/bin \nprepend-path LD_LIBRARY_PATH $HPC_SDK/compilers/lib\n\n#%Module1.0 proc ModulesHelp { } { puts stderr “NVIDIA HPC SDK 25.1” } module-whatis “NVIDIA HPC SDK 25.1”\nset HPC_SDK /opt/nvidia/hpc_sdk/Linux_x86_64/25.1 prepend-path PATH $HPC_SDK/compilers/bin prepend-path LD_LIBRARY_PATH $HPC_SDK/compilers/lib\n\n\n4. Compile\nFollowing compilation is for GPU architecture of 61\ncmake .. \\\n      -DNRN_ENABLE_CORENEURON=ON \\\n      -DCORENRN_ENABLE_GPU=ON \\\n      -DNRN_ENABLE_INTERVIEWS=OFF \\\n      -DNRN_ENABLE_RX3D=OFF \\\n      -DCMAKE_INSTALL_PREFIX=$HOME/install \\\n      -DCMAKE_C_COMPILER=nvc \\\n      -DCMAKE_CXX_COMPILER=nvc++ \\\n      -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu \\\n      -DCMAKE_CUDA_ARCHITECTURES=\"61\" \\  # need to know this\n      -DCMAKE_EXE_LINKER_FLAGS: \"-cuda -gpu=cuda12.6,lineinfo,cc61 -acc\" # not necessary\n      -DCMAKE_CXX_FLAGS=\"-O3 -g\" \\\n      -DCMAKE_C_FLAGS=\"-O3 -g\" \\\n      -DCMAKE_BUILD_TYPE=Custom\nCheck the CUDA_ARCHITECTURES via (nvidia-smi; assuming it is installed)\nnvidia-smi --query-gpu=compute_cap --format=csv\n\n\n7. Install (make)\n\nmake -j \nmake install\n\n\n8. Export Python PATH variables to ~/.bashrc\n  \nexport PATH=$HOME/install/bin:$PATH  # assuming $HOME/install is the cmake installtion directory (see 5-th line of cmake in Step 4.)\n\nexport PYTHONPATH=$HOME/install/lib/python:$PYTHONPATH\n\n\n9. Ringtest\nBest way to check if the installation is working properly is by running the ring test as in\ngit clone https://github.com/nrnhines/ringtest.git\ncd ringtest\n\nnrnivmodl -coreneuron mod\n\n# in any NEURON code add the following lines for CoreNEURON support (python files only)\n\nh.cvode.cache_efficient(1)\nif use_coreneuron:\n    from neuron import coreneuron\n    coreneuron.enable = True\n    coreneuron.gpu = coreneuron_gpu\n    \n    \n#run the three performance test \n\n# NEURON CPU Run\nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64\n\n# CoreNEURON CPU Run\nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron\n\n# CoreNEURON GPU Run\nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron -gpu",
    "crumbs": [
      "Neuron",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "Welcome!",
    "section": "Work Experience",
    "text": "Work Experience"
  },
  {
    "objectID": "Tutorials/RandomProcess/FilteringofRP.html",
    "href": "Tutorials/RandomProcess/FilteringofRP.html",
    "title": "Filtering of random process",
    "section": "",
    "text": "Filtering of random process",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "Filtering of randomprocess"
    ]
  },
  {
    "objectID": "Simulations/Neuron/Neuron.html#batch-files-for-running-in-cpu",
    "href": "Simulations/Neuron/Neuron.html#batch-files-for-running-in-cpu",
    "title": "NEURON simulator",
    "section": "Batch files for running in cpu",
    "text": "Batch files for running in cpu\n#!/bin/bash\n#SBATCH --job-name=rt_cpu       # Job name\n#SBATCH --output=rt_cpu.out     # Standard output and error log\n#SBATCH --error=rt_cpu.err      # Error log file                                                                                                     \n#SBATCH --ntasks=1                # Number of MPI processes\n#SBATCH --time=00:20:00           # Time limit (HH:MM:SS)\n#SBATCH --partition=olaf_c_core       # Partition name (adjust as needed)\n\nmodule purge\nmodule load gcc/12.2.0 openmpi/4.1.1-Rocky python/.3.12.3\n\nmodule load /opt/ibs_lib/modulefiles/libraries/.cuda/25.1\nmodule load /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1\n\n# Run the MPI program\n\n#mpiexec -n 2 ./x86_64/special -mpi -python ringtest.py -tstop 100\n#mpiexec -n 2 ./x86_64/special -mpi -python ringtest.py -tstop 100 -coreneuron\n\n# NEURON CPU Run ( in partition olaf_c_core ) \n#mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64\n\n# CoreNEURON CPU Run ( in partition olaf_c_core ) \nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron",
    "crumbs": [
      "Neuron",
      "Overview"
    ]
  },
  {
    "objectID": "Simulations/Neuron/Neuron.html#batch-files-for-running-in-gpu",
    "href": "Simulations/Neuron/Neuron.html#batch-files-for-running-in-gpu",
    "title": "NEURON simulator",
    "section": "Batch files for running in GPU",
    "text": "Batch files for running in GPU\n#!/bin/bash\n#SBATCH --job-name=ringtest_gpu\n#SBATCH --output=ringtest_gpu.out\n#SBATCH --error=ringtest_gpu.err\n#SBATCH --partition=AIP\n#SBATCH --gpus-per-node=1\n#SBATCH --ntasks=1\n#SBATCH --time=01:00:00\n\n# Load necessary modules (adjust as needed for your system)\nmodule purge\nmodule load gcc/12.2.0 openmpi/4.1.1-Rocky python/.3.12.3\n\nmodule load /opt/ibs_lib/modulefiles/libraries/.cuda/25.1\nmodule load /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1\n\n# Run the CoreNEURON simulation\nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron -gpu",
    "crumbs": [
      "Neuron",
      "Overview"
    ]
  },
  {
    "objectID": "Simulations/Neuron/Neuron.html#batch-files-for-running-in-multi-node-gpu",
    "href": "Simulations/Neuron/Neuron.html#batch-files-for-running-in-multi-node-gpu",
    "title": "NEURON simulator",
    "section": "Batch files for running in multi-node GPU",
    "text": "Batch files for running in multi-node GPU\n#!/bin/bash\n#SBATCH --job-name=ringtest_multi_gpu\n#SBATCH --output=ringtest_multi_gpu_%j.out\n#SBATCH --error=ringtest_multi_gpu_%j.err\n#SBATCH --partition=AIP\n#SBATCH --nodes=2               # Number of nodes\n#SBATCH --gpus-per-node=2       # GPUs per node (adjust based on node GPU capacity)\n#SBATCH --ntasks-per-node=2     # MPI tasks per node (matches GPUs-per-node)\n#SBATCH --time=01:00:00\n#SBATCH --exclusive             # Request exclusive node access\n\n# Load modules (confirm paths match your cluster's setup)\nmodule purge\nmodule load gcc/12.2.0 \\\n            openmpi/4.1.1-Rocky \\\n            python/3.12.3 \\\n            /opt/ibs_lib/modulefiles/libraries/cuda/25.1 \\\n            /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1\n\n# Ensure CUDA and MPI are visible\nexport CUDA_VISIBLE_DEVICES=0,1  # Map GPUs per node (adjust if needed)\nexport OMP_NUM_THREADS=1         # Disable threading unless required\n\n# Run with MPI across nodes\nmpiexec --bind-to none --map-by node \\\n    -n $((SLURM_NNODES * SLURM_NTASKS_PER_NODE)) \\\n    ./x86_64/special -mpi -python ringtest.py \\\n    -tstop 10 -nring 128 -ncell 128 -branch 32 64 \\\n    -coreneuron -gpu",
    "crumbs": [
      "Neuron",
      "Overview"
    ]
  },
  {
    "objectID": "Research/WSK.html",
    "href": "Research/WSK.html",
    "title": "Wavelet Shift Keying",
    "section": "",
    "text": "Wavelet-based schemes enhance digital communication spectral efficiency through two main methods: pulse shaping and wavelet shift keying (WSK) modulation. In pulse shaping, orthonormal wavelets and their dyadic expansions serve as baseband pulses, enabling single-sideband transmission for higher data rates (1.12 b/s/Hz) compared to raised-cosine systems (0.83 b/s/Hz) while satisfying Nyquist criteria. The wavelet approach also provides coding gains without bandwidth penalties by using optimized waveforms. For modulation, WSK encodes data streams as scaled versions of mother wavelets, improving spectral efficiency as user numbers increase while maintaining consistent power efficiency. Both methods leverage wavelets’ time-frequency localization and orthogonality to outperform traditional communication techniques in bandwidth-constrained scenarios"
  },
  {
    "objectID": "Research/Krylov.html",
    "href": "Research/Krylov.html",
    "title": "Krylov Subspace-based Channel Estimation",
    "section": "",
    "text": "Channel estimation based on Kylov subspace approach."
  }
]