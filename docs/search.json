[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I am a computational neuroscience enthusiast with signal processing expertise, currently studying about brain. A passionate explorer of computational neuroscience methods, my diverse interests span from analyzing neural signals (spikes/fMRI/EEG/DTI) to developing cutting-edge dynamic models for neuroscientific applications.\nWhile acknowledging the value of established methodologies in neuroscience, I am driven by an insatiable curiosity to push beyond the boundaries of current knowledge. My ambition is to forge new pathways of discovery, developing innovative techniques (rather than recycling techniques) and paradigms that will revolutionize our understanding of the brain.\nIf you would like to collaborate with my explorations, please contact me."
  },
  {
    "objectID": "index.html#current-research-interests",
    "href": "index.html#current-research-interests",
    "title": "Welcome!",
    "section": "Current Research Interests",
    "text": "Current Research Interests\n\nComputational Neuroscience\nSignal Processing\nStatistics and Probability"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Welcome!",
    "section": "Education",
    "text": "Education\nPhD in Signal Processing\nMasters in Communication Engineering\nBachelors in Electronics and Communication Engineering"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "List of publications",
    "section": "",
    "text": "James, Oliver and Kim, Y. J. (2025), “Gain-controlled reconfiguration of long-range coupling explains visibility-dependent spatiotemporal neural coding dynamics’’, Submitted, Sep. 2025 https://www.biorxiv.org/content/10.1101/2025.09.24.677967v1\nJea Kwon, Sunpil Kim, Junsung Woo, Keiko Tanaka-Yamamoto, James, Oliver, Erik De Schutter, Sungho Hong, and C. Justin Lee, (2025), “Cerebellar tonic inhibition orchestrates the maturation of information processing and motor coordination’’, Revised to Experimental and Molecular Medicine, Sep. 2025 https://www.biorxiv.org/content/10.1101/2024.05.30.596563v1",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "publications/index.html#biorcolormaroonchiiv",
    "href": "publications/index.html#biorcolormaroonchiiv",
    "title": "List of publications",
    "section": "",
    "text": "James, Oliver and Kim, Y. J. (2025), “Gain-controlled reconfiguration of long-range coupling explains visibility-dependent spatiotemporal neural coding dynamics’’, Submitted, Sep. 2025 https://www.biorxiv.org/content/10.1101/2025.09.24.677967v1\nJea Kwon, Sunpil Kim, Junsung Woo, Keiko Tanaka-Yamamoto, James, Oliver, Erik De Schutter, Sungho Hong, and C. Justin Lee, (2025), “Cerebellar tonic inhibition orchestrates the maturation of information processing and motor coordination’’, Revised to Experimental and Molecular Medicine, Sep. 2025 https://www.biorxiv.org/content/10.1101/2024.05.30.596563v1",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "publications/index.html#arcolormaroonchiiv",
    "href": "publications/index.html#arcolormaroonchiiv",
    "title": "List of publications",
    "section": "ar\\color{Maroon}{\\chi}iv",
    "text": "ar\\color{Maroon}{\\chi}iv\nJames, Oliver., & Lee, H.-N. (2025). “Concise probability distributions of eigenvalues of real-valued Wishart matrices”, Retrieved from http://arxiv.org/abs/1402.6757\nJames, Oliver., & Lee, H.-N. (2025). “Restricted isometry random variables: probability distributions, RIC prediction and phase transition analysis for Gaussian encoders”, Retrieved from https://arxiv.org/abs/1410.1956",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "publications/index.html#under-revision",
    "href": "publications/index.html#under-revision",
    "title": "List of publications",
    "section": "Under Revision",
    "text": "Under Revision\nLee, Y. B.,James, Oliver., Lee, D. Y., and Kim, Y. J. (2025). “Hierarchical summary statistics encoding across primary visual and posterior parietal cortices’’, Adanced Science.",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "publications/index.html#in-preparation",
    "href": "publications/index.html#in-preparation",
    "title": "List of publications",
    "section": "In Preparation",
    "text": "In Preparation\nMiguel Sanchez-Valpuesta, James, Oliver, Joonyeol Lee and Gunsoo Kim, “Effects of locomotion on the population frequency representation in the mouse auditory midbrain’’, Writing stage.\nOliver James and Sung Ho Hong, (2025). “Container-based framework for simulating biological cerebellar neural networks”, Writing stage.",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "publications/index.html#sec-journals",
    "href": "publications/index.html#sec-journals",
    "title": "List of publications",
    "section": "Published",
    "text": "Published\nBae, J., Jung, K., James, Oliver., Suzuki, S., & Kim, Y. J. (2025). “Frontal engagement in perceptual integration under low subjective visibility”, NeuroImage, 305, 120984.\nDo, J., James, Oliver., & Kim, Y.-J. (2024). “Choice-dependent delta-band neural trajectory during semantic category decision making in the human brain”, Iscience, 27(7).\nDo, J., Eo, K. Y., James, Oliver., Lee, J., & Kim, Y.-J. (2022). “The representational dynamics of sequential perceptual averaging”, Journal of Neuroscience, 42(6), 1141–1153.\nAdalarasu, K., Jagannath, M., & James, Oliver. (2020). “Assessment of techniques for teaching school children with autism”, IRBM, 41(2), 88–93.\nJames, Oliver., Park, H., & Kim, S.-G. (2019). “Impact of sampling rate on statistical significance for single subject fMRI connectivity analysis”, Human Brain Mapping, 40(11), 3321–3337.\nPark, B.-Y., Shim, W. M., James, Oliver., & Park, H. (2019). “Possible links between the lag structure in visual cortex and visual streams using fMRI”, Scientific Reports, 9(1), 4283.\nEo, K., James, Oliver., Son, S., Kang, M.-S., Chong, S. C., & Kim, Y.-J. (2018). “Representational dynamics of ensemble average of simultaneously presented objects”, Journal of Vision, 18(10), 80–80.\nKwon, J., Kim, M., Park, H., Kang, B.-M., Jo, Y., Kim, J.-H., Oliver James… Others. (2017). “Label-free nanoscale optical metrology on myelinated axons in vivo”, Nature Communications, 8(1), 1832.\nLee, Y., Park, B.-Y., James, Oliver., Kim, S.-G., & Park, H. (2017). “Autism spectrum disorder related functional connectivity changes in the language network in children, adolescents and adults”, Frontiers in Human Neuroscience, 11, 418.\nJames, Oliver. (2016). “Revisiting the RIP of Real and Complex Gaussian Sensing Matrices Through RIV Framework”, Wireless Personal Communications, 87(10), 513–526.\nJames, Oliver. (2015). “Stability Analysis of LASSO and Dantzig Selector via Constrained Minimal Singular Value of Gaussian Sensing Matrices”, International Journal of Computer Applications, 975, 8887.\nJames, Oliver, Lee, W.-B., Park, S.-J., & Lee, H.-N. (2013b). “Evaluation of Resolution Improvement Ability of a DSP Technique for Filter-Array-Based Spectrometers”, The Journal of Korean Institute of Communications and Information Sciences, 38(6), 497–502.\nJames, Oliver, WoongBi, L., Heung-No, & Lee. (2013). “Filters with random transmittance for improving resolution in filter-array-based spectrometers”, Optics Express, 21(4), 3969–3989.\nJames, Oliver., Lee, W., Park, S., & Lee, H.-N. (2012). Improving resolution of miniature spectrometers by exploiting sparse nature of signals. Optics Express, 20(3), 2613–2625.\nOliver, James, Aravind, R., & Prabhu, K. M. M. (2012). Improved least squares channel estimation for orthogonal frequency division multiplexing. IET Signal Processing, 6(1), 45–53.\nJames, Oliver, Aravind, R., & Prabhu, K. M. M. (2010). A Krylov subspace based low-rank channel estimation in OFDM systems. Signal Processing, 90(6), 1861–1872.\nJames, Oliver., Aravind, R., & Prabhu, K. M. M. (2008). Sparse channel estimation in OFDM systems by threshold-based pruning. Electronics Letters, 44(13), 830–832.",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "publications/index.html#book-chapters",
    "href": "publications/index.html#book-chapters",
    "title": "List of publications",
    "section": "Book Chapters",
    "text": "Book Chapters\nUmapathy, K., Muthukumaran, D., Chandramohan, S., Sivakumar, M., James, O. (2023). Low Power Methodologies for FPGA—An Overview. In: Sharma, D.K., Sharma, R., Jeon, G., Polkowski, Z. (eds) Low Power Architectures for IoT Applications. Springer Tracts in Electrical and Electronics Engineering. Springer, Singapore. https://doi.org/10.1007/978-981-99-0639-0_4",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "publications/index.html#conferences",
    "href": "publications/index.html#conferences",
    "title": "List of publications",
    "section": "Conferences",
    "text": "Conferences\nJames, Oliver and Yee Joon Kim, (2023) On the representation of objective and subjective visible/invisible stimuli in humans via EEG-based report and no- report, Cognitive Neuroscience Symposium, CNS, SFA, USA\nJisub Bae, James Oliver, and Yee Joon Kim, (2023) The influence of subjective visibility on illusory contour perception: An EEG study, Cognitive Neuroscience Symposium, CNS, SFA, USA\nJames, Oliver., Lee, W.-B., & Lee, H.-N. (2013). Random Transmittance Based Filter Array Spectrometers: Sparse Spectrum Recovery And Resolution Improvement. Global Conference on Signal and Information Processing (GlobalSIP), 2013 IEEE, 615–615. IEEE.\nJames,Oliver ., Lee, W.-B., Park, S., & Lee, H.-N. (2013a). A new signal processing technique for improving resolution of spectrometers. 한국통신학회 학술대회논문집, 62–63.\nLee, W.-B., James,Oliver, Kim, S.-C., & Lee, H.-N. (2013). Random optical scatter filters for spectrometers: Implementation and Estimation. Imaging Systems and Applications, JTu4A-33. Optica Publishing Group.\nJames, Oliver, S. P., WoongBi Lee, & Lee, H.-N. (2013). Evaluation of resolution improvement capability of a DSP technique for filter-array-based spectrometers. Korean Information and Communication Society Journal, (6), 497–502.\nJames, Oliver, & Lee, H.-N. (2011). A realistic distributed compressive sensing framework for multiple wireless sensor networks. Proc. 4th Signal Process. with Adapt. Sparse Struct. Repr, 105.\nJames, Oliver., Aravind, R., & Prabhu, K. M. M. (2010). Pilot sequence design for improving OFDM channel estimation in the presence of CFO. 2010 International Conference on Signal Processing and Communications (SPCOM), 1–5. IEEE.\nJames, Oliver., Aravind, R., & Prabhu, K. M. M. (2009). Improved channel estimation in OFDM systems in the presence of CFO. 2009 15th Asia-Pacific Conference on Communications, 398–401. IEEE.\nJames, Oliver, Kumari, R. S. S., & Sadasivam, V. (2005). Wavelets for improving spectral efficiency in a digital communication system. Sixth International Conference on Computational Intelligence and Multimedia Applications (ICCIMA’05), 198–203. IEEE.",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "publications/index.html#international-patents",
    "href": "publications/index.html#international-patents",
    "title": "List of publications",
    "section": "International Patents",
    "text": "International Patents\nHeung-No Lee, Oliver James, Sang-Jun Park, and Woong-Bi Lee, Method and Apparatus for Processing Optical Signal Of Spectrometer Using Sparse Nature of Signals, US patent number: 13/711,930, date: Dec. 12, 2012. (Contribution 30%).",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "publications/index.html#korean-patents",
    "href": "publications/index.html#korean-patents",
    "title": "List of publications",
    "section": "Korean Patents",
    "text": "Korean Patents\nHeung-No Lee, Oliver James, and Woong-Bi Lee, Apparatus for Improving Spectral resolution using Random Transmittance in Optical Spectrometers, No. 10-2012-0112453, registered date: Oct. 10, 2012. (Contribution 30%).\nHeung-No Lee, Oliver James, Sang-Jun Park, and Woong-Bi Lee, Method and Apparatus for Processing Optical Signal Of Spectrometer Using Sparse Nature of Signals, No. 10-2012-0079171, registered date: July. 20, 2012. (Contribution 30%).",
    "crumbs": [
      "Publications",
      "List of publications"
    ]
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#linear-variants",
    "href": "gleanings/Kalman/Kalman.html#linear-variants",
    "title": "Kalman Filter",
    "section": "Linear Variants",
    "text": "Linear Variants\n\n1. Standard/Vanilla Kalman Filter\nLinear systems with Gaussian noise - Reference: Kalman, R. E. (1960). “A New Approach to Linear Filtering and Prediction Problems.” Journal of Basic Engineering, 82(1), 35-45.\n\n\n2. Steady-State Kalman Filter\nUses constant gain after convergence - Reference: Anderson, B. D., & Moore, J. B. (1979). Optimal Filtering. Prentice-Hall.\n\n\n3. Information Filter\nInverse covariance form of KF, useful for sensor fusion - Reference: Maybeck, P. S. (1979). Stochastic Models, Estimation, and Control, Vol. 1. Academic Press.\n\n\n4. Square Root Kalman Filter\nNumerically stable using Cholesky decomposition - Reference: Bierman, G. J. (1977). Factorization Methods for Discrete Sequential Estimation. Academic Press.\n\n\n5. UD Filter\nU-D factorization for improved numerical stability - Reference: Bierman, G. J. (1977). “Factorization methods for discrete sequential estimation.” Mathematics in Science and Engineering, Vol. 128.\n\n\n6. Joseph Form Kalman Filter\nMore numerically stable covariance update - Reference: Bucy, R. S., & Joseph, P. D. (1968). Filtering for Stochastic Processes with Applications to Guidance. Wiley-Interscience."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#nonlinear-system-variants",
    "href": "gleanings/Kalman/Kalman.html#nonlinear-system-variants",
    "title": "Kalman Filter",
    "section": "Nonlinear System Variants",
    "text": "Nonlinear System Variants\n\n7. Extended Kalman Filter (EKF)\nFirst-order linearization via Jacobian matrices - Reference: Jazwinski, A. H. (1970). Stochastic Processes and Filtering Theory. Academic Press.\n\n\n8. Second-Order Extended Kalman Filter\nUses second-order Taylor expansion with Hessian matrices - Reference: Athans, M., Wishner, R. P., & Bertolini, A. (1968). “Suboptimal state estimation for continuous-time nonlinear systems.” IEEE Transactions on Automatic Control, 13(5), 504-514.\n\n\n9. Iterated Extended Kalman Filter (IEKF)\nIterative refinement of EKF measurement update - Reference: Bell, B. M., & Cathey, F. W. (1993). “The iterated Kalman filter update as a Gauss-Newton method.” IEEE Transactions on Automatic Control, 38(2), 294-297.\n\n\n10. Unscented Kalman Filter (UKF)\nSigma point method using unscented transformation - Reference: Julier, S. J., & Uhlmann, J. K. (1997). “New extension of the Kalman filter to nonlinear systems.” Signal Processing, Sensor Fusion, and Target Recognition VI, SPIE, 3068, 182-193. - Reference: Wan, E. A., & Van Der Merwe, R. (2000). “The unscented Kalman filter for nonlinear estimation.” IEEE Adaptive Systems for Signal Processing Symposium, 153-158.\n\n\n11. Central Difference Kalman Filter (CDKF)\nNumerical differentiation approach using Stirling’s interpolation - Reference: Nørgaard, M., Poulsen, N. K., & Ravn, O. (2000). “New developments in state estimation for nonlinear systems.” Automatica, 36(11), 1627-1638.\n\n\n12. Divided Difference Filter (DDF)\nUses Stirling’s interpolation formula - Reference: Nørgaard, M., Poulsen, N. K., & Ravn, O. (2000). “New developments in state estimation for nonlinear systems.” Automatica, 36(11), 1627-1638.\n\n\n13. Cubature Kalman Filter (CKF)\nSpherical-radial cubature rules for numerical integration - Reference: Arasaratnam, I., & Haykin, S. (2009). “Cubature Kalman filters.” IEEE Transactions on Automatic Control, 54(6), 1254-1269.\n\n\n14. Gauss-Hermite Kalman Filter\nGauss-Hermite quadrature integration - Reference: Ito, K., & Xiong, K. (2000). “Gaussian filters for nonlinear filtering problems.” IEEE Transactions on Automatic Control, 45(5), 910-927.\n\n\n15. Quadrature Kalman Filter (QKF)\nGeneral quadrature-based approach - Reference: Ito, K., & Xiong, K. (2000). “Gaussian filters for nonlinear filtering problems.” IEEE Transactions on Automatic Control, 45(5), 910-927."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#particlesequential-monte-carlo-variants",
    "href": "gleanings/Kalman/Kalman.html#particlesequential-monte-carlo-variants",
    "title": "Kalman Filter",
    "section": "Particle/Sequential Monte Carlo Variants",
    "text": "Particle/Sequential Monte Carlo Variants\n\n16. Particle Filter (PF)\nMonte Carlo sampling method for non-Gaussian, nonlinear systems - Reference: Gordon, N. J., Salmond, D. J., & Smith, A. F. (1993). “Novel approach to nonlinear/non-Gaussian Bayesian state estimation.” IEE Proceedings F, 140(2), 107-113.\n\n\n17. Bootstrap Filter\nBasic particle filter implementation - Reference: Gordon, N. J., Salmond, D. J., & Smith, A. F. (1993). “Novel approach to nonlinear/non-Gaussian Bayesian state estimation.” IEE Proceedings F, 140(2), 107-113.\n\n\n18. Auxiliary Particle Filter\nUses auxiliary variables for improved importance sampling - Reference: Pitt, M. K., & Shephard, N. (1999). “Filtering via simulation: Auxiliary particle filters.” Journal of the American Statistical Association, 94(446), 590-599.\n\n\n19. Regularized Particle Filter\nAdds kernel density estimation to avoid sample impoverishment - Reference: Musso, C., Oudjane, N., & Le Gland, F. (2001). “Improving regularised particle filters.” Sequential Monte Carlo Methods in Practice, Springer, 247-271.\n\n\n20. Rao-Blackwellized Particle Filter\nMarginalizes linear subspaces analytically - Reference: Doucet, A., de Freitas, N., Murphy, K., & Russell, S. (2000). “Rao-Blackwellised particle filtering for dynamic Bayesian networks.” Uncertainty in Artificial Intelligence, 176-183."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#adaptive-variants",
    "href": "gleanings/Kalman/Kalman.html#adaptive-variants",
    "title": "Kalman Filter",
    "section": "Adaptive Variants",
    "text": "Adaptive Variants\n\n21. Adaptive Kalman Filter\nAdapts noise statistics online - Reference: Mehra, R. K. (1970). “On the identification of variances and adaptive Kalman filtering.” IEEE Transactions on Automatic Control, 15(2), 175-184.\n\n\n22. Fading Memory Filter\nExponential forgetting factor for time-varying systems - Reference: Jazwinski, A. H. (1970). Stochastic Processes and Filtering Theory. Academic Press.\n\n\n23. Multiple Model Adaptive Estimation (MMAE)\nBank of parallel filters for different models - Reference: Magill, D. T. (1965). “Optimal adaptive estimation of sampled stochastic processes.” IEEE Transactions on Automatic Control, 10(4), 434-439.\n\n\n24. Interacting Multiple Model (IMM)\nSwitching between models with Markov transitions - Reference: Blom, H. A., & Bar-Shalom, Y. (1988). “The interacting multiple model algorithm for systems with Markovian switching coefficients.” IEEE Transactions on Automatic Control, 33(8), 780-783.\n\n\n25. Generalized Pseudo-Bayesian (GPB)\nMultiple hypothesis tracking - Reference: Ackerson, G. A., & Fu, K. S. (1970). “On state estimation in switching environments.” IEEE Transactions on Automatic Control, 15(1), 10-17."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#robust-variants",
    "href": "gleanings/Kalman/Kalman.html#robust-variants",
    "title": "Kalman Filter",
    "section": "Robust Variants",
    "text": "Robust Variants\n\n26. H-infinity Filter\nMinimax robust filtering for worst-case disturbances - Reference: Hassibi, B., Sayed, A. H., & Kailath, T. (1999). Indefinite-Quadratic Estimation and Control. SIAM.\n\n\n27. Robust Kalman Filter\nOutlier rejection mechanisms - Reference: Fratello, F., & Lertora, L. (2006). “Robust Kalman filtering.” IEEE International Conference on Multisensor Fusion and Integration, 155-160.\n\n\n28. Maximum Correntropy Kalman Filter\nRobust to non-Gaussian noise using correntropy - Reference: Chen, B., Liu, X., Zhao, H., & Principe, J. C. (2017). “Maximum correntropy Kalman filter.” Automatica, 76, 70-77.\n\n\n29. Huber-based Kalman Filter\nM-estimation approach for robustness - Reference: Gandhi, M. A., & Mili, L. (2010). “Robust Kalman filter based on a generalized maximum-likelihood-type estimator.” IEEE Transactions on Signal Processing, 58(5), 2509-2520.\n\n\n30. Variational Bayesian Kalman Filter\nHandles uncertain noise statistics via variational inference - Reference: Sarkka, S., & Nummenmaa, A. (2009). “Recursive noise adaptive Kalman filtering by variational Bayesian approximations.” IEEE Transactions on Automatic Control, 54(3), 596-600."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#decentralizeddistributed-variants",
    "href": "gleanings/Kalman/Kalman.html#decentralizeddistributed-variants",
    "title": "Kalman Filter",
    "section": "Decentralized/Distributed Variants",
    "text": "Decentralized/Distributed Variants\n\n31. Federated Kalman Filter\nMultiple local filters with master filter for sensor fusion - Reference: Carlson, N. A. (1988). “Federated square root filter for decentralized parallel processes.” IEEE Transactions on Aerospace and Electronic Systems, 26(3), 517-525.\n\n\n32. Distributed Kalman Filter\nConsensus-based estimation for multi-agent systems - Reference: Olfati-Saber, R. (2007). “Distributed Kalman filtering for sensor networks.” IEEE Conference on Decision and Control, 5492-5498.\n\n\n33. Covariance Intersection Filter\nConservative fusion without cross-correlation information - Reference: Julier, S. J., & Uhlmann, J. K. (1997). “A non-divergent estimation algorithm in the presence of unknown correlations.” American Control Conference, 2369-2373.\n\n\n34. Consensus Kalman Filter\nMulti-agent coordination via consensus protocols - Reference: Olfati-Saber, R., & Sandell, N. R. (2008). “Distributed tracking for mobile sensor networks with information-driven mobility.” American Control Conference, 4606-4612."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#constrained-variants",
    "href": "gleanings/Kalman/Kalman.html#constrained-variants",
    "title": "Kalman Filter",
    "section": "Constrained Variants",
    "text": "Constrained Variants\n\n35. Constrained Kalman Filter\nEnforces state constraints (equality/inequality) - Reference: Simon, D., & Chia, T. L. (2002). “Kalman filtering with state equality constraints.” IEEE Transactions on Aerospace and Electronic Systems, 38(1), 128-136.\n\n\n36. Projected Kalman Filter\nProjects estimates onto constraint surface - Reference: Simon, D. (2010). “Kalman filtering with state constraints: a survey of linear and nonlinear algorithms.” IET Control Theory & Applications, 4(8), 1303-1318.\n\n\n37. Perfect Measurement Filter\nZero-uncertainty pseudo-measurements for constraints - Reference: Simon, D., & Chia, T. L. (2002). “Kalman filtering with state equality constraints.” IEEE Transactions on Aerospace and Electronic Systems, 38(1), 128-136."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#smoother-variants-backward-processing",
    "href": "gleanings/Kalman/Kalman.html#smoother-variants-backward-processing",
    "title": "Kalman Filter",
    "section": "Smoother Variants (Backward Processing)",
    "text": "Smoother Variants (Backward Processing)\n\n38. Rauch-Tung-Striebel (RTS) Smoother\nFixed-interval smoothing (forward-backward pass) - Reference: Rauch, H. E., Tung, F., & Striebel, C. T. (1965). “Maximum likelihood estimates of linear dynamic systems.” AIAA Journal, 3(8), 1445-1450.\n\n\n39. Two-Filter Smoother\nForward and backward filters combined - Reference: Fraser, D., & Potter, J. (1969). “The optimum linear smoother as a combination of two optimum linear filters.” IEEE Transactions on Automatic Control, 14(4), 387-390.\n\n\n40. Fixed-Lag Smoother\nDelayed state estimation with fixed delay - Reference: Meditch, J. S. (1973). “A survey of data smoothing for linear and nonlinear dynamic systems.” Automatica, 9(2), 151-162.\n\n\n41. Fixed-Point Smoother\nEstimates single past state with all available data - Reference: Meditch, J. S. (1973). “A survey of data smoothing for linear and nonlinear dynamic systems.” Automatica, 9(2), 151-162."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#special-purpose-variants",
    "href": "gleanings/Kalman/Kalman.html#special-purpose-variants",
    "title": "Kalman Filter",
    "section": "Special Purpose Variants",
    "text": "Special Purpose Variants\n\n42. Ensemble Kalman Filter (EnKF)\nMonte Carlo ensemble approach for high-dimensional systems - Reference: Evensen, G. (1994). “Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte Carlo methods.” Journal of Geophysical Research, 99(C5), 10143-10162.\n\n\n43. Error-State Kalman Filter (ESKF)\nEstimates error states rather than full states (common in IMU/SLAM) - Reference: Solà, J. (2017). “Quaternion kinematics for the error-state Kalman filter.” arXiv preprint arXiv:1711.02508.\n\n\n44. Invariant Extended Kalman Filter (InEKF)\nExploits system symmetries and Lie group structure - Reference: Barrau, A., & Bonnabel, S. (2017). “The invariant extended Kalman filter as a stable observer.” IEEE Transactions on Automatic Control, 62(4), 1797-1812.\n\n\n45. Sigma-Point Kalman Filter (SPKF)\nGeneral term for UKF/CKF family using sigma points - Reference: Van der Merwe, R., & Wan, E. A. (2001). “The square-root unscented Kalman filter for state and parameter-estimation.” IEEE International Conference on Acoustics, Speech, and Signal Processing, 3461-3464.\n\n\n46. Alpha-Beta Filter\nSimplified fixed-gain tracker (g-h filter) - Reference: Benedict, T. R., & Bordner, G. W. (1962). “Synthesis of an optimal set of radar track-while-scan smoothing equations.” IRE Transactions on Automatic Control, 7(4), 27-32.\n\n\n47. Alpha-Beta-Gamma Filter\nTracks position, velocity, and acceleration - Reference: Kalata, P. R. (1984). “The tracking index: A generalized parameter for α-β and α-β-γ target trackers.” IEEE Transactions on Aerospace and Electronic Systems, AES-20(2), 174-182.\n\n\n48. Schmidt-Kalman Filter\nConsiders but doesn’t estimate nuisance parameters - Reference: Schmidt, S. F. (1966). “Application of state-space methods to navigation problems.” Advances in Control Systems, 3, 293-340.\n\n\n49. Dual Kalman Filter\nSimultaneous state and parameter estimation with two filters - Reference: Wan, E. A., & Nelson, A. T. (2001). “Dual extended Kalman filter methods.” Kalman Filtering and Neural Networks, Wiley, 123-173.\n\n\n50. Joint Kalman Filter\nAugmented state vector for parameter estimation - Reference: Nelson, A. T. (2000). “Nonlinear estimation and modeling of noisy time-series by dual Kalman filtering methods.” PhD dissertation, Oregon Graduate Institute."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#suboptimalsimplified-variants",
    "href": "gleanings/Kalman/Kalman.html#suboptimalsimplified-variants",
    "title": "Kalman Filter",
    "section": "Suboptimal/Simplified Variants",
    "text": "Suboptimal/Simplified Variants\n\n51. Decoupled Kalman Filter\nSeparates estimation problems for computational efficiency - Reference: Carlson, N. A. (1973). “Fast triangular formulation of the square root filter.” AIAA Journal, 11(9), 1259-1265.\n\n\n52. Reduced-Order Kalman Filter\nLower-dimensional approximation - Reference: Bernstein, D. S., & Hyland, D. C. (1985). “The optimal projection equations for reduced-order state estimation.” IEEE Transactions on Automatic Control, 30(6), 583-585.\n\n\n53. Fast Kalman Filter\nComputational efficiency optimizations using fast algorithms - Reference: Grewal, M. S., & Andrews, A. P. (2014). Kalman Filtering: Theory and Practice Using MATLAB, 4th ed. Wiley.\n\n\n54. Chandrasekhar Kalman Filter\nRecursive gain computation for time-invariant systems - Reference: Kailath, T., Sayed, A. H., & Hassibi, B. (2000). Linear Estimation. Prentice Hall."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#modern-machine-learning-hybrids",
    "href": "gleanings/Kalman/Kalman.html#modern-machine-learning-hybrids",
    "title": "Kalman Filter",
    "section": "Modern Machine Learning Hybrids",
    "text": "Modern Machine Learning Hybrids\n\n55. Kalman Filter Network (KFN)\nNeural network implementation of Kalman filtering - Reference: Haarnoja, T., Ajay, A., Levine, S., & Abbeel, P. (2016). “Backprop KF: Learning discriminative deterministic state estimators.” Advances in Neural Information Processing Systems, 4376-4384.\n\n\n56. Differentiable Kalman Filter\nLearnable parameters via backpropagation - Reference: Kloss, A., Schaal, S., & Bohg, J. (2021). “Combining learned and analytical models for predicting action effects.” International Journal of Robotics Research, 40(8-9), 1074-1095.\n\n\n57. Deep Kalman Filter\nDeep learning state transition and observation models - Reference: Krishnan, R. G., Shalit, U., & Sontag, D. (2017). “Structured inference networks for nonlinear state space models.” AAAI Conference on Artificial Intelligence.\n\n\n58. Variational Kalman Filter\nVariational inference approach with neural networks - Reference: Fraccaro, M., Kamronn, S., Paquet, U., & Winther, O. (2017). “A disentangled recognition and nonlinear dynamics model for unsupervised learning.” Advances in Neural Information Processing Systems, 3601-3610."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#application-specific-variants",
    "href": "gleanings/Kalman/Kalman.html#application-specific-variants",
    "title": "Kalman Filter",
    "section": "Application-Specific Variants",
    "text": "Application-Specific Variants\n\n59. GPS Kalman Filter\nSpecialized for GPS navigation and positioning - Reference: Brown, R. G., & Hwang, P. Y. (1997). Introduction to Random Signals and Applied Kalman Filtering, 3rd ed. Wiley.\n\n\n60. IMU Kalman Filter\nInertial measurement unit sensor fusion - Reference: Titterton, D. H., & Weston, J. L. (2004). Strapdown Inertial Navigation Technology, 2nd ed. IET.\n\n\n61. SLAM Kalman Filter (EKF-SLAM)\nSimultaneous localization and mapping - Reference: Dissanayake, M. G., Newman, P., Clark, S., Durrant-Whyte, H. F., & Csorba, M. (2001). “A solution to the simultaneous localization and map building (SLAM) problem.” IEEE Transactions on Robotics and Automation, 17(3), 229-241.\n\n\n62. Quaternion Kalman Filter\nAttitude estimation using quaternion representation - Reference: Crassidis, J. L., & Markley, F. L. (2003). “Unscented filtering for spacecraft attitude estimation.” Journal of Guidance, Control, and Dynamics, 26(4), 536-542.\n\n\n63. Multiplicative Extended Kalman Filter (MEKF)\nAttitude estimation with multiplicative error representation - Reference: Markley, F. L. (2003). “Attitude error representations for Kalman filtering.” Journal of Guidance, Control, and Dynamics, 26(2), 311-317."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#key-textbooks-and-survey-papers",
    "href": "gleanings/Kalman/Kalman.html#key-textbooks-and-survey-papers",
    "title": "Kalman Filter",
    "section": "Key Textbooks and Survey Papers",
    "text": "Key Textbooks and Survey Papers\n\nBar-Shalom, Y., Li, X. R., & Kirubarajan, T. (2001). Estimation with Applications to Tracking and Navigation. Wiley.\nGrewal, M. S., & Andrews, A. P. (2014). Kalman Filtering: Theory and Practice Using MATLAB, 4th ed. Wiley-IEEE Press.\nSimon, D. (2006). Optimal State Estimation: Kalman, H∞, and Nonlinear Approaches. Wiley.\nSärkkä, S. (2013). Bayesian Filtering and Smoothing. Cambridge University Press.\nArulampalam, M. S., Maskell, S., Gordon, N., & Clapp, T. (2002). “A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking.” IEEE Transactions on Signal Processing, 50(2), 174-188."
  },
  {
    "objectID": "gleanings/Kalman/Kalman.html#summary",
    "href": "gleanings/Kalman/Kalman.html#summary",
    "title": "Kalman Filter",
    "section": "Summary",
    "text": "Summary\nThis comprehensive list represents the major variants developed over six decades of Kalman filtering research, from Kalman’s original 1960 paper to modern machine learning hybrids. Each variant addresses specific challenges such as:\n\nNonlinearity: EKF, UKF, CKF, particle filters\nNon-Gaussian noise: Particle filters, robust variants\nComputational efficiency: Reduced-order filters, fast algorithms\nRobustness: H-infinity, maximum correntropy filters\nSpecialized applications: SLAM, attitude estimation, GPS navigation\nModern AI integration: Deep Kalman filters, differentiable filters\n\nThe Kalman filter remains one of the most successful algorithms in estimation theory, with applications spanning aerospace, robotics, autonomous vehicles, finance, signal processing, and countless other domains."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "List of tutorials here",
    "section": "",
    "text": "List of tutorials here\n\nSpecial Random Processes",
    "crumbs": [
      "Tutorials",
      "List of tutorials here"
    ]
  },
  {
    "objectID": "tutorials/EstimationTheory/index.html",
    "href": "tutorials/EstimationTheory/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThe central problem in estimation theory is that given a vector of observations \\bm{y} = [y_1, y_2,\\cdots , y_N] that depends on the parameter vector\\bm{\\theta} = [\\theta_1, \\theta_2,\\cdots, \\theta_N], determine \\bm{\\theta} using some function of \\bm{y}. Mathematically,\n\n\\bm{\\hat{\\theta}} = g(\\bm{y} )",
    "crumbs": [
      "Tutorials",
      "Estimation Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "tutorials/RandomProcess/AR.html",
    "href": "tutorials/RandomProcess/AR.html",
    "title": "Auto-regressive (AR) processes",
    "section": "",
    "text": "Auto-regressive (AR) processes\nThe YW equation for AR(p) process can be obtained by setting q=0 in YW equation for ARMA(p,q) that results in\n\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l)  = \\sigma_v^2  b(0)  h^*(0) \\delta(k)~~;~~ k\\geq0\n\nFor a stable, causal filter with rational transfer function, we have (using intial value of z-transform, that is, h(0) = \\lim_{z-&gt;\\infty} H(z) results in h^*(0)=b(0), then, the RHS becomes \\sigma_v^2 |b(0)|^2.\nIn the matrix form, this would be for k=0,1,\\cdots,p\n\n\\begin{bmatrix}\nr_x(0) &     r_x(-1)  &   r_x(-2) & \\cdots &  r_x(-p)  \\\\\nr_x(1) &     r_x(0)  &    r_x(-1) &  \\cdots &  r_x(-(p-1)) \\\\  \n\\vdots &     \\ddots  &    \\ddots &  \\vdots &  \\vdots  \\\\   \nr_x(p) &    r_x(p-1)  &  r_x(p-2) & \\cdots & r_x(0)  \n\\end{bmatrix}\n\\begin{bmatrix}  1 \\\\ a(1) \\\\ a(2)  \\\\ \\vdots \\\\ a(p) \\\\\n\\end{bmatrix}\n= \\sigma_v^2 |b(0)|^2  \\begin{bmatrix}  1 \\\\ 0 \\\\ 0  \\\\ \\vdots \\\\ 0  \n\\end{bmatrix}\n\nThe filter coefficients are linear in autocorrelation and thence it would be easy to find them give autocorrelation sequence.\nConsider for example AR(1) process (p=1),\n\n\\begin{bmatrix}\nr_x(0) &     r_x(-1)   \\\\\nr_x(1) &     r_x(0)   \n\\end{bmatrix}\n\\begin{bmatrix}  1 \\\\ a(1)   \n\\end{bmatrix}\n= \\sigma_v^2 |b(0)|^2\\begin{bmatrix}  1 \\\\ 0  \n\\end{bmatrix}\n\nSince, r_x(k)=r_x(-k), we have\n\n\\begin{split}\nr_x(0) +     r_x(1) a(1)  & =\\sigma_v^2 |b(0)|^2 \\\\\nr_x(1) +     a(1) r_x(0) &= 0  \n\\end{split}\n\nthat leads to\n\n     a(1)   = -\\frac{r_x(1)} {r_x(0)}   \n\nIn a similar vein, it is also possible to find the autocorrelation sequence given the filter coefficients a(i)s.",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "Auto-regressive (AR) processes"
    ]
  },
  {
    "objectID": "tutorials/RandomProcess/ARMA.html",
    "href": "tutorials/RandomProcess/ARMA.html",
    "title": "Auto-regressive moving average (ARMA) processes",
    "section": "",
    "text": "Auto-regressive moving average (ARMA) processes\nLet x(n) and v(n) are stationary random processes. If they are related by the linear constant coefficient differential equations\n\nx(n) + \\sum_{l=1}^{p} a(l) x(n-l)  = \\sum_{l=0}^q b(l)v(n-l)\n\nthen x(n) is called an ARMA(p,q) process.\n\nAutocorrelation\nOn both sides, multiplying by x^*(n-k) and taking the expectation \\mathbb{E} leads to\n\n\\begin{split}\n\\mathbb{E}[x(n)x^*(n-k)] + \\sum_{l=1}^{p} a(l) \\mathbb{E}[x(n-l) x^*(n-k)] & = \\sum_{l=0}^q b(l)\\mathbb{E}[v(n-l)x^*(n-k)] \\\\\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l) & = \\sum_{l=0}^q b(l)r_{vx}(k-l)\n\\end{split}\n\\tag{1}\nIn the present form, Equation 1 is not useful. It would be nice to relate the cross-correlation term r_{vx}(k) on the RHS of Equation 1 to the autocorrelation term r_x(k) by invoking the linear filter theory (that is convolution). Since\n\nx(n) = \\sum_{m={-\\infty}}^{\\infty} v(m) h^*(n-m)\n\nwhere h(n) is the impulse response of LTI system.\n\n\\begin{split}\nr_{vx}(k) &= \\mathbb{E}[v(n)x^*(n-k)]  \\\\\n& =  \\sum_{m=-\\infty}^{\\infty} \\mathbb{E}[v(n)v^*(m)  h^*(n-m-k)] \\\\\n  & = \\sigma_v^2 h^*(-k)\n\\end{split}\n\nThe above simplification is due to the assumption that v(n) is white noise. Now,\n\nr_{vx}(k-l) =  \\sigma_v^2 h^*(l-k)  \n\\tag{2}\nSubstituting Equation 2 in to Equation 1\n\n\\begin{split}\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l) & = \\sum_{l=0}^q b(l)r_{vx}(k-l) \\\\\n& = \\sigma_v^2\\sum_{l=0}^q b(l)  h^*(l-k) \\\\\n& = \\sigma_v^2\\sum_{l=0}^{q-k} b(l+k)  h^*(l)\n\\end{split}\n\nThe last step is due to the assumption of causal h(n).\nThe equation\n\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l)  = \\sigma_v^2\\sum_{l=0}^{q-k} b(l+k)  h^*(l)\n\\tag{3}\nis called Yule-Walker equation. Let the RHS be\n  \nC_q(k)  = \\sigma_v^2\\sum_{l=0}^{q-k} b(l+k)  h^*(l)  \n\n\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l) =\\left\\{ \\begin{array}{ c l }\n     C_q(k) & k  \\leq q \\\\\n    0                 &   k &gt;q\n  \\end{array}\\right.\n\nThe above YW equations can be written (for k=0,\\cdots,p+q) as\n\n\\begin{split}\nr_x(0) +   a(1) r_x(-1)  + a(2) r_x(-2) + \\cdots + a(p) r_x(-p) &= C_q(0)\\\\\nr_x(1) +   a(1) r_x(0)  + a(2) r_x(-1) + \\cdots + a(p) r_x(-(p-1)) &= C_q(1)\\\\\n\\vdots ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\\\\nr_x(q) +   a(1) r_x(q-1)  + a(2) r_x(q-2) + \\cdots + a(p) r_x(-(p-q)) &= C_q(q)\\\\\n---------------------------\\\\\nr_x(q+1) +   a(1) r_x(q)  + a(2) r_x(q-1) + \\cdots + a(p) r_x(-(p-(q+1)) &= 0\\\\\nr_x(q+2) +   a(1) r_x(q+1)  + a(2) r_x(q) + \\cdots + a(p) r_x(-(p-(q+2))) &= 0\\\\\n\\vdots ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\\\\nr_x(q+p) +   a(1) r_x(q+p-1)  + a(2) r_x(q-1) + \\cdots + a(p) r_x(q) &= 0\\\\\n\\end{split}\n\nIn matrix form,\n\n\\begin{bmatrix}\nr_x(0) &     r_x(-1)  &   r_x(-2) & \\cdots &  r_x(-p)  \\\\\nr_x(1) &     r_x(0)  &    r_x(-1) &  \\cdots &  r_x(-(p-1)) \\\\  \n\\vdots &     \\ddots  &    \\ddots &  \\vdots &  \\vdots  \\\\   \nr_x(q) &    r_x(q-1)  &  r_x(q-2) & \\cdots & r_x(-(p-q)) \\\\\n---- &     ----   &    ----  &  ----  & ----   \\\\   \nr_x(q+1) &    r_x(q)  &  r_x(q-1) & \\cdots &   r_x(-(p-(q+1)) \\\\\nr_x(q+2) &  r_x(q+1)  & r_x(q) & \\cdots &   r_x(-(p-(q+2)))  \\\\\n\\vdots &     \\ddots  &    \\ddots &  \\vdots &  \\vdots  \\\\   \nr_x(q+p) &    r_x(q+p-1)  & r_x(q-1) & \\cdots &  r_x(q)  \n\\end{bmatrix}\n\\begin{bmatrix}  1 \\\\ a(1) \\\\ a(2)  \\\\ \\vdots \\\\ a(p) \\\\\n\\end{bmatrix}\n= \\sigma_v^2 \\begin{bmatrix}  C_q(0) \\\\ C_q(1) \\\\ C_q(2)  \\\\ \\vdots \\\\ C_q(q)\\\\ ---- \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0\n\\end{bmatrix}\n\nThe YW equations can be used\n\nTo find the (AR) filter coefficients a(i) given autocorrelation values.\nRecurrence relationship for autocorrelation sequence for extrapolation.\nYW equations can be used to extrapolate the\n\n\n\nRecurrence for autocorrelation sequence for extrapolation\nFrom\n\nr_x(k) + \\sum_{l=1}^{p} a(l) r_x(k-l) =\\left\\{ \\begin{array}{ c l }\n     C_q(k) & k  \\leq q \\\\\n    0                 &   k &gt;q\n  \\end{array}\\right.\n\nfor k&gt;q,\n\nr_x(k) =- \\sum_{l=1}^{p} a(l) r_x(k-l)   \n\nIn the case of ARMA(1,1) process where p=q=1, we have for k&gt;1\n\nr_x(k) =-  a(1) r_x(k-1)   \n\nIf r_x(0) and r_x(1) are known, then the r_x(k), \\forall k&gt;1 can be obtained from the above recurrence.\nIn the case of ARMA(3,1) process where p=3,q=1, we have for k&gt;1\n\nr_x(k) =-  a(1) r_x(k-1)   -  a(2) r_x(k-2) -  a(1) r_x(k-3)\n\nIf r_x(0),r_x(1) are known, then the r_x(k), \\forall k&gt;1 can be obtained from the above recurrence.\nIn the case of ARMA(1,3) process where p=1,q=3, we have for k&gt;3\n\nr_x(k) =-  a(1) r_x(k-1)     \n\nIf r_x(0),r_x(1),r_x(2),r_x(3) are known, then the r_x(k), \\forall k&gt;3 can be obtained from the above recurrence.\nAlso, if p \\geq q and if r_x(0), \\cdots, r_x(p-1) are known, the for any k \\geq p, one can obtain \nr_x(k) =- \\sum_{l=1}^{p} a(l) r_x(k-l)",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "Auto-regressive moving average (ARMA) processes"
    ]
  },
  {
    "objectID": "tutorials/RandomProcess/index.html",
    "href": "tutorials/RandomProcess/index.html",
    "title": "Random processes",
    "section": "",
    "text": "Random processes\nA list of random processes we discuss in this tutorial are\n\nARMA\nAR\nMA\nHarmonic processes\n\nExcept the harmonic processes, the random processes are obtained by filtering white noise via linear time-invariant filter with rational transfer function given by\n\nH(z) =\\frac{B(z)}{A(z)} =  \\frac{\\sum_{k=0}^qb(k)z^{-k}}{1+\\sum_{k=1}^pa(k)z^{-k}}",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "Random processes"
    ]
  },
  {
    "objectID": "research/SparseChannel.html",
    "href": "research/SparseChannel.html",
    "title": "Sparse Channel Estimation",
    "section": "",
    "text": "A threshold-based procedure to estimate sparse channels in an orthogonal frequency division multiplexing (OFDM) system is proposed. An optimal threshold is derived by maximising the probability of correct detection between significant and zero-valued taps estimated by the least squares (LS) estimator. Improved LS estimates are obtained by pruning the LS estimates with the statistically derived threshold.",
    "crumbs": [
      "Research",
      "Sparse Channel Estimation"
    ]
  },
  {
    "objectID": "research/RIPofRealComplex.html",
    "href": "research/RIPofRealComplex.html",
    "title": "On the RIP of real and complex Gaussian sensing matrices via RIV framework in sparse signal recovery analysis",
    "section": "",
    "text": "In this paper, we aim to revisit the restricted isometry property (RIP) of real and complex Gaussian sensing matrices. We do this reconsideration via the recently introduced restricted isometry random variable (RIV) framework for the real Gaussian sensing matrices. We first generalize the RIV framework to the complex settings and illustrate that the restricted isometry constants (RICs) of complex Gaussian sensing matrices are smaller than their real-valued counterpart. The reasons behind the better RIC nature of complex sensing matrices over their real-valued counterpart is delineated. We also demonstrate via critical functions, upper bounds on the RICs, that complex Gaussian matrices with prescribed RICs exist for larger number of problem sizes than the real Gaussian matrices.",
    "crumbs": [
      "Research",
      "On the RIP of real and complex Gaussian sensing matrices via RIV framework in sparse signal recovery analysis"
    ]
  },
  {
    "objectID": "research/ImpactOfSamplingRate.html",
    "href": "research/ImpactOfSamplingRate.html",
    "title": "Impact of sampling rate on fMRI connectivity analysis",
    "section": "",
    "text": "A typical time series in functional magnetic resonance imaging (fMRI) exhibits autocorrelation, that is, the samples of the time series are dependent. In addition, temporal filtering, one of the crucial steps in preprocessing of functional magnetic resonance images, induces its own autocorrelation. While performing connectivity analysis in fMRI, the impact of the autocorrelation is largely ignored. Recently, autocorrelation has been addressed by variance correction approaches, which are sensitive to the sampling rate. In this article, we aim to investigate the impact of the sampling rate on the variance correction approaches. Toward this end, we first derived a generalized expression for the variance of the sample Pearson correlation coefficient (SPCC) in terms of the sampling rate and the filter cutoff frequency, in addition to the autocorrelation and cross-covariance functions of the time series. Through simulations, we illustrated the importance of the variance correction for a fixed sampling rate. Using the real resting state fMRI data sets, we demonstrated that the data sets with higher sampling rates were more prone to false positives, in agreement with the existing empirical reports. We further demonstrated with single subject results that for the data sets with higher sampling rates, the variance correction strategy restored the integrity of true connectivity.",
    "crumbs": [
      "Research",
      "Impact of sampling rate on fMRI connectivity analysis"
    ]
  },
  {
    "objectID": "research/WSK.html",
    "href": "research/WSK.html",
    "title": "Wavelet Shift Keying",
    "section": "",
    "text": "Wavelet-based schemes enhance digital communication spectral efficiency through two main methods: pulse shaping and wavelet shift keying (WSK) modulation. In pulse shaping, orthonormal wavelets and their dyadic expansions serve as baseband pulses, enabling single-sideband transmission for higher data rates (1.12 b/s/Hz) compared to raised-cosine systems (0.83 b/s/Hz) while satisfying Nyquist criteria. The wavelet approach also provides coding gains without bandwidth penalties by using optimized waveforms. For modulation, WSK encodes data streams as scaled versions of mother wavelets, improving spectral efficiency as user numbers increase while maintaining consistent power efficiency. Both methods leverage wavelets’ time-frequency localization and orthogonality to outperform traditional communication techniques in bandwidth-constrained scenarios",
    "crumbs": [
      "Research",
      "Wavelet Shift Keying"
    ]
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "blogs will be updated soon",
    "section": "",
    "text": "blogs will be updated soon",
    "crumbs": [
      "Blogs",
      "blogs will be updated soon"
    ]
  },
  {
    "objectID": "simulations/neuron/Fundamentals.html",
    "href": "simulations/neuron/Fundamentals.html",
    "title": "Fundamentals",
    "section": "",
    "text": "A few fundamentals would help the beginners aiming to use the NEURON simulator via python interface.\n\n\nParmeters: L - L is the length of the entire section in microns),\nnseg - (L/nseg)\ndiam -\nRa (Axial resistivity in ohm-cm -\nconnectivity -\nSection vs segment"
  },
  {
    "objectID": "simulations/neuron/Fundamentals.html#section",
    "href": "simulations/neuron/Fundamentals.html#section",
    "title": "Fundamentals",
    "section": "",
    "text": "Parmeters: L - L is the length of the entire section in microns),\nnseg - (L/nseg)\ndiam -\nRa (Axial resistivity in ohm-cm -\nconnectivity -\nSection vs segment"
  },
  {
    "objectID": "simulations/neuron/index.html",
    "href": "simulations/neuron/index.html",
    "title": "NEURON simulator",
    "section": "",
    "text": "NEURON simulator excels at simulating complex, multi-compartmental models with detailed biophysics. The NEURON simulator is widely used in the simulation of detailed neural mechanisms and networks of neurons in the neuroscience literature. The original programming language used to code the NEURON is Higher Order Calculator (HOC). NEURON may also be programmed in Python, which we will explore more in this series of writings.",
    "crumbs": [
      "Neuron",
      "NEURON simulator"
    ]
  },
  {
    "objectID": "simulations/neuron/index.html#installation-of-neuron-with-gpu-support-coreneuron",
    "href": "simulations/neuron/index.html#installation-of-neuron-with-gpu-support-coreneuron",
    "title": "NEURON simulator",
    "section": "Installation of NEURON with GPU support (CoreNEURON)",
    "text": "Installation of NEURON with GPU support (CoreNEURON)\n\n1. Clone the latest version of NEURON\ngit clone https://github.com/neuronsimulator/nrn\ncd nrn\n\n\n2. Build\nmkdir build\ncd build\n\n\n3. Load the modules\nIn normal Linux systems, the modules are in the path /usr/share/modules/modulefiles. If the directory does not exists, then install using\n\nsudo apt-get install environment-modules\n\n# and configure the ~/.bashrc with \nsource /etc/profile.d/modules.sh\n\nLoad the following modules\nmodule load  openmpi python cmake nvidia-hpc-sdk/25.1 cuda  \nIf they are not loading make sure the following files are available in /usr/share/modules/modulefiles\n\nopenmpi\npython\ncmake\nnvidia-hpc-sdk/25.3\ncuda\n\nFor example, the content of the file in nvidia-hpc-sdk/25.1 should read as\n#%Module1.0 \n\nproc ModulesHelp { } { puts stderr \"NVIDIA HPC SDK 25.1\" } \nmodule-whatis \"NVIDIA HPC SDK 25.1\"\n\nset HPC_SDK /opt/nvidia/hpc_sdk/Linux_x86_64/25.1 \nprepend-path PATH $HPC_SDK/compilers/bin \nprepend-path LD_LIBRARY_PATH $HPC_SDK/compilers/lib\n\n#%Module1.0 proc ModulesHelp { } { puts stderr “NVIDIA HPC SDK 25.1” } module-whatis “NVIDIA HPC SDK 25.1”\nset HPC_SDK /opt/nvidia/hpc_sdk/Linux_x86_64/25.1 prepend-path PATH $HPC_SDK/compilers/bin prepend-path LD_LIBRARY_PATH $HPC_SDK/compilers/lib\n\n\n4. Compile\nFollowing compilation is for GPU architecture of 61\ncmake .. \\\n      -DNRN_ENABLE_CORENEURON=ON \\\n      -DCORENRN_ENABLE_GPU=ON \\\n      -DNRN_ENABLE_INTERVIEWS=ON \\\n      -DNRN_ENABLE_RX3D=OFF \\\n      -DCMAKE_INSTALL_PREFIX=$HOME/install \\\n      -DCMAKE_C_COMPILER=nvc \\\n      -DCMAKE_CXX_COMPILER=nvc++ \\\n      -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu \\\n      -DCMAKE_CUDA_ARCHITECTURES=\"61\" \\  # need to know this\n      -DCMAKE_EXE_LINKER_FLAGS: \"-cuda -gpu=cuda12.6,lineinfo,cc61 -acc\" # not necessary\n      -DCMAKE_CXX_FLAGS=\"-O3 -g\" \\\n      -DCMAKE_C_FLAGS=\"-O3 -g\" \\\n      -DCMAKE_BUILD_TYPE=Custom\nCheck the CUDA_ARCHITECTURES via (nvidia-smi; assuming it is installed)\nnvidia-smi --query-gpu=compute_cap --format=csv\n\n# in the cluster HPC it can be \n\nsrun --partition=@@@@ --gres=gpu:1 nvidia-smi --query-gpu=compute_cap --format=csv\n\n# Make sure to replace the partition name @@@@ with the actual partition name\n\nThe above command works well only for the architectures above 70 (for example, starting from Tesla V100 (Volta).\nFor older architectures, the compute_cap fiedd is not available. In such as, first search for the available fields by\nnvidia-smi --help-query-gpu  \n\n# in the cluster HPC it can be\n\nsrun --partition=jepyc --gres=gpu:1 nvidia-smi --help-query-gpu\n# Make sure to replace the partition name @@@@ with the actual partition name\nGet the name GPU name in such case and map it to the GPU architecture or compute capability\n\nnvidia-smi --query-gpu=name --format=csv\n\nsrun --partition=jepyc --gres=gpu:1 nvidia-smi --query-gpu=name --format=cs\n\n\n7. Install (make)\n\nmake -j \nmake install\n\n\n8. Export Python PATH variables to ~/.bashrc\n  \nexport PATH=$HOME/install/bin:$PATH  # assuming $HOME/install is the cmake installtion directory (see 5-th line of cmake in Step 4.)\n\nexport PYTHONPATH=$HOME/install/lib/python:$PYTHONPATH\n\n\n9. Ringtest\nBest way to check if the installation is working properly is by running the ring test as in\ngit clone https://github.com/nrnhines/ringtest.git\ncd ringtest\n\nnrnivmodl -coreneuron mod\n\n# in any NEURON code add the following lines for CoreNEURON support (python files only)\n\nh.cvode.cache_efficient(1)\nif use_coreneuron:\n    from neuron import coreneuron\n    coreneuron.enable = True\n    coreneuron.gpu = coreneuron_gpu\n    \n    \n#run the three performance test \n\n# NEURON CPU Run\nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64\n\n# CoreNEURON CPU Run\nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron\n\n# CoreNEURON GPU Run\nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron -gpu",
    "crumbs": [
      "Neuron",
      "NEURON simulator"
    ]
  },
  {
    "objectID": "simulations/neuron/index.html#batch-files-for-running-in-cpu",
    "href": "simulations/neuron/index.html#batch-files-for-running-in-cpu",
    "title": "NEURON simulator",
    "section": "Batch files for running in cpu",
    "text": "Batch files for running in cpu\n#!/bin/bash\n#SBATCH --job-name=rt_cpu       # Job name\n#SBATCH --output=rt_cpu.out     # Standard output and error log\n#SBATCH --error=rt_cpu.err      # Error log file                                                                                                     \n#SBATCH --ntasks=1                # Number of MPI processes\n#SBATCH --time=00:20:00           # Time limit (HH:MM:SS)\n#SBATCH --partition=olaf_c_core       # Partition name (adjust as needed)\n\nmodule purge\nmodule load gcc/12.2.0 openmpi/4.1.1-Rocky python/.3.12.3\n\nmodule load /opt/ibs_lib/modulefiles/libraries/.cuda/25.1\nmodule load /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1\n\n# Run the MPI program\n\n#mpiexec -n 2 ./x86_64/special -mpi -python ringtest.py -tstop 100\n#mpiexec -n 2 ./x86_64/special -mpi -python ringtest.py -tstop 100 -coreneuron\n\n# NEURON CPU Run ( in partition olaf_c_core ) \n#mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64\n\n# CoreNEURON CPU Run ( in partition olaf_c_core ) \nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron",
    "crumbs": [
      "Neuron",
      "NEURON simulator"
    ]
  },
  {
    "objectID": "simulations/neuron/index.html#batch-files-for-running-in-gpu",
    "href": "simulations/neuron/index.html#batch-files-for-running-in-gpu",
    "title": "NEURON simulator",
    "section": "Batch files for running in GPU",
    "text": "Batch files for running in GPU\n#!/bin/bash\n#SBATCH --job-name=ringtest_gpu\n#SBATCH --output=ringtest_gpu.out\n#SBATCH --error=ringtest_gpu.err\n#SBATCH --partition=AIP\n#SBATCH --gpus-per-node=1\n#SBATCH --ntasks=1\n#SBATCH --time=01:00:00\n\n# Load necessary modules (adjust as needed for your system)\nmodule purge\nmodule load gcc/12.2.0 openmpi/4.1.1-Rocky python/.3.12.3\n\nmodule load /opt/ibs_lib/modulefiles/libraries/.cuda/25.1\nmodule load /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1\n\n# Run the CoreNEURON simulation\nmpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron -gpu",
    "crumbs": [
      "Neuron",
      "NEURON simulator"
    ]
  },
  {
    "objectID": "simulations/neuron/index.html#batch-files-for-running-in-multi-node-gpu",
    "href": "simulations/neuron/index.html#batch-files-for-running-in-multi-node-gpu",
    "title": "NEURON simulator",
    "section": "Batch files for running in multi-node GPU",
    "text": "Batch files for running in multi-node GPU\n#!/bin/bash\n#SBATCH --job-name=ringtest_multi_gpu\n#SBATCH --output=ringtest_multi_gpu_%j.out\n#SBATCH --error=ringtest_multi_gpu_%j.err\n#SBATCH --partition=AIP\n#SBATCH --nodes=2               # Number of nodes\n#SBATCH --gpus-per-node=2       # GPUs per node (adjust based on node GPU capacity)\n#SBATCH --ntasks-per-node=2     # MPI tasks per node (matches GPUs-per-node)\n#SBATCH --time=01:00:00\n#SBATCH --exclusive             # Request exclusive node access\n\n# Load modules (confirm paths match your cluster's setup)\nmodule purge\nmodule load gcc/12.2.0 \\\n            openmpi/4.1.1-Rocky \\\n            python/3.12.3 \\\n            /opt/ibs_lib/modulefiles/libraries/cuda/25.1 \\\n            /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1\n\n# Ensure CUDA and MPI are visible\nexport CUDA_VISIBLE_DEVICES=0,1  # Map GPUs per node (adjust if needed)\nexport OMP_NUM_THREADS=1         # Disable threading unless required\n\n# Run with MPI across nodes\nmpiexec --bind-to none --map-by node \\\n    -n $((SLURM_NNODES * SLURM_NTASKS_PER_NODE)) \\\n    ./x86_64/special -mpi -python ringtest.py \\\n    -tstop 10 -nring 128 -ncell 128 -branch 32 64 \\\n    -coreneuron -gpu",
    "crumbs": [
      "Neuron",
      "NEURON simulator"
    ]
  },
  {
    "objectID": "simulations/neuron/index.html#installation-of-deepdendrite",
    "href": "simulations/neuron/index.html#installation-of-deepdendrite",
    "title": "NEURON simulator",
    "section": "Installation of DeepDendrite",
    "text": "Installation of DeepDendrite\n\n1. Compilation of CoreNEURON first in the DeepDendrite way\n\n# Docker is here\n# https://deepdendrite.readthedocs.io/\n# docker run --gpus '\"device=0\"' -itd --user $(id -u):$(id -g) --volume $(pwd):/workdir deepdendrite_docker:1.0 \n# If you use the Docker version, please change the settings inside the Docker file such as ARG HPCSDK_VER=\"25.1\", ARG CUDA_VER=\"12.6\", ARG UBUNTU_VER=\"24.04\" according to your sister version\n# coredat folder is the data folder (gen_data.py for Fig5)\n# If it is generated already, you can simply run the run.py inside the Fig5 folder\n\n\n\n# Manual installation \n\nDIR=\"/home/olive/DeepDendrite\"\n\n# Check if the directory exists\nif [ -d \"$DIR\" ]; then \n    rm -rf \"$DIR\"\n    echo \"Directory removed: $DIR\"  \nfi\n    \necho \"Cloing in to : $DIR\" \ngit clone https://github.com/pkuzyc/DeepDendrite.git\ncd $DIR/src/nrn_modify \n \n#chmod +x configure\n\n\n# you may set the variables before this command as well\n\n# before configuration\n\n# make sure that the \n# AM_INIT_AUTOMAKE([foreign]) in configure.ac to AM_INIT_AUTOMAKE([1.16 foreign])\n# if your systerm automake version is 1.16\n# check it via automake --version\n\n\necho \"######################################################\"\necho \"Configure\"\necho \"######################################################\" \n  \n  \n# Locate the python library flags\n#python3-config --ldflags\n#python3-config --libs\n\n# the flags during configure should be inaccordance with your system ldflags and libs. Check them via the above command\n\n./configure --prefix ~/DeepDendrite/install --without-iv --with-paranrn --with-nrnpython=`which python3` PYLIB=\"-L/usr/lib/python3.12/config-3.12-x86_64-linux-gnu -lpython3.12\" PYLIBDIR=\"/usr/lib/x86_64-linux-gnu\" PYLIBLINK=\"-L/usr/lib/python3.12/config-3.12-x86_64-linux-gnu -lpython3.12\"\n\nautoreconf -f -i  # this will correct for any version mismatch during the configure\n\n# This will make \nmake -j8  \n\nexport PYTHONPATH=$PYTHONPATH:/home/olive/DeepDendrite/install/lib/python\n\nmake install \n\n# After installtion start running the Figure 5 and Figure 4 files\n\n\n2. Installation of DeepDendrite\n\n\nrm -rf build\nmkdir build\ncd build\n\n\n# for older versions of pgi, load the moduleflie as following\nmodule purge\nmodule load openmpi\nmodule load /usr/share/modules/modulefiles/nvhpc-hpcx-cuda12/25.1\n\nexport CC=mpicc\nexport CXX=mpicxx\ncmake .. -DCMAKE_C_FLAGS:STRING=\"-lrt -g -O0 -mp -mno-abm\" -DCMAKE_CXX_FLAGS:STRING=\"-lrt -std=c++11 -g -O0 -mp -mno-abm\" -DCOMPILE_LIBRARY_TYPE=STATIC -DCMAKE_INSTALL_PREFIX=\"/home/olive/DeepDendrite/install\" -DADDITIONAL_MECHPATH=\"/home/olive/DeepDendrite/src/all_mechanisms\" -DCUDA_HOST_COMPILER=`which gcc` -DCUDA_PROPAGATE_HOST_FLAGS=OFF -DENABLE_SELECTIVE_GPU_PROFILING=ON -DENABLE_OPENACC=ON -DAUTO_TEST_WITH_SLURM=OFF -DAUTO_TEST_WITH_MPIEXEC=OFF -DFUNCTIONAL_TESTS=OFF -DUNIT_TESTS=OFF\n\nmake -j24\nmake install",
    "crumbs": [
      "Neuron",
      "NEURON simulator"
    ]
  },
  {
    "objectID": "simulations/r/index.html",
    "href": "simulations/r/index.html",
    "title": "R",
    "section": "",
    "text": "R\nA few R packages",
    "crumbs": [
      "R"
    ]
  },
  {
    "objectID": "simulations/brian2/index.html",
    "href": "simulations/brian2/index.html",
    "title": "Brian2",
    "section": "",
    "text": "Brian2\nBrian2 allows scientists to simulate spiking neural network models using simple and concise high-level descriptions. This makes it more accessible to researchers who may not have extensive programming experience.",
    "crumbs": [
      "Brian2"
    ]
  },
  {
    "objectID": "simulations/neuron/Introduction.html",
    "href": "simulations/neuron/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nRecently, NEURON simulator has python interface, with excellent beginners tutorials.\n\n\nBall and stick 1: Basic cell\nLoad the necessary modules\n\nfrom neuron import h\nfrom neuron.units import ms, mV, µm \n\nimport matplotlib.pyplot as plt   # For plotting using matplotlib\n\n  \nh.load_file(\"stdrun.hoc\") #load the standard run library to give us high-level simulation control functions (e.g. running a simulation for a given period of time):\n\nDefine one cell with soma and dendrite (with necessary parameters, morphology) using python class\n\nclass BallAndStick:\n    def __init__(self, gid):\n        self._gid = gid\n        self._setup_morphology()\n        self._setup_biophysics()\n\n    def _setup_morphology(self):\n        self.soma = h.Section(name=\"soma\", cell=self)\n        self.dend = h.Section(name=\"dend\", cell=self)\n        self.dend.connect(self.soma)\n        self.all = self.soma.wholetree()\n        self.soma.L = self.soma.diam = 12.6157 * µm\n        self.dend.L = 200 * µm\n        self.dend.diam = 1 * µm\n\n    def _setup_biophysics(self):\n        for sec in self.all:\n            sec.Ra = 100  # Axial resistance in Ohm * cm\n            sec.cm = 1  # Membrane capacitance in micro Farads / cm^2\n        self.soma.insert(\"hh\")\n        for seg in self.soma:\n            seg.hh.gnabar = 0.12  # Sodium conductance in S/cm2\n            seg.hh.gkbar = 0.036  # Potassium conductance in S/cm2\n            seg.hh.gl = 0.0003  # Leak conductance in S/cm2\n            seg.hh.el = -54.3 * mV  # Reversal potential \n        self.dend.insert(\"pas\")  # # Insert passive current in the dendrite  \n        for seg in self.dend:   \n            seg.pas.g = 0.001  # Passive conductance in S/cm2        \n            seg.pas.e = -65 * mV  # Leak reversal potential             \n\n    def __repr__(self):\n        return \"BallAndStick[{}]\".format(self._gid)\n\n\nmy_cell = BallAndStick(0)\n\n\n# Make sure soma is hh and dendtrite is passive membrane\nfor sec in h.allsec():\n    print(\"%s: %s\" % (sec, \", \".join(sec.psection()[\"density_mechs\"].keys())))\n\n\nStimulus\n\nstim = h.IClamp(my_cell.dend(1))  # at the origin of the dentrite\nstim.delay = 5 #ms\nstim.dur = 1 #ms\nstim.amp = 0.1  # nA\n\n\n\nRecording soma voltage\n\nsoma_v = h.Vector().record(my_cell.soma(0.5)._ref_v)\nt = h.Vector().record(h._ref_t)\n\n\n\nInitialization\n\nh.finitialize(-65 * mV) # initialize membrane potential \n\nh.continuerun(25 * ms) # run until time 25 ms:\n\n\nplt.figure()\nplt.plot(t, soma_v)\nplt.xlabel(\"t (ms)\")\nplt.ylabel(\"v (mV)\")\nplt.show()",
    "crumbs": [
      "Neuron",
      "Introduction"
    ]
  },
  {
    "objectID": "simulations/neuron/NMODL.html",
    "href": "simulations/neuron/NMODL.html",
    "title": "NMODL",
    "section": "",
    "text": "The acronym “nrnivmodl” stands for NEURON Interpreter and Model Description Language Compiler. It is part of the NEURON simulation environment and is used to compile .mod files (written in NMODL) into a specialized version of NEURON called special. This version includes user-defined mechanisms for simulating physiological neuron models.\nWhy NMODL in Neuron simulator?\nThe purpose is to define biophysical mechanisms such as ion channels, synaptic models, or custom current/voltage sources. The .mod files extend NEURON’s capabilities by specifying kinetic equations, state variables, and interactions with ions.\nMechanisms are normally local. That is they do not depend on what is happening at other places on the neuron.",
    "crumbs": [
      "Neuron",
      "NMODL"
    ]
  },
  {
    "objectID": "simulations/neuron/NMODL.html#core-blocks-in-nmodl",
    "href": "simulations/neuron/NMODL.html#core-blocks-in-nmodl",
    "title": "NMODL",
    "section": "Core Blocks in NMODL",
    "text": "Core Blocks in NMODL",
    "crumbs": [
      "Neuron",
      "NMODL"
    ]
  },
  {
    "objectID": "simulations/neuron/NMODL.html#neuron-block",
    "href": "simulations/neuron/NMODL.html#neuron-block",
    "title": "NMODL",
    "section": "NEURON BLOCK",
    "text": "NEURON BLOCK\nDeclares mechanism properties and interface variables:\n\n\n\n\n\n\nNEURON BLOCK\n\n\n\nNEURON {\nSUFFIX…\nRANGE …\nGLOBAL …\nNONSPECIFIC_CURRENT …\nUSEION … READ … WRITE … VALENCE real // Specifies ion interaction (see Nernst potential below)\nPOINT_PROCESS …\nPOINTER …\nEXTERNAL …\n}\n\n\nThe RANGE and GLOABAL names should also be declared in the normal PARAMETER or ASSIGNED statement outside of the NEURON block, otherwise they will be hidden from the user.",
    "crumbs": [
      "Neuron",
      "NMODL"
    ]
  },
  {
    "objectID": "simulations/neuron/NMODL.html#key-differences",
    "href": "simulations/neuron/NMODL.html#key-differences",
    "title": "NMODL",
    "section": "Key Differences",
    "text": "Key Differences",
    "crumbs": [
      "Neuron",
      "NMODL"
    ]
  },
  {
    "objectID": "simulations/neuron/NMODL.html#range-variables",
    "href": "simulations/neuron/NMODL.html#range-variables",
    "title": "NMODL",
    "section": "Range Variables",
    "text": "Range Variables\n\nDefinition: Range variables can have different values in each compartment (segment) of a section.\nUsage: These are used for variables that vary spatially along the length of a neuron section, such as membrane potential or ion concentrations.\nAccess: Values can be accessed at specific positions using syntax like section(x).variable, where xx is the normalized position (e.g., 0 to 1).\nExamples: Membrane potential ($v$ ), ion channel conductance ( g_{ion}), or local currents.",
    "crumbs": [
      "Neuron",
      "NMODL"
    ]
  },
  {
    "objectID": "simulations/neuron/NMODL.html#global-variables",
    "href": "simulations/neuron/NMODL.html#global-variables",
    "title": "NMODL",
    "section": "Global Variables",
    "text": "Global Variables\n\nDefinition: Global variables have the same value across all compartments and sections.\nUsage: These are used for parameters or variables that do not vary spatially, such as universal constants or shared properties.\nAccess: They are accessed directly without specifying a position, e.g., variable.\nExamples: Temperature, reversal potentials, or a global scaling factor.\n\n\nAbout point process\nNEURON makes the distinction between mechanisms that are attributed to an entire section (e.g., HH channels) and mechanisms that are associated with a particular point in the section (e.g., voltage clamp or synapse). While the former are most conveniently expressed in terms of per unit area, the point processes are more conveniently expressed in absolute terms (e.g., current injection is usually expressed in terms of nA instead of nA/cm2). Point processes also differ in that you can insert several in the same segment.\nThere are several built-in point processes, including: IClamp, VClamp and ExpSyn. Additional point processes can be built into the simulator with the model description language.\nNernst potential\n\n\\displaystyle E={\\frac {RT}{zF}}\\ln {\\frac {[{\\text{ion outside cell}}]}{[{\\text{ion inside cell}}]}}=2.3026{\\frac {RT}{zF}}\\log _{10}{\\frac {[{\\text{ion outside cell}}]}{[{\\text{ion inside cell}}]}}.\n\nwhen finitialize() is called Nernst equation is computed.",
    "crumbs": [
      "Neuron",
      "NMODL"
    ]
  },
  {
    "objectID": "simulations/neuron/NMODL.html#verbatim-block",
    "href": "simulations/neuron/NMODL.html#verbatim-block",
    "title": "NMODL",
    "section": "VERBATIM BLOCK",
    "text": "VERBATIM BLOCK\nSections of code surrounded by VERBATIM and ENDVERBATIM blocks are interpreted as literal C/C++ code. This feature is typically used to interface with external C/C++ libraries, or to use NEURON features (such as random number generation) that are not explicitly supported in the NMODL language.",
    "crumbs": [
      "Neuron",
      "NMODL"
    ]
  },
  {
    "objectID": "simulations/python/index.html",
    "href": "simulations/python/index.html",
    "title": "Python codes",
    "section": "",
    "text": "Python codes",
    "crumbs": [
      "Python",
      "Python codes"
    ]
  },
  {
    "objectID": "research/ImprovedLS.html",
    "href": "research/ImprovedLS.html",
    "title": "Improved LS Channel Estimation in CFO",
    "section": "",
    "text": "The authors consider the design of pilot sequences for channel estimation in the presence of carrier frequency offset (CFO) in systems that employ orthogonal frequency division multiplexing (OFDM). The CFO introduces intercarrier interference (ICI) which degrades the accuracy of the channel estimation. In order to minimise this effect, the authors design pilot sequence that minimises the mean square error (MSE) of the modified least squares (mLS) channel estimator. Since the identical pilot sequence, which minimises this MSE, has high peak-to-average power ratio of the OFDM signal, an alternative approach is proposed for channel estimation. The authors first introduce a new estimator as an alternative to the mLS estimator and design a low PAPR pilot sequences tailored to this new estimator. They show that the proposed procedure completely eliminates the effect of the ICI on the channel estimate. They then extend their design of pilot sequences for realistic sparse channels. Both analytical and computer simulation results presented in this study demonstrate the superiority of the proposed approach over conventional methods for channel estimation in the presence of ICI.",
    "crumbs": [
      "Research",
      "Improved LS Channel Estimation in CFO"
    ]
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Filters with random transmittance for improving resolution in filter-array-based spectrometers\n\n\n\n\n\n\n\n\n2013-02-25\n\n\nOliver James, Woongbi Lee, and Heung-No Lee\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of sampling rate on fMRI connectivity analysis\n\n\n\n\n\n\n\n\n2019-01-01\n\n\nOliver James, Hyunjin Park, Seong-Gi Kim\n\n\n\n\n\n\n\n\n\n\n\n\nImproved LS Channel Estimation in CFO\n\n\n\n\n\n\n\n\n2012-02-01\n\n\nOliver James, Aravind, R. ; Prabhu, K. M. M.\n\n\n\n\n\n\n\n\n\n\n\n\nImproving resolution of miniature spectrometers by exploiting sparse nature of signals\n\n\n\n\n\n\n\n\n2012-03-30\n\n\nOliver James, Woongbi Lee, Sangjun Park, and Heung-No Lee\n\n\n\n\n\n\n\n\n\n\n\n\nKrylov Subspace-based Channel Estimation\n\n\n\n\n\n\n\n\n2010-01-01\n\n\nOliver James, Aravind, R. ; Prabhu, K. M. M.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the RIP of real and complex Gaussian sensing matrices via RIV framework in sparse signal recovery analysis\n\n\n\n\n\n\n\n\n2015-06-01\n\n\nOliver James\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Channel Estimation\n\n\n\n\n\n\n\n\n2008-02-01\n\n\nOliver James, Aravind, R. ; Prabhu, K. M. M.\n\n\n\n\n\n\n\n\n\n\n\n\nWavelet Shift Keying\n\n\n\n\n\n\n\n\n2005-08-16\n\n\nOliver\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/Spec2.html",
    "href": "research/Spec2.html",
    "title": "Filters with random transmittance for improving resolution in filter-array-based spectrometers",
    "section": "",
    "text": "In this paper, we introduce a method for improving the resolution of miniature spectrometers. Our method is based on using filters with random transmittance. Such filters sense fine details of an input signal spectrum, which, when combined with a signal processing algorithm, aid in improving resolution. We also propose an approach for designing filters with random transmittance using optical thin-film technology. We demonstrate that the improvement in resolution is 7-fold when using the filters with random transmittance over what was achieved in our previous work.",
    "crumbs": [
      "Research",
      "Filters with random transmittance for improving resolution in filter-array-based spectrometers"
    ]
  },
  {
    "objectID": "research/Krylov.html",
    "href": "research/Krylov.html",
    "title": "Krylov Subspace-based Channel Estimation",
    "section": "",
    "text": "We investigate a low-rank minimum mean-square error (MMSE) channel estimator in orthogonal frequency division multiplexing (OFDM) systems. The proposed estimator is derived by using the multi-stage nested Wiener filter (MSNWF) identified in the literature as a Krylov subspace approach for rank reduction. We describe the low-rank MMSE expressions for exploiting the time correlation function of the channel path gains. The Krylov subspace technique requires neither eigenvalue decomposition (EVD) nor the inverse of the covariance matrices for parameter estimation. We show that the Krylov channel estimator can perform as well as the EVD estimator with a much smaller rank. Simulation results obtained confirm the superiority of the proposed Krylov low-rank channel estimator in approaching near full-rank MSE performance.",
    "crumbs": [
      "Research",
      "Krylov Subspace-based Channel Estimation"
    ]
  },
  {
    "objectID": "research/Spec1.html",
    "href": "research/Spec1.html",
    "title": "Improving resolution of miniature spectrometers by exploiting sparse nature of signals",
    "section": "",
    "text": "In this paper, we present a signal processing approach to improve the resolution of a spectrometer with a fixed number of low-cost, non-ideal filters. We aim to show that the resolution can be improved beyond the limit set by the number of filters by exploiting the sparse nature of a signal spectrum. We consider an underdetermined system of linear equations as a model for signal spectrum estimation. We design a non-negative L_1norm minimization algorithm for solving the system of equations. We demonstrate that the resolution can be improved multiple times by using the proposed algorithm.",
    "crumbs": [
      "Research",
      "Improving resolution of miniature spectrometers by exploiting sparse nature of signals"
    ]
  },
  {
    "objectID": "tutorials/RandomProcess/MA.html",
    "href": "tutorials/RandomProcess/MA.html",
    "title": "Moving Average (MA) process",
    "section": "",
    "text": "Moving Average (MA) process\nThe YW equation for MA(1) process can be obtained by setting all the a(l) in YW equation to zero for ARMA(p,q) and noting that h(n)=b(n), the finite impulse response filter.\n\nC(k)  =  \\sigma_v^2 \\sum_{l=0}^{q-k} b(l+k) b^*(l)~ \\forall k\n\nSince the FIR filter is causal we have h(n)=0, n&lt;0. The above equation can be written as\n\nC(k)  =  \\sigma_v^2 \\sum_{l=0}^{q-|k|} b(l+|k|) b^*(l)~ \\forall k\n\nThen the YW equation for the MA(q) process is\n\nr_x(k)  =  \\sigma_v^2 \\sum_{l=0}^{q-|k|} b(l+|k|) b^*(l)~~~~ \\forall k\n\nbut r_x(k) is non-zero for k \\in [-q,q]. Thus, the autocorrelation of MA(q) process is totally determined by the autocorrelation of filter coefficients that generates the process.",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "Moving Average (MA) process"
    ]
  },
  {
    "objectID": "tutorials/RandomProcess/FilteringofRP.html",
    "href": "tutorials/RandomProcess/FilteringofRP.html",
    "title": "Filtering of random process",
    "section": "",
    "text": "Filtering of random process\nThe linear time-invariant filters are widely employed in applications ranging from modern communication system, audio processing and radar systems. The input of such systems would be usually random processes. Thus, it is of importance to study the how the statistics of the random processes change as a result of filtering. Of particular importance is the mean and autocorrelation of the input and output processes.\nConsider the input/output of an LTI system with the input process being non-zero mean \\mu_x and autocorrelation r_x(k)\n\ny(n) = \\sum_{k=-\\infty}^{\\infty} h(l)x(n-l)\n\\tag{1}\n\nMean of the output process\nThe mean of the output process is\n\n\\begin{split}\n\\mathbb{E}[y_n] & = \\sum_{k=-\\infty}^{\\infty} h(l) \\mathbb{E}[x(n-l)] \\\\\n& = \\mu \\sum_{k=-\\infty}^{\\infty} h(l)  \\\\\n& = \\mu H(e^{j0})\n\\end{split}\n\nThe mean of the output process is given by the product of the mean of the input process and the frequency response of the filter evaluated at w=0. Consequently, if the mean of the input process is zero or if H(e^{j0}) is zero, the output process will also have zero mean.\n\n\nAutocorrelation of the output\nMultiplying both sides of Equation 1 with y_{n-k} and taking expectation on both sides leads to\n\n\\begin{split}\n\\mathbb{E}[y(n) y^*(n-k)] & = \\sum_{k=-\\infty}^{\\infty} h(l)\\mathbb{E}[ x(n-l) y^*(n-k)] \\\\\nr_y(k) & = \\sum_{k=-\\infty}^{\\infty} h(l)r_{xy}(k-l)  \\\\\n\\end{split}",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "Filtering of random process"
    ]
  },
  {
    "objectID": "tutorials/EstimationTheory/CramerRaoBound.html",
    "href": "tutorials/EstimationTheory/CramerRaoBound.html",
    "title": "Cramer-Rao Lower Bound (CRLB)",
    "section": "",
    "text": "Cramer-Rao Lower Bound (CRLB)\nThe Cramer-Rao Lower Bound (CRLB) is a fundamental concept in estimation theory. It provides a theoretical lower bound on the variance of any unbiased estimator of a deterministic parameter. The ability to determine a lower bound for the variance of any unbiased estimator is extremely useful in practical situations. It allows us to assert that an estimator is MUV or provides a benchmark against which we can compare the performance of any unbiased estimator. This bound is derived using the Fisher information and is widely used to assess the efficiency of estimators.\nImportantly, the bound readily helps us to determine an estimator that attains the bound. So it is a way of finding the UB estimator.\nThe Bayesian Cramér-Rao Lower Bound (BCRLB) is an extension of the classical Cramér-Rao Lower Bound (CRLB) to Bayesian estimation problems. Unlike the classical CRLB, which applies to deterministic parameters, the BCRLB is used when the parameter being estimated is treated as a random variable with a known prior distribution.\nThere are other bounds exists. But we will try to discuss them!",
    "crumbs": [
      "Tutorials",
      "Estimation Theory",
      "Cramer-Rao Lower Bound (CRLB)"
    ]
  },
  {
    "objectID": "tutorials/EstimationTheory/PointEstimation.html",
    "href": "tutorials/EstimationTheory/PointEstimation.html",
    "title": "Point Estimation",
    "section": "",
    "text": "Point Estimation",
    "crumbs": [
      "Tutorials",
      "Estimation Theory",
      "Point Estimation"
    ]
  },
  {
    "objectID": "gleanings/index.html",
    "href": "gleanings/index.html",
    "title": "List of gleanings",
    "section": "",
    "text": "List of gleanings\n\nKalman Filter and its variants",
    "crumbs": [
      "Gleanings",
      "List of gleanings"
    ]
  },
  {
    "objectID": "gleanings/PCA/PCA.html#optimization-problems",
    "href": "gleanings/PCA/PCA.html#optimization-problems",
    "title": "PCA",
    "section": "Optimization Problems",
    "text": "Optimization Problems\n\n1. Variance Maximization Formulation\nPCA finds a linear subspace that maximizes the variance of the projected data. Let ( X ^{n d} ) be the mean-centered data matrix, and let ( = X^X ) be its empirical covariance matrix.\n[\n\\begin{aligned}\n\\text{maximize} \\quad & \\operatorname{Tr}(W^\\top \\Sigma W) \\\\\n\\text{subject to} \\quad & W^\\top W = I_k\n\\end{aligned}\n]\n\n( W ^{d k} ) is the matrix of principal directions (orthonormal columns).\n( ) denotes the trace of a matrix.\n\n\n\n\n2. Reconstruction Error Minimization Formulation\nPCA can also be seen as finding a rank-( k ) approximation of ( X ) that minimizes the reconstruction error:\n[\n\\begin{aligned}\n\\text{minimize} \\quad & \\| X - ZW^\\top \\|_F^2 \\\\\n\\text{subject to} \\quad & W^\\top W = I_k\n\\end{aligned}\n]\n\n( Z ^{n k} ) are the low-dimensional projections (scores).\n( W ^{d k} ) is the projection matrix with orthonormal columns.\n( | |_F ) is the Frobenius norm."
  },
  {
    "objectID": "tutorials/RandomProcess/Harmonic.html",
    "href": "tutorials/RandomProcess/Harmonic.html",
    "title": "Harmonic process",
    "section": "",
    "text": "Harmonic process",
    "crumbs": [
      "Tutorials",
      "Random Processes",
      "Harmonic process"
    ]
  },
  {
    "objectID": "journalclub/index.html",
    "href": "journalclub/index.html",
    "title": "My presentation materials the Journal Clubs",
    "section": "",
    "text": "My presentation materials the Journal Clubs"
  },
  {
    "objectID": "gleanings/laws/index.html",
    "href": "gleanings/laws/index.html",
    "title": "Laws",
    "section": "",
    "text": "If you can write down a problem clearly, you’ve already solved half of it.” Writing is a powerful tool for problem-solving, because writing is thinking. You cannot write clearly if you aren’t thinking clearly."
  },
  {
    "objectID": "gleanings/laws/index.html#kidlins-law-origin-unknown",
    "href": "gleanings/laws/index.html#kidlins-law-origin-unknown",
    "title": "Laws",
    "section": "",
    "text": "If you can write down a problem clearly, you’ve already solved half of it.” Writing is a powerful tool for problem-solving, because writing is thinking. You cannot write clearly if you aren’t thinking clearly."
  },
  {
    "objectID": "gleanings/laws/index.html#pareto-law",
    "href": "gleanings/laws/index.html#pareto-law",
    "title": "Laws",
    "section": "Pareto law",
    "text": "Pareto law\nWhat is the 80 20 rule in psychology?\nIt is also known as the Pareto law, and as the principle of least effort. It states that a surprisingly small proportion of efforts and inputs (20%) lead to 80% of our results. In other words, there is an extremely lopsided distribution of inputs and outcomes."
  },
  {
    "objectID": "gleanings/OUprocess/index.html",
    "href": "gleanings/OUprocess/index.html",
    "title": "Ornstein-Uhlenbeck Process: Theory and Simulation",
    "section": "",
    "text": "The Ornstein-Uhlenbeck (OU) process is a stochastic process that describes the velocity of a massive Brownian particle under friction. It’s widely used in physics, finance, and biology as a mean-reverting process."
  },
  {
    "objectID": "gleanings/OUprocess/index.html#introduction",
    "href": "gleanings/OUprocess/index.html#introduction",
    "title": "Ornstein-Uhlenbeck Process: Theory and Simulation",
    "section": "",
    "text": "The Ornstein-Uhlenbeck (OU) process is a stochastic process that describes the velocity of a massive Brownian particle under friction. It’s widely used in physics, finance, and biology as a mean-reverting process."
  },
  {
    "objectID": "gleanings/OUprocess/index.html#mathematical-definition",
    "href": "gleanings/OUprocess/index.html#mathematical-definition",
    "title": "Ornstein-Uhlenbeck Process: Theory and Simulation",
    "section": "Mathematical Definition",
    "text": "Mathematical Definition\nThe OU process is defined by the stochastic differential equation (SDE):\ndX_t = \\theta(\\mu - X_t)dt + \\sigma dW_t\nwhere:\n\nX_t is the process value at time t\n\\theta &gt; 0 is the mean reversion rate (speed of reversion)\n\\mu is the long-term mean (equilibrium level)\n\\sigma &gt; 0 is the volatility (diffusion coefficient)\nW_t is a Wiener process (standard Brownian motion)\ndW_t \\sim N(0, dt)\n\n\nParameters Explained\n\nθ (theta): Mean reversion speed\n\nHigher θ → faster reversion to mean\nLower θ → slower reversion (more persistent deviations)\n\nμ (mu): Long-term mean\n\nThe level around which the process oscillates\nThe expected value as t \\to \\infty\n\nσ (sigma): Volatility\n\nControls the magnitude of random fluctuations\nHigher σ → more noisy process"
  },
  {
    "objectID": "gleanings/OUprocess/index.html#discrete-time-approximation",
    "href": "gleanings/OUprocess/index.html#discrete-time-approximation",
    "title": "Ornstein-Uhlenbeck Process: Theory and Simulation",
    "section": "Discrete-Time Approximation",
    "text": "Discrete-Time Approximation\nUsing the Euler-Maruyama method:\nX_{t+\\Delta t} = X_t + \\theta(\\mu - X_t)\\Delta t + \\sigma\\sqrt{\\Delta t}\\,Z_t\nwhere Z_t \\sim N(0,1) are independent standard normal variables."
  },
  {
    "objectID": "gleanings/OUprocess/index.html#simulation-in-r",
    "href": "gleanings/OUprocess/index.html#simulation-in-r",
    "title": "Ornstein-Uhlenbeck Process: Theory and Simulation",
    "section": "Simulation in R",
    "text": "Simulation in R\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Set seed for reproducibility\nset.seed(123)\n\n\nBasic OU Process Simulation\n\nsimulate_ou &lt;- function(n, dt, x0, theta, mu, sigma) {\n  # n: number of time steps\n  # dt: time increment\n  # x0: initial value\n  # theta: mean reversion rate\n  # mu: long-term mean\n  # sigma: volatility\n  \n  x &lt;- numeric(n)\n  x[1] &lt;- x0\n  \n  for (i in 2:n) {\n    dW &lt;- rnorm(1, mean = 0, sd = sqrt(dt))\n    x[i] &lt;- x[i-1] + theta * (mu - x[i-1]) * dt + sigma * dW\n  }\n  \n  return(x)\n}\n\n\n\nSingle Trajectory Example\n\n# Parameters\nn &lt;- 1000\ndt &lt;- 0.01\nx0 &lt;- 0\ntheta &lt;- 1.5\nmu &lt;- 5\nsigma &lt;- 2\n\n# Simulate\ntime &lt;- seq(0, (n-1)*dt, by = dt)\ntrajectory &lt;- simulate_ou(n, dt, x0, theta, mu, sigma)\n\n# Create data frame\ndf_single &lt;- data.frame(time = time, value = trajectory)\n\n# Plot\nggplot(df_single, aes(x = time, y = value)) +\n  geom_line(color = \"steelblue\", linewidth = 0.8) +\n  geom_hline(yintercept = mu, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = max(time)*0.9, y = mu + 0.5, \n           label = paste0(\"μ = \", mu), color = \"red\") +\n  labs(title = \"Ornstein-Uhlenbeck Process: Single Trajectory\",\n       subtitle = paste0(\"θ = \", theta, \", μ = \", mu, \", σ = \", sigma),\n       x = \"Time\", y = \"X(t)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\nMultiple Trajectories\n\n# Simulate multiple paths\nn_paths &lt;- 10\ntrajectories &lt;- matrix(0, nrow = n, ncol = n_paths)\n\nfor (i in 1:n_paths) {\n  trajectories[, i] &lt;- simulate_ou(n, dt, x0, theta, mu, sigma)\n}\n\n# Create data frame\ndf_multi &lt;- data.frame(time = time, trajectories) %&gt;%\n  pivot_longer(cols = -time, names_to = \"path\", values_to = \"value\")\n\n# Plot\nggplot(df_multi, aes(x = time, y = value, color = path)) +\n  geom_line(alpha = 0.6, linewidth = 0.6) +\n  geom_hline(yintercept = mu, linetype = \"dashed\", color = \"black\", linewidth = 1) +\n  labs(title = \"Multiple OU Process Trajectories\",\n       subtitle = paste0(\"θ = \", theta, \", μ = \", mu, \", σ = \", sigma),\n       x = \"Time\", y = \"X(t)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))"
  },
  {
    "objectID": "gleanings/OUprocess/index.html#effect-of-parameters",
    "href": "gleanings/OUprocess/index.html#effect-of-parameters",
    "title": "Ornstein-Uhlenbeck Process: Theory and Simulation",
    "section": "Effect of Parameters",
    "text": "Effect of Parameters\n\nVarying Mean Reversion Rate (θ)\n\ntheta_values &lt;- c(0.5, 1.5, 3.0)\ndf_theta &lt;- data.frame()\n\nfor (theta_val in theta_values) {\n  traj &lt;- simulate_ou(n, dt, x0, theta_val, mu, sigma)\n  df_temp &lt;- data.frame(\n    time = time,\n    value = traj,\n    theta = paste0(\"θ = \", theta_val)\n  )\n  df_theta &lt;- rbind(df_theta, df_temp)\n}\n\nggplot(df_theta, aes(x = time, y = value, color = theta)) +\n  geom_line(linewidth = 0.8) +\n  geom_hline(yintercept = mu, linetype = \"dashed\", color = \"black\") +\n  facet_wrap(~theta, ncol = 1) +\n  labs(title = \"Effect of Mean Reversion Rate (θ)\",\n       subtitle = paste0(\"μ = \", mu, \", σ = \", sigma),\n       x = \"Time\", y = \"X(t)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\nVarying Volatility (σ)\n\nsigma_values &lt;- c(0.5, 1.5, 3.0)\ndf_sigma &lt;- data.frame()\n\nfor (sigma_val in sigma_values) {\n  traj &lt;- simulate_ou(n, dt, x0, theta, mu, sigma_val)\n  df_temp &lt;- data.frame(\n    time = time,\n    value = traj,\n    sigma = paste0(\"σ = \", sigma_val)\n  )\n  df_sigma &lt;- rbind(df_sigma, df_temp)\n}\n\nggplot(df_sigma, aes(x = time, y = value, color = sigma)) +\n  geom_line(linewidth = 0.8) +\n  geom_hline(yintercept = mu, linetype = \"dashed\", color = \"black\") +\n  facet_wrap(~sigma, ncol = 1) +\n  labs(title = \"Effect of Volatility (σ)\",\n       subtitle = paste0(\"θ = \", theta, \", μ = \", mu),\n       x = \"Time\", y = \"X(t)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\"))"
  },
  {
    "objectID": "gleanings/OUprocess/index.html#statistical-properties",
    "href": "gleanings/OUprocess/index.html#statistical-properties",
    "title": "Ornstein-Uhlenbeck Process: Theory and Simulation",
    "section": "Statistical Properties",
    "text": "Statistical Properties\n\nStationary Distribution\nFor large t, the OU process has a stationary distribution:\nX_\\infty \\sim N\\left(\\mu, \\frac{\\sigma^2}{2\\theta}\\right)\n\n# Simulate long trajectory to reach stationarity\nn_long &lt;- 50000\ntraj_long &lt;- simulate_ou(n_long, dt, x0, theta, mu, sigma)\n\n# Take last 10000 points (should be stationary)\nstationary_sample &lt;- tail(traj_long, 10000)\n\n# Theoretical stationary variance\nvar_theoretical &lt;- sigma^2 / (2 * theta)\nsd_theoretical &lt;- sqrt(var_theoretical)\n\n# Plot histogram\ndf_hist &lt;- data.frame(value = stationary_sample)\n\nggplot(df_hist, aes(x = value)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, \n                 fill = \"steelblue\", alpha = 0.7) +\n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sd_theoretical),\n                color = \"red\", linewidth = 1.2) +\n  labs(title = \"Stationary Distribution of OU Process\",\n       subtitle = paste0(\"Theoretical: N(\", mu, \", \", round(var_theoretical, 3), \")\"),\n       x = \"X\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n# Print statistics\ncat(\"Empirical mean:\", mean(stationary_sample), \"\\n\")\n\nEmpirical mean: 5.119992 \n\ncat(\"Theoretical mean:\", mu, \"\\n\")\n\nTheoretical mean: 5 \n\ncat(\"Empirical variance:\", var(stationary_sample), \"\\n\")\n\nEmpirical variance: 1.19043 \n\ncat(\"Theoretical variance:\", var_theoretical, \"\\n\")\n\nTheoretical variance: 1.333333"
  },
  {
    "objectID": "gleanings/OUprocess/index.html#applications",
    "href": "gleanings/OUprocess/index.html#applications",
    "title": "Ornstein-Uhlenbeck Process: Theory and Simulation",
    "section": "Applications",
    "text": "Applications\nThe OU process is used in:\n\nPhysics: Modeling particle motion with friction\nFinance: Interest rate models (Vasicek model), pairs trading\nBiology: Neural membrane potential, population dynamics\nNeuroscience: Spike train analysis\nClimate Science: Temperature anomalies"
  },
  {
    "objectID": "gleanings/OUprocess/index.html#conclusion",
    "href": "gleanings/OUprocess/index.html#conclusion",
    "title": "Ornstein-Uhlenbeck Process: Theory and Simulation",
    "section": "Conclusion",
    "text": "Conclusion\nThe Ornstein-Uhlenbeck process is a fundamental mean-reverting stochastic process with three key parameters:\n\nθ: Controls reversion speed\nμ: Determines the equilibrium level\nσ: Sets the noise magnitude\n\nIts mathematical tractability and realistic behavior make it invaluable across multiple scientific domains."
  },
  {
    "objectID": "gleanings/StochasticCalculus/index.html",
    "href": "gleanings/StochasticCalculus/index.html",
    "title": "Stochastic Calculus - Fundamentals",
    "section": "",
    "text": "Notation and code for generating visuals are presented in the Appendix.\n\n\nThis document is a brief introduction to stochastic calculus. Like, an actual introduction. Not the textbook “introductions” which immediately blast you with graduate-level probability theory axioms and definitions.\nThe goal of this blog post is more to focus on the physical intuition and derivation of Brownian motion, which is the foundation of stochastic calculus. I will avoid very technical formalisms such as probability spaces, measure theory, filtrations, etc. in favor of a more informal approach by considering only well-behaved cases. I also try to avoid introducing too many new concepts and vocabulary.\nI hope that a wider audience can feel inspired as to how stochastic calculus emerges naturally from the physical world. Then, hopefully, more people can appreciate the beauty and meaning of the mathematics behind it, and decide to dig deeper into the subject.\n\n\nBrownian motion and Itô calculus are notable examples of fairly high-level mathematics that are applied to model the real world. Stock prices jiggle erratically, molecules bounce in fluids, and noise partially corrupts signals. Stochastic calculus gives us tools to predict, optimize, and understand these messy systems in a simpified model.\n\nPhysics: Einstein used Brownian motion to prove atoms exist—its jittering matched molecular collisions.\nFinance: Option pricing (e.g., the famous Black-Scholes equation) relies on stochastic differential equations like dS = \\mu S dt + \\sigma S dW.\nBiology: Random walks model how species spread or neurons fire.\n\nThis is just the tip of the iceberg. More and more applications are emerging, notably in machine learning, as Song et al. (2021) have shown in their great paper “Score-Based Generative Modeling through Stochastic Differential Equations”.\nThey precisely use a stochastic differential equation using Itô calculus to model the evolution of noise over time, which they can then reverse in time to generate new samples. This framework generalizes previous ones and improves performance, allowing for new paths of innovation to be explored.\n\n\n\n\nPascal’s triangle gives the number of paths that go either left or right at each step, up to a certain point:\n\n\\begin{array}{cccccc}\n& & & 1 & & & \\\\\n& & 1 & & 1 & & \\\\\n& 1 & & 2 & & 1 & \\\\\n1 & & 3 & & 3 & & 1\n\\end{array}\n\nUsing 0-indexing, the number of ways to reach the k-th spot in the n-th row is \\binom{n}{k} = \\frac{n!}{k!(n-k)!}. For example, in row 3, there are \\binom{3}{2} = 3 ways to hit position 2.\n Code 2D image: All 3 paths for the 2nd position in the 3rd row of Pascal’s triangle\nWhy care? This setup powers the binomial distribution, which models repeated trials with two outcomes—win or lose, heads or tails. Think of: - A basketball player shooting free throws with probability p of success and q = 1 - p of failure. - A gambler betting on dice rolls.\nPascal’s triangle tells us there are \\binom{n}{k} ways to get k wins in n trials. If the trials are independent, we can use the multiplication rule for probabilities:\n\nNote that the independence assumption is strong. Real life isn’t always so clean—winning streaks in games often tie to mentality or momentum, not just chance. Keep in mind that this model can and will be inaccurate, especially visibile for very long streaks in phenomena like stock prices or sports. However, in more common scenarios, it usually approximates reality well.\n\n\nP(A \\text{ and } B \\text{ and } C \\dots) = P(A) P(B) P(C) \\dots\n\nFor one sequence with k wins (probability p each) and n - k losses (probability q each), the probability is p^k q^{n-k}. Multiply by the number of ways to arrange those wins, and we get:\n\nP(k \\text{ wins in } n \\text{ trials}) = \\binom{n}{k} p^k q^{n-k}\n\nThis is the binomial distribution—great for discrete setups. Now, let’s zoom out. The real world often involves continuous processes, like: - The motion of a falling object, - Gas diffusing through a room, - Stock prices jumping around, - Molecules colliding in a liquid.\nFor these, the binomial model gets messy as trials pile up. Calculus, with its focus on continuous change, feels more natural. In the continuous case:\n\nPoints and sums (discrete tools) lead to infinities. We need intervals and integrals instead.\n\n\n\n\nIt’s actually known what happens to the binomial distribution as it becomes continuous. But what does that conversion mean mathematically? Let’s dig in with examples and then formalize it.\nIn calculus, going from discrete to continuous means shrinking step sizes and cranking up the number of steps. For an interval [a, b], we: 1. Split it into n chunks of size h = \\frac{b - a}{n}, 2. Sum up contributions (like a Riemann sum), 3. Let n \\to \\infty and h \\to 0, landing on an integral.\nCan we adapt this to the binomial distribution? Let’s try.\nPicture the n-th row of Pascal’s triangle as a random walk: at each of n steps, we move +1 (a win) or -1 (a loss).\nWe’ll set the probabability of winning as p = 0.5 as a first example since it’s symmetric, making each direction equally likely and simpler to work with.\nThe number of ways to get k wins (and n - k losses) is \\binom{n}{k}. Let’s try to plot this for a different values n over k. (The code can be found in the Appendix.)\n Code 2D image: Binomial distribution plots for n=5,10,25,50,100\nThat looks awfully familiar, doesn’t it? It’s a bell curve, so naturally, we might guess that the limit is a normal distribution (aka Gaussian distribution).\nWhere does such a normal distribution arise from? The answer lies in the Central Limit Theorem, which states that the sum of a large number of independent random variables will be approximately normally distributed. So where’s the sum happening here? Let’s proceed to formalizing our intuition.\nTo accomplish this, let’s define a random variable for a single step as:\n\nX(t) = \\begin{cases}\n    1 & \\text{with probability } \\frac{1}{2} \\\\\n    -1 & \\text{with probability } \\frac{1}{2} \\\\\n\\end{cases}\n\nHere, X(t) will encode our displacement at the t-th step where t \\in \\{1,\\dots,n\\} is an indexing parameter. As before, we assume that X(t_1) is independent of X(t_2) for t_1 \\ne t_2 . At each step t, X(t) has mean 0 and variance 1.\nThen, the overall displacement S(n) is:\n\nS(n) = X(1) + X(2) + \\dots + X(n) = \\sum_{t=1}^n X(t)\n\nSo there it is! The central limit theorem states more precisely that given n independent and identically distributed random variables X_1, X_2, \\dots, X_n with mean \\mu and variance \\sigma^2, we have:\n\nX_1 + \\dots + X_n \\sim N(n\\mu, n\\sigma^2) \\text{ as } n \\to \\infty\n\nThis is precisely what we need. As we take n \\to \\infty, we have that\n\nS(n) \\sim N(0, n)\n\nsuch that\n\n\\lim_{n \\to \\infty} \\frac{1}{\\sqrt{n}} \\cdot S(n) = N(0, 1)\n\nwhich is our desired limit. We have shown that a “continuous binomial distribution” is in fact a normal distribution.\nHere are some very nice 3D animations of sample paths with the distribution evolving over the number of steps:\n Code 3D animation: Discrete Random Walk, 15 steps\n Code 3D Animation: Discrete Random Walk, 100 steps over 5 seconds\n Code 2D animation: Normal distribution approximation by discrete random walks\n\n\n\nLet’s consider a scenario faced by Scottish botanist Robert Brown in the 1820s. Imagine a small particle, like dust or pollen, floating on a body of water.\nBrown realized that its movement was surprisingly erratic. It seemed like the small-scale nature of the setup resulted in such sensitivity to fluctuations, so much is that the real movement from external forces would completely overtake the previous one. Hence, in a simplified mathematical model we scale consider the events at different times as independent.\nIn addition, there is positional symmetry: the average position of the particle at time t seemed to float approximately around the origin.\nMotivated by these observations as well as our previous intuition on continuous random walks, let’s first think about a simplified model for 1-dimensional discrete case. We’ll list some properties that a continuous random walk should have.\n\nStarting Point: As a mathematical convenience, we position our coordinate system to set the starting point of the walk to be zero.\nPositional Symmetry: The walk has no directional bias. For each step, the expected displacement is zero, such that the overall expected displacement is also zero.\nIndependence: Steps at different times are independent. The displacement between two different intervals of time is independent.\nContinuity: The walk is continuous, with no jumps or gaps.\nNormality: As we established by taking discrete random walks in the continuous limit, the distribution of positions at any given time should be normal.\n\nSo let’s write this mathematically. Such a random variable is usually denoted either by B_t for “Brownian motion”, which is the physical phenomenon, or W_t for “Wiener process”, in honor of the mathematician Norbert Wiener who developed a lot of its early theory.\nI will use W(t) to emphasize its dependence on t.\nLet W(t) be the position of the Brownian motion at time t, and let \\Delta W(t_1,t_2) be the displacement of the Brownian motion from time t_1 to time t_2.\n\nNote that, unlike the discrete case, we cannot consider a single increment and have a single index t for displacements as we did with X(t). As mentioned, the continuous case requires considering intervals instead of single steps.\n\nThen, we write some properties of Brownian motion:\n\nW(0)=0 almost surely\n$W(t)\\sim N(0,t)$\n\nWith the first condition, this is often written equivalently as \\Delta W(s,t)\\sim N(0,t-s) for all s \\ne t\n\n\\Delta W(t_1,t_2) is independent of \\Delta W(t_2,t_3) for arbitrary distinct t_1 &lt; t_2 \\le t_3\n\nWe can straightforwardly use these conditions are enough to find\n\nE[W(t)]=0 for all t\nVar(W(t))=t for all t\n\nThis is analogous to the discrete case.\nBut it also turns out that these conditions are sufficient to prove continuity, although it’s more involved:\n\nThe sample path t \\mapsto W(t)  is almost surely uniformly Hölder continuous for each exponent \\gamma &lt; \\frac{1}{2}, but is nowhere Hölder continuous for \\gamma &gt;= \\frac{1}{2}. p.30,33 of source\n\nIn particular, a sample path t \\mapsto W(t) is almost surely nowhere differentiable.\n\n\nSo, W(t) is our mathematical model for Brownian motion: a continuous, random, zero-mean process with variance proportional to time. It’s wild—it’s globally somewhat predictable yet locally completely unpredictable. A plot of W(t) looks like a jagged mess, but it’s got structure under the hood. (You can generate one yourself with the code in Appendix.)\n Code 2D image: Sample Brownian motion path\n Code 3D animation: Brownian motion with evolving distribution\nNow, let’s take this beast and do something useful with it.\n\n\n\n\nBrownian motion W(t) is continuous but so irregular that it’s nowhere differentiable. To see why, consider the rate of change over a small interval dt:\n\n\\lim_{dt \\to 0} \\frac{W(t + dt) - W(t)}{dt} = \\lim_{dt \\to 0} \\frac{\\Delta W(t, t + dt)}{dt}\n\nSince \\Delta W(t, t + dt) \\sim N(0, dt) = \\sqrt{dt} \\, N(0, 1):\n\n\\frac{\\Delta W(t, t + dt)}{dt} = \\frac{\\sqrt{dt} \\, N(0, 1)}{dt} = \\frac{1}{\\sqrt{dt}} N(0, 1)\n\nAs dt \\to 0, \\frac{1}{\\sqrt{dt}} grows without bound, and the expression becomes dominated by random fluctuations—it doesn’t converge to a finite derivative. This rules out standard calculus for handling Brownian motion, but we still need a way to work with processes driven by it, like stock prices or particle diffusion.\nIn the 1940s, Kiyosi Itô developed a framework to address this: Itô calculus. Rather than forcing Brownian motion into the rules of regular calculus, Itô built a new system tailored to its random nature, forming the foundation of stochastic calculus.\n\n\nDefine the small change in Brownian motion over an interval dt:\n\ndW := W(t + dt) - W(t) = \\Delta W(t, t + dt)\n\nFrom Section 3, W(t + dt) - W(t) \\sim N(0, dt), so:\n\ndW = \\sqrt{dt} \\, N(0, 1)\n\nUnlike the deterministic dx in regular calculus, dW is random—its magnitude scales with \\sqrt{dt}, and its sign depends on a standard normal distribution N(0, 1). It’s a small but erratic step, with: - E[dW] = 0, - Var(dW) = E[(dW)^2] = dt.\nNow consider (dW)^2. Its expected value is dt, but what about its variability? The variance is Var[(dW)^2] = 2 dt^2, which becomes negligible as dt \\to 0. This stability allows us to treat (dW)^2 \\approx dt in Itô calculus (formally, in the mean-square sense—see the Appendix for details). In contrast to regular calculus, where (dx)^2 is too small to matter, (dW)^2 is on the same scale as dt, which changes how we handle calculations.\n\n\n\nIn regular calculus, \\int_a^b f(x) \\, dx approximates the area under a curve by summing rectangles, \\sum f(x_i) \\Delta x, and taking the limit as \\Delta x \\to 0. For Brownian motion, we want something like \\int_0^t f(s) \\, dW(s), where dW(s) replaces dx. Here, the steps are random: \\Delta W(s_i, s_{i+1}) \\sim \\sqrt{\\Delta s} \\, N(0, 1). We approximate:\n\n\\int_0^t f(s) \\, dW(s) \\approx \\sum_{i=0}^{n-1} f(s_i) [\\Delta W(s_i, s_{i+1})]\n\nover a partition s_0, s_1, \\dots, s_n of [0, t], then let n \\to \\infty. Unlike a deterministic integral, the result is a random variable, reflecting W(t)’s randomness. Using f(s_i) from the left endpoint keeps the integral “non-anticipating”—we only use information up to time s_i, which aligns with the forward-evolving nature of stochastic processes.\n\n\n\nFor a function f(t, W(t)), regular calculus gives:\n\ndf = \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial W} dW\n\nBut Brownian motion’s roughness requires a second-order term. Taylor-expand f(t, W(t)):\n\ndf = \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial W} dW + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W^2} (dW)^2 + \\text{smaller terms}\n\nAs dt \\to 0: - dt^2 and dt \\, dW vanish, - (dW)^2 \\approx dt stays significant.\nThis leaves:\n\ndf = \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial W} dW + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W^2} dt\n\nThis is Itô’s Lemma. The extra \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W^2} dt arises because (dW)^2 contributes at the dt scale, capturing the effect of Brownian motion’s curvature.\nSince we have the algebraic heuristic dW^2 = dt, we could in some define everything in terms of powers dW to expand things algebraically and implicitly compute derivative rules.\nThis is precisely the idea behind my blog post on Automatic Stochastic Differentiation, where we use \\mathbb{R}[\\epsilon]/\\epsilon^3 in a similar fashion to dual numbers \\mathbb{R}[\\epsilon]/\\epsilon^2 for automatic differentiation in deterministic calculus.\nIf you haven’t already, I highly recommend checking it out.\n\n\n\nTake f(t, W(t)) = W^2: - \\frac{\\partial f}{\\partial t} = 0, - \\frac{\\partial f}{\\partial W} = 2W, - \\frac{\\partial^2 f}{\\partial W^2} = 2.\nThen:\n\nd(W^2) = 0 \\cdot dt + 2W \\, dW + \\frac{1}{2} \\cdot 2 \\cdot dt = 2W \\, dW + dt\n\nIntegrate from 0 to t (with W(0) = 0):\n\nW(t)^2 = \\int_0^t 2W(s) \\, dW(s) + t\n\nThe t term matches E[W(t)^2] = t, and the integral is a random component with mean 0, consistent with Brownian motion’s properties.\n\n\n\n\n\nItô calculus gives us tools—integrals and a chain rule—to handle Brownian motion. Now we can model systems where randomness and trends coexist, using stochastic differential equations (SDEs). Unlike regular differential equations (e.g., \\frac{dx}{dt} = -kx) that describe smooth dynamics, SDEs blend deterministic behavior with stochastic noise, fitting phenomena like stock prices or diffusing particles.\n\n\nConsider a process influenced by both a predictable trend and random fluctuations:\n\ndX(t) = a(t, X(t)) \\, dt + b(t, X(t)) \\, dW(t)\n\n\nX(t): The evolving quantity (e.g., position or price).\na(t, X(t)) \\, dt: The “drift”—the systematic part, scaled by dt.\nb(t, X(t)) \\, dW(t): The “diffusion”—random perturbations from Brownian motion.\n\nHere, a and b are functions of time and state, and dW(t) = \\sqrt{dt} \\, N(0, 1) brings the noise. Solutions to SDEs aren’t fixed curves but random paths, each run producing a different trajectory with statistical patterns we can study.\n\n\n\nItô’s lemma actually applies to a function f(t, X(t)) and its stochastic derivative df(t, X(t)) for a general dX(t) = b(t,X(t))dt+\\sigma(t,X(t))dW, and this is done through the linearity of the Itô differential (as seen using the \\mathbb{R}[\\epsilon]/\\epsilon^3 formulation).\nConsidering that dX=O(dW), we consider terms up to dX^2=O(dW^2):\n\n\\begin{aligned}\ndf &= f_t \\, dt + f_X \\, dX + \\frac{1}{2}f_{XX} dX^2 \\\\\n&= f_t \\, dt + f_X \\, (b \\, dt+\\sigma \\, dW) + \\frac{1}{2}f_{XX} (b \\, dt+\\sigma \\, dW)^2 \\\\\n&= (f_t + bf_X+\\frac{1}{2}\\sigma^2 f_{XX}) \\, dt + \\sigma f_X \\, dW\n\\end{aligned}\n\nwhich is the general form typically presented.\n\n\n\nThe drift a(t, X) sets the average direction, like a current pushing a particle. The diffusion b(t, X) determines the random jitter’s strength. If b = 0, we get a standard ODE; if a = 0, it’s just scaled Brownian motion. Together, they model systems with both structure and uncertainty.\nTake a simple case:\n\ndX(t) = \\mu \\, dt + \\sigma \\, dW(t)\n\n\n\\mu: Constant drift.\n\\sigma: Constant noise amplitude.\n\nStarting at X(0) = 0, integrate:\n\nX(t) = \\int_0^t \\mu \\, ds + \\int_0^t \\sigma \\, dW(s) = \\mu t + \\sigma W(t)\n\nSince W(t) \\sim N(0, t), we have X(t) \\sim N(\\mu t, \\sigma^2 t)—a process drifting linearly with noise spreading over time. It’s a basic model for things like a stock with steady growth and volatility.\n Code 2D image: Sample SDE path with mu=1.0, sigma=0.5\n\n\n\nFor systems where changes scale with size—like stock prices or certain physical processes—consider geometric Brownian motion (GBM):\n\ndS(t) = \\mu S(t) \\, dt + \\sigma S(t) \\, dW(t)\n\n\nS(t): The state (e.g., stock price).\n\\mu S(t): Proportional drift.\n\\sigma S(t): Proportional noise.\n\nThe percentage change \\frac{dS}{S} = \\mu \\, dt + \\sigma \\, dW has a trend and randomness. To solve, let f = \\ln S: - \\frac{\\partial f}{\\partial t} = 0, - \\frac{\\partial f}{\\partial S} = \\frac{1}{S}, - \\frac{\\partial^2 f}{\\partial S^2} = -\\frac{1}{S^2}.\nUsing Itô’s lemma:\n\nd(\\ln S) = \\frac{1}{S} (\\mu S \\, dt + \\sigma S \\, dW) + \\frac{1}{2} \\left( -\\frac{1}{S^2} \\right) (\\sigma^2 S^2 dt)\n\n\n= \\left( \\mu - \\frac{1}{2} \\sigma^2 \\right) dt + \\sigma \\, dW\n\nIntegrate from 0 to t:\n\n\\ln S(t) - \\ln S(0) = \\left( \\mu - \\frac{1}{2} \\sigma^2 \\right) t + \\sigma W(t)\n\n\nS(t) = S(0) \\exp\\left( \\left( \\mu - \\frac{1}{2} \\sigma^2 \\right) t + \\sigma W(t) \\right)\n\nThe drift is adjusted by -\\frac{1}{2} \\sigma^2 due to the second-order effect of noise, and \\sigma W(t) adds random fluctuations. This form underlies the Black-Scholes model in finance.\n Code 2D image: A sample path of a geometric Brownian motion with parameters μ = 0.15 and σ = 0.2\n Code 3D animation: Geometric Brownian Motion drifting over time\n\n\n\nAnalytical solutions like GBM’s are exceptions. Most SDEs require numerical simulation (e.g., stepping X(t + \\Delta t) = X(t) + \\mu \\Delta t + \\sigma \\sqrt{\\Delta t} \\, N(0, 1)) or statistical analysis via equations like Fokker-Planck. See the appendix for simulation code.\n\n\n\n\n\nRecall Itô’s lemma:\n\ndf = \\left(\\frac{\\partial f}{\\partial t} + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial X^2}\\right) dt + \\frac{\\partial f}{\\partial X} dX\n\nThat second derivative term is pretty annoying to deal with in calculations. Is there a way we can simplify it to the familiar chain rule in regular calculus?\n\ndf = \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial X} dX\n\nThe answer is yes, and it’s called Stratonovich calculus. Let’s explore a bit. First, the deterministic part clearly satisfies the regular chain rule, since we can directly apply it using linearity. The trouble arises in the stochastic part, which we need to analyze. This means we only need to consider a function f(X(t)).\nRemember, for the Itô form, we chose to define the integral by choosing the left endpoint of each interval. In other words, it is this stochastic part that will vary. To delete this second order term, we need to somehow absorb it into the stochastic part by defining some Stratonovich differential, typically denoted by \\circ dW.\nGoing back to our Riemann sum definitions, our degrees of freedom lie in the choice of the evaluation point for each interval:\n\n\\int_{0}^{T} f(X(t)) \\diamond dW = \\lim_{n \\to \\infty} \\sum_{i=0}^{n-1} f(X(t_i) + \\lambda \\Delta X(t_i,t_{i+1})) \\Delta W(t_i, t_{i+1})\n\nwhere \\lambda \\in [0,1] is a constant that linearly interpolates between the left and right endpoints of each interval giving a corresponding differential \\diamond dW, and \\Delta X(t_i,t_{i+1}):=X(t_{i+1})-X(t_i).\nIn the deterministic case, since we always have O(dX^2) \\to 0, it doesn’t matter where we choose the evaluation point. However, in the stochastic case, remember that O(dW^2) \\to O(dt), so we need a more careful choice of evaluation point.\nMathematically, our goal is to define a new stochastic integral that preserves the standard chain rule:\n\ndf = f_X \\circ dX\n\nIn the limiting discrete form, let’s try setting every term equal to each other:\n\nf(X+\\Delta X) - f(X) = f_X(X+\\lambda \\Delta X) \\Delta X\n\nIn other words, our newly defined differential should result in the derivative being a linear approximation of the original function instead of quadratic:\n\n\\frac{f(X+\\Delta X)-f(X)}{\\Delta X} = f_X(X+\\lambda \\Delta X)\n\nBut watch what happens as we take the Taylor expansion on both sides about X (recalling that o(\\Delta X^2)\\to 0):\n\nf_X + \\frac{1}{2}f_{XX}\\Delta X = f_X + \\lambda f_{XX}\\Delta X\n\nComparing coefficients, we wish to set \\lambda = 1/2 to preserve the chain rule. So Stratonovich integrals are defined by the midpoint evaluation rule:\n\n\\begin{aligned}\n\\int_{0}^{T} f(X(t)) \\circ dW &= \\lim_{n \\to \\infty} \\sum_{i=0}^{n-1} f(X(t_i) + \\frac{1}{2} \\Delta X(t_i,t_{i+1})) \\Delta W(t_i, t_{i+1}) \\\\\n&= \\lim_{n \\to \\infty} \\sum_{i=0}^{n-1} f\\left(\\frac{X(t_i)+X(t_{i+1})}{2}\\right) \\Delta W(t_i, t_{i+1}) \\\\\n\\end{aligned}\n\n\n\nThere is a formula to convert the Stratonovich differential into a corresponding Itô SDE that depends on the Itô differential as well as the volatility function \\sigma.\nRecall that Itô’s lemma states that for dX = a dt + b dW:\n\ndf = f_t dt + f_X dX + \\frac{1}{2}f_{XX} dX^2 = (af_t + \\frac{1}{2} b^2 f_{XX}) dt + bf_X dW\n\nIn parallel, we defined Stratonovich’s chain rule to satisfy for dX = \\tilde a dt + \\tilde b \\circ dW:\n\ndf = f_t dt + f_X \\circ dX = (f_t + \\tilde a f_X) dt + \\tilde b f_X \\circ dW\n\nHence, between Itô and Stratonovich SDEs, we have in both cases that the differential is scaled by the volatility function of X and f_X, but the drift function changes. Let’s find a conversion formula between the two.\nSuppose we have:\n\ndX = a dt + b dW = \\tilde a dt + b \\circ dW\n\nThen, our objective is to find \\tilde a in terms of a.\nRecall from the integral definition that b(X) \\circ dW = b(X+\\frac{1}{2}dX) dW. If we Taylor expand around X, we have:\n\nb(X+\\frac{1}{2}dX) dW = b(X)dW + b_X(X)\\frac{1}{2}dX dW + o(dt)\n\nNow, if we plug in dX=a dt + b dW, the first term vanishes, leaving b_X b \\frac{1}{2}dW^2 \\sim \\frac{1}{2}b_X b dt (where the arguments X are left implicit).\nHence:\n\na = \\tilde a + \\frac{1}{2} b_X b.\n\n\n\n\nStratonovich calculus, with its midpoint evaluation rule, adjusts how we handle stochastic integrals compared to Itô’s left-endpoint approach. This shift makes it valuable in certain fields where its properties align with physical systems or simplify calculations. Below are some practical applications, each with a concrete mathematical example.\n\nPhysics with Multiplicative Noise: In physical systems, noise often scales with the state—like a particle in a fluid where random kicks depend on its position. Consider a damped oscillator with position X(t) under state-dependent noise:\n\ndX = -k X \\, dt + \\sigma X \\circ dW\n\nHere, k &gt; 0 is the damping constant, \\sigma is the noise strength, and \\circ dW denotes the Stratonovich differential. Using Stratonovich’s chain rule, for f(X) = \\ln X:\n\nd(\\ln X) = \\frac{1}{X} (-k X \\, dt + \\sigma X \\circ dW) = -k \\, dt + \\sigma \\circ dW\n\nThis integrates to X(t) = X(0) e^{-kt + \\sigma W(t)}, matching the expected exponential decay with noise. Stratonovich fits here because it preserves symmetries in continuous physical processes, unlike Itô, which adds a \\frac{1}{2} \\sigma^2 X \\, dt drift term.\nWong-Zakai Theorem and Smooth Noise: Real-world noise isn’t perfectly white (uncorrelated like dW)—it’s often smoother. The Wong-Zakai theorem shows that approximating smooth noise (e.g., \\eta(t) with correlation time \\epsilon) as \\epsilon \\to 0 yields a Stratonovich SDE. Take a simple system:\n\n\\dot{x} = a x + b x \\eta(t)\n\nAs \\eta(t) \\to white noise, this becomes dX = a X \\, dt + b X \\circ dW. In Stratonovich form, the solution is X(t) = X(0) e^{a t + b W(t)}. This is useful in engineering, like modeling voltage in a circuit with thermal fluctuations, where noise has slight smoothness.\nStochastic Control: In control problems, Stratonovich can simplify dynamics under feedback. Consider a system with control input u(t) and noise:\n\ndX = (a X + u) \\, dt + \\sigma X \\circ dW\n\nFor f(X) = X^2, the Stratonovich rule gives:\n\nd(X^2) = 2X (a X + u) \\, dt + 2X \\cdot \\sigma X \\circ dW = (2a X^2 + 2X u) \\, dt + 2\\sigma X^2 \\circ dW\n\nThe lack of a second-derivative term (unlike Itô’s + \\sigma^2 X^2 dt) aligns with classical control intuition, making it easier to design u(t) for, say, stabilizing a noisy pendulum or a drone in wind.\nBiological Diffusion: In biology, noise can depend on spatial gradients, like protein diffusion across a cell. Model this as:\n\ndX = \\mu \\, dt + \\sigma(X) \\circ dW, \\quad \\sigma(X) = \\sqrt{2D (1 + k X^2)}\n\nwhere D is diffusivity and k adjusts noise with position. Stratonovich ensures the diffusion term reflects physical conservation laws, matching experimental data in systems like bacterial motility better than Itô, which alters the drift.\nNumerical Stability: For simulations, Stratonovich pairs well with midpoint methods. Take dX = -a X \\, dt + \\sigma \\circ dW. A Stratonovich discretization might use:\n\nX_{n+1} = X_n - a \\left(\\frac{X_n + X_{n+1}}{2}\\right) \\Delta t + \\sigma \\Delta W_n\n\nThis implicit scheme leverages the midpoint rule, reducing numerical artifacts in models like chemical kinetics compared to Itô’s explicit steps.\n\nThe choice between Stratonovich and Itô depends on context. Stratonovich suits systems where noise is tied to physical continuity or symmetry, while Itô dominates in finance for its non-anticipating properties. The conversion a = \\tilde{a} + \\frac{1}{2} b b_X lets you switch forms as needed.\n\n\n\n\n\n\n\nAn Intuitive Introduction For Understanding and Solving Stochastic Differential Equations - Chris Rackauckas (2017)\nStochastic analysis - Paul Bourgade (2010)\nAN INTRODUCTION TO STOCHASTIC DIFFERENTIAL EQUATIONS VERSION 1.2 - Lawrence C. Evans (2013)\nStochastic differential equations An introduction with applications - Bernt K. Øksendal (2003)\nWikipedia: Stochastic calculus\nWikipedia: Stochastic differential equation\n\n\n\n\nHere is a list of notation used in this document:\n\n\\binom{n}{k}=\\frac{n!}{k!(n-k)!} is the binomial coefficient\nX: \\Omega \\to \\mathbb{R} is a random variable from a sample space \\Omega to a real number\nP(A) is the probability of event A\nE[X]=\\int_{\\omega \\in \\Omega} X(\\omega) dP(\\omega) is the expected value of X\nN(\\mu, \\sigma^2) is a normal distribution with mean \\mu and variance \\sigma^2\nW(t) is the position of a Brownian motion at time t\n\\Delta W(t_1,t_2) is the displacement of a Brownian motion from time t_1 to time t_2\ndt is an infinitesimal time increment\ndW := \\Delta W(t,t+dt) is an infinitesimal increment of Brownian motion over time\n(dW)^2 \\sim dt denotes that (dW^2) = dt + o(dt) where \\lim_{t \\to 0} \\frac{o(dt)}{dt} = 0, such that (dW)^2 is asymptotically equal to dt in the mean-square limit:\n\n\n\\lim_{dt \\to 0} \\frac{E[(dW)^2-dt]^2}{dt}=0\n\n\nf_t:=\\frac{\\partial f}{\\partial t} is the partial derivative of f with respect to t\nf_xx:=\\frac{\\partial^2 f}{\\partial x^2} is the second order partial derivative of f with respect to x\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nn_values = [5, 10, 25, 50, 100]\np = 0.5\n\n# Individual plots\nfor n in n_values:\n    k = np.arange(0, n + 1)\n    positions = 2 * k - n\n    probs = binom.pmf(k, n, p)\n    \n    plt.figure(figsize=(6, 4))\n    plt.bar(positions, probs, width=1.0, color='skyblue', edgecolor='black')\n    plt.title(f'n = {n}')\n    plt.xlabel('Position (# wins - # losses)')\n    plt.ylabel('Probability')\n    plt.ylim(0, max(probs) * 1.2)\n    plt.savefig(f'random_walk_n_{n}.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\n# Combined plot\nfig, axes = plt.subplots(5, 1, figsize=(8, 12), sharex=True)\nfor i, n in enumerate(n_values):\n    k = np.arange(0, n + 1)\n    positions = 2 * k - n\n    probs = binom.pmf(k, n, p)\n    axes[i].bar(positions, probs, width=1.0, color='skyblue', edgecolor='black')\n    axes[i].set_title(f'n = {n}')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_ylim(0, max(probs) * 1.2)\naxes[-1].set_xlabel('Position (# wins - # losses)')\nplt.tight_layout()\nplt.savefig('random_walk_combined.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate Brownian motion\nnp.random.seed(42)\nt = np.linspace(0, 1, 1000)  # Time from 0 to 1\ndt = t[1] - t[0]\ndW = np.sqrt(dt) * np.random.normal(0, 1, size=len(t)-1)  # Increments\nW = np.concatenate([[0], np.cumsum(dW)])  # Cumulative sum starts at 0\n\n# Plot\nplt.plot(t, W)\nplt.title(\"Sample Brownian Motion Path\")\nplt.xlabel(\"Time t\")\nplt.ylabel(\"W(t)\")\nplt.grid(True)\nplt.show()\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate simple SDE: dX = mu dt + sigma dW\nnp.random.seed(42)\nT = 1.0\nN = 1000\ndt = T / N\nt = np.linspace(0, T, N+1)\nmu, sigma = 1.0, 0.5\nX = np.zeros(N+1)\nfor i in range(N):\n    dW = np.sqrt(dt) * np.random.normal(0, 1)\n    X[i+1] = X[i] + mu * dt + sigma * dW\n\nplt.plot(t, X, label=f\"μ={mu}, σ={sigma}\")\nplt.title(\"Sample Path of dX = μ dt + σ dW\")\nplt.xlabel(\"Time t\")\nplt.ylabel(\"X(t)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate simple SDE: dX = mu dt + sigma dW\nnp.random.seed(42)\n\n# Simulate Geometric Brownian Motion (exact solution)\nT_gbm = 10.0  # Longer time to show exponential nature\nN_gbm = 1000\nt_gbm = np.linspace(0, T_gbm, N_gbm+1)\nS0 = 100.0  # Initial stock price\nmu, sigma = 0.15, 0.2  # Slightly larger for visibility\nS = S0 * np.exp((mu - 0.5 * sigma**2) * t_gbm + sigma * np.sqrt(t_gbm) * np.random.normal(0, 1, N_gbm+1))\n\nplt.figure(figsize=(8, 4))\nplt.plot(t_gbm, S, label=f\"μ={mu}, σ={sigma}\")\nplt.title(\"Sample Path: Geometric Brownian Motion\")\nplt.xlabel(\"Time t\")\nplt.ylabel(\"S(t)\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"gbm_path.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\begin{document}\n\n\\begin{tikzpicture}[scale=0.8]\n    % Add a white background rectangle\n  \\fill[white] (-12, 1) rectangle (10, -5);\n  \n  % Row labels (only once, to the left of the first diagram)\n  \\node[align=right] at (-11, 0) {Row 0};\n  \\node[align=right] at (-11, -1) {Row 1};\n  \\node[align=right] at (-11, -2) {Row 2};\n  \\node[align=right] at (-11, -3) {Row 3};\n\n  % Diagram 1: Path RRL\n  \\node at (-6, 0) {1}; % Row 0\n  \\node at (-7, -1) {1}; % Row 1\n  \\node at (-5, -1) {1};\n  \\node at (-8, -2) {1}; % Row 2\n  \\node at (-6, -2) {2};\n  \\node at (-4, -2) {1};\n  \\node at (-9, -3) {1}; % Row 3\n  \\node at (-7, -3) {3};\n  \\node at (-5, -3) {3};\n  \\node at (-3, -3) {1};\n  \\draw[-&gt;, red, thick] (-6, 0) -- (-5, -1) -- (-4, -2) -- (-5, -3); % RRL\n  \\node at (-6, -4) {Right-Right-Left};\n\n  % Diagram 2: Path RLR\n  \\node at (0, 0) {1}; % Row 0\n  \\node at (-1, -1) {1}; % Row 1\n  \\node at (1, -1) {1};\n  \\node at (-2, -2) {1}; % Row 2\n  \\node at (0, -2) {2};\n  \\node at (2, -2) {1};\n  \\node at (-3, -3) {1}; % Row 3\n  \\node at (-1, -3) {3};\n  \\node at (1, -3) {3};\n  \\node at (3, -3) {1};\n  \\draw[-&gt;, blue, thick] (0, 0) -- (1, -1) -- (0, -2) -- (1, -3); % RLR\n  \\node at (0, -4) {Right-Left-Right};\n\n  % Diagram 3: Path LRR\n  \\node at (6, 0) {1}; % Row 0\n  \\node at (5, -1) {1}; % Row 1\n  \\node at (7, -1) {1};\n  \\node at (4, -2) {1}; % Row 2\n  \\node at (6, -2) {2};\n  \\node at (8, -2) {1};\n  \\node at (3, -3) {1}; % Row 3\n  \\node at (5, -3) {3};\n  \\node at (7, -3) {3};\n  \\node at (9, -3) {1};\n  \\draw[-&gt;, green, thick] (6, 0) -- (5, -1) -- (6, -2) -- (7, -3); % LRR\n  \\node at (6, -4) {Left-Right-Right};\n\\end{tikzpicture}\n\n\\end{document}\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\nimport imageio.v3 as imageio  # using modern imageio v3 API\nimport os\nfrom scipy.special import comb\nfrom scipy.stats import norm\n\n# Create a directory for frames\nos.makedirs('gif_frames', exist_ok=True)\n\n##############################################\n# Part 1: Discrete Binomial Random Walk (N = 15)\n##############################################\n\nN = 15  # total number of steps (kept small for clear discreteness)\nnum_sample_paths = 5  # number of sample paths to overlay\n\n# Simulate a few discrete random walk sample paths\nsample_paths = []\nfor i in range(num_sample_paths):\n    steps = np.random.choice([-1, 1], size=N)\n    path = np.concatenate(([0], np.cumsum(steps)))\n    sample_paths.append(path)\nsample_paths = np.array(sample_paths)  # shape: (num_sample_paths, N+1)\n\nframes = []\nfor t_step in range(N + 1):\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # For each discrete time slice up to the current time, plot the PMF\n    for t in range(t_step + 1):\n        # For a random walk starting at 0, possible positions are -t, -t+2, ..., t\n        x_values = np.arange(-t, t + 1, 2)\n        if t == 0:\n            p_values = np.array([1.0])\n        else:\n            # k = (x + t)/2 gives the number of +1 steps\n            k = (x_values + t) // 2  \n            p_values = comb(t, k) * (0.5 ** t)\n        # Plot the discrete PMF as blue markers (and connect them with a line)\n        ax.scatter(x_values, [t]*len(x_values), p_values, color='blue', s=50)\n        ax.plot(x_values, [t]*len(x_values), p_values, color='blue', alpha=0.5)\n    \n    # Overlay the sample random walk paths (projected at z=0)\n    for sp in sample_paths:\n        ax.plot(sp[:t_step + 1], np.arange(t_step + 1), np.zeros(t_step + 1),\n                'r-o', markersize=5, label='Sample Path' if t_step == 0 else \"\")\n    \n    ax.set_xlabel('Position (x)')\n    ax.set_ylabel('Time (steps)')\n    ax.set_zlabel('Probability')\n    ax.set_title(f'Discrete Binomial Random Walk: Step {t_step}')\n    ax.set_zlim(0, 1.0)\n    ax.view_init(elev=30, azim=-60)\n    \n    frame_path = f'gif_frames/discrete_binomial_{t_step:02d}.png'\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\n# Compute per-frame durations: 0.25 sec for all frames except the last one (2 sec)\ndurations = [0.25] * (len(frames) - 1) + [2.0]\n\n# Write the GIF with variable durations and infinite looping\nimageio.imwrite('discrete_binomial.gif', frames, duration=durations, loop=0)\n\n##############################################\n# Part 2: Discrete Random Walk Normalizing (N = 50)\n##############################################\n\nN = 50  # total number of steps (increased to show gradual convergence)\nnum_sample_paths = 5  # number of sample paths to overlay\n\n# Simulate a few discrete random walk sample paths\nsample_paths = []\nfor i in range(num_sample_paths):\n    steps = np.random.choice([-1, 1], size=N)\n    path = np.concatenate(([0], np.cumsum(steps)))\n    sample_paths.append(path)\nsample_paths = np.array(sample_paths)  # shape: (num_sample_paths, N+1)\n\nframes = []\nfor t_step in range(N + 1):\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # Plot the PMFs for all time slices from 0 to the current step\n    for t in range(t_step + 1):\n        # For a random walk starting at 0, possible positions are -t, -t+2, ..., t\n        x_values = np.arange(-t, t + 1, 2)\n        if t == 0:\n            p_values = np.array([1.0])\n        else:\n            # For each x, number of +1 steps is (x+t)/2\n            k = (x_values + t) // 2\n            p_values = comb(t, k) * (0.5 ** t)\n        \n        # Plot the discrete PMF as blue markers and lines\n        ax.scatter(x_values, [t]*len(x_values), p_values, color='blue', s=50)\n        ax.plot(x_values, [t]*len(x_values), p_values, color='blue', alpha=0.5)\n        \n        # For the current time slice, overlay the normal approximation in red\n        if t == t_step and t &gt; 0:\n            x_cont = np.linspace(-t, t, 200)\n            normal_pdf = norm.pdf(x_cont, 0, np.sqrt(t))\n            ax.plot(x_cont, [t]*len(x_cont), normal_pdf, 'r-', linewidth=2, label='Normal Approx.')\n    \n    # Overlay the sample random walk paths (projected along the z=0 plane)\n    for sp in sample_paths:\n        ax.plot(sp[:t_step + 1], np.arange(t_step + 1), np.zeros(t_step + 1),\n                'g-o', markersize=5, label='Sample Path' if t_step == 0 else \"\")\n    \n    ax.set_xlabel('Position (x)')\n    ax.set_ylabel('Time (steps)')\n    ax.set_zlabel('Probability')\n    ax.set_title(f'Discrete Binomial Random Walk at Step {t_step}')\n    ax.set_zlim(0, 1.0)\n    ax.view_init(elev=30, azim=-60)\n    \n    frame_path = f'gif_frames/discrete_binomial_{t_step:02d}.png'\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\n# Compute per-frame durations: 0.25 sec for all frames except the last one (2 sec)\ndurations = [0.25] * (len(frames) - 1) + [2.0]\n\n# Write the GIF with variable durations and infinite looping\nimageio.imwrite('discrete_binomial_normalizing.gif', frames, duration=durations, loop=0)\n\n\n\n\nNormal distribution sweeping and evolving across time according Brownian motion\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\nfrom scipy.stats import norm\nimport imageio.v3 as imageio  # using modern API\nimport os\n\nos.makedirs('gif_frames', exist_ok=True)\n\n# Parameters for continuous Brownian motion\nnum_frames = 100  # more frames for smoother animation\nt_values = np.linspace(0.1, 5, num_frames)\nx = np.linspace(-5, 5, 200)  # increased resolution\n\nnum_sample_paths = 5\nsample_paths = np.zeros((num_sample_paths, len(t_values)))\ndt_cont = t_values[1] - t_values[0]\nfor i in range(num_sample_paths):\n    increments = np.random.normal(0, np.sqrt(dt_cont), size=len(t_values)-1)\n    sample_paths[i, 1:] = np.cumsum(increments)\n\nframes = []\nfor i, t in enumerate(t_values):\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    mask = t_values &lt;= t\n    T_sub, X_sub = np.meshgrid(t_values[mask], x)\n    P_sub = (1 / np.sqrt(2 * np.pi * T_sub)) * np.exp(-X_sub**2 / (2 * T_sub))\n    ax.plot_surface(X_sub, T_sub, P_sub, cmap='viridis', alpha=0.7, edgecolor='none')\n    \n    for sp in sample_paths:\n        ax.plot(sp[:i+1], t_values[:i+1], np.zeros(i+1), 'r-', marker='o', markersize=3)\n    \n    ax.set_xlabel('Position (x)')\n    ax.set_ylabel('Time (t)')\n    ax.set_zlabel('Density')\n    ax.set_title(f'Continuous Brownian Motion at t = {t:.2f}')\n    ax.view_init(elev=30, azim=-60)\n    \n    frame_path = f'gif_frames/continuous_3d_smooth_t_{t:.2f}.png'\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\nimageio.imwrite('continuous_brownian_3d_smooth.gif', frames, duration=0.1, loop=0)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\nimport imageio.v3 as imageio  # modern API\nimport os\n\nos.makedirs('gif_frames', exist_ok=True)\n\n# Parameters for geometric Brownian motion (GBM)\nS0 = 1.0    # initial stock price\nmu = 0.2    # drift rate (increased for noticeable drift)\nsigma = 0.2 # volatility\n\nnum_frames = 100\nt_values = np.linspace(0.1, 5, num_frames)  # avoid t=0 to prevent singularity in density\nS_range = np.linspace(0.01, 5, 200)         # price range\n\n# Simulate GBM sample paths\nnum_sample_paths = 5\nsample_paths = np.zeros((num_sample_paths, len(t_values)))\ndt = t_values[1] - t_values[0]\nfor i in range(num_sample_paths):\n    increments = np.random.normal(0, np.sqrt(dt), size=len(t_values)-1)\n    W = np.concatenate(([0], np.cumsum(increments)))\n    sample_paths[i] = S0 * np.exp((mu - 0.5 * sigma**2) * t_values + sigma * W)\n\nframes = []\nfor i, t in enumerate(t_values):\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    mask = t_values &lt;= t\n    T_sub, S_sub = np.meshgrid(t_values[mask], S_range)\n    P_sub = (1 / (S_sub * sigma * np.sqrt(2 * np.pi * T_sub))) * \\\n            np.exp(- (np.log(S_sub / S0) - (mu - 0.5 * sigma**2) * T_sub)**2 / (2 * sigma**2 * T_sub))\n    ax.plot_surface(S_sub, T_sub, P_sub, cmap='viridis', alpha=0.7, edgecolor='none')\n    \n    for sp in sample_paths:\n        ax.plot(sp[:i+1], t_values[:i+1], np.zeros(i+1), 'r-', marker='o', markersize=3)\n    \n    ax.set_xlabel('Stock Price S')\n    ax.set_ylabel('Time t')\n    ax.set_zlabel('Density')\n    ax.set_title(f'Geometric Brownian Motion at t = {t:.2f}')\n    ax.view_init(elev=30, azim=-60)\n    \n    frame_path = f'gif_frames/geometric_brownian_drifted_3d_t_{t:.2f}.png'\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\nimageio.imwrite('geometric_brownian_drifted_3d.gif', frames, duration=0.1)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport imageio.v3 as imageio  # modern ImageIO v3 API\nimport os\nfrom scipy.special import comb\n\n# Create a directory for frames\nos.makedirs('gif_frames', exist_ok=True)\n\n# 1. Continuous Brownian Motion with Sample Paths\n\n# Define time values and x range for density\nt_values = np.linspace(0.1, 5, 50)  # Times from 0.1 to 5\nx = np.linspace(-5, 5, 100)          # Range of x values\n\n# Simulate a few sample Brownian motion paths\nnum_sample_paths = 5\ndt_cont = t_values[1] - t_values[0]  # constant time step (~0.1)\nsample_paths = np.zeros((num_sample_paths, len(t_values)))\nsample_paths[:, 0] = 0\nincrements = np.random.normal(0, np.sqrt(dt_cont), size=(num_sample_paths, len(t_values)-1))\nsample_paths[:, 1:] = np.cumsum(increments, axis=1)\n\nframes = []\nfor i, t in enumerate(t_values):\n    p = (1 / np.sqrt(2 * np.pi * t)) * np.exp(-x**2 / (2 * t))\n    \n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, p, 'b-', label=f't = {t:.2f}')\n    plt.title('Brownian Motion Distribution')\n    plt.xlabel('x')\n    plt.ylabel('Density p(x,t)')\n    plt.ylim(0, 0.8)\n    plt.legend()\n    plt.grid(True)\n    \n    plt.subplot(1, 2, 2)\n    for sp in sample_paths:\n        plt.plot(t_values[:i+1], sp[:i+1], '-o', markersize=3)\n    plt.title('Sample Brownian Paths')\n    plt.xlabel('Time')\n    plt.ylabel('Position')\n    plt.xlim(0, 5)\n    plt.grid(True)\n    \n    frame_path = f'gif_frames/continuous_t_{t:.2f}.png'\n    plt.tight_layout()\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\n# Save the continuous Brownian motion GIF\n# (duration in seconds per frame; adjust as desired)\nimageio.imwrite('continuous_brownian.gif', frames, duration=0.1)\n\n# 2. Discrete Random Walk with Sample Paths\n\ndef simulate_random_walk(dt, T, num_paths):\n    \"\"\"Simulate random walk paths with step size sqrt(dt).\"\"\"\n    n_steps = int(T / dt)\n    positions = np.zeros((num_paths, n_steps + 1))\n    for i in range(num_paths):\n        increments = np.random.choice([-1, 1], size=n_steps) * np.sqrt(dt)\n        positions[i, 1:] = np.cumsum(increments)\n    return positions\n\ndt = 0.01  # Step size\nT = 5.0    # Total time\nnum_paths = 10000  # For histogram\ntimes = np.arange(0, T + dt, dt)\npositions = simulate_random_walk(dt, T, num_paths)\nsample_indices = np.arange(5)\n\nframes = []\nfor i, t in enumerate(times):\n    if i % 10 == 0:  # Use every 10th frame for the GIF\n        current_positions = positions[:, i]\n        x_vals = np.linspace(-5, 5, 100)\n        p_theoretical = norm.pdf(x_vals, 0, np.sqrt(t) if t &gt; 0 else 1e-5)\n        \n        plt.figure(figsize=(12, 4))\n        plt.subplot(1, 2, 1)\n        plt.hist(current_positions, bins=50, density=True, alpha=0.6, label=f't = {t:.2f}')\n        plt.plot(x_vals, p_theoretical, 'r-', label='N(0,t)')\n        plt.title('Discrete Random Walk Distribution')\n        plt.xlabel('Position')\n        plt.ylabel('Density')\n        plt.ylim(0, 0.8)\n        plt.legend()\n        plt.grid(True)\n        \n        plt.subplot(1, 2, 2)\n        for idx in sample_indices:\n            plt.plot(times[:i+1], positions[idx, :i+1], '-o', markersize=3)\n        plt.title('Sample Random Walk Paths')\n        plt.xlabel('Time')\n        plt.ylabel('Position')\n        plt.xlim(0, T)\n        plt.grid(True)\n        \n        frame_path = f'gif_frames/discrete_t_{t:.2f}.png'\n        plt.tight_layout()\n        plt.savefig(frame_path)\n        plt.close()\n        frames.append(imageio.imread(frame_path))\n\n# Save the discrete random walk GIF with infinite looping\nimageio.imwrite('discrete_random_walk.gif', frames, duration=0.1, loop=0)"
  },
  {
    "objectID": "gleanings/StochasticCalculus/index.html#appendix",
    "href": "gleanings/StochasticCalculus/index.html#appendix",
    "title": "Stochastic Calculus - Fundamentals",
    "section": "",
    "text": "An Intuitive Introduction For Understanding and Solving Stochastic Differential Equations - Chris Rackauckas (2017)\nStochastic analysis - Paul Bourgade (2010)\nAN INTRODUCTION TO STOCHASTIC DIFFERENTIAL EQUATIONS VERSION 1.2 - Lawrence C. Evans (2013)\nStochastic differential equations An introduction with applications - Bernt K. Øksendal (2003)\nWikipedia: Stochastic calculus\nWikipedia: Stochastic differential equation\n\n\n\n\nHere is a list of notation used in this document:\n\n\\binom{n}{k}=\\frac{n!}{k!(n-k)!} is the binomial coefficient\nX: \\Omega \\to \\mathbb{R} is a random variable from a sample space \\Omega to a real number\nP(A) is the probability of event A\nE[X]=\\int_{\\omega \\in \\Omega} X(\\omega) dP(\\omega) is the expected value of X\nN(\\mu, \\sigma^2) is a normal distribution with mean \\mu and variance \\sigma^2\nW(t) is the position of a Brownian motion at time t\n\\Delta W(t_1,t_2) is the displacement of a Brownian motion from time t_1 to time t_2\ndt is an infinitesimal time increment\ndW := \\Delta W(t,t+dt) is an infinitesimal increment of Brownian motion over time\n(dW)^2 \\sim dt denotes that (dW^2) = dt + o(dt) where \\lim_{t \\to 0} \\frac{o(dt)}{dt} = 0, such that (dW)^2 is asymptotically equal to dt in the mean-square limit:\n\n\n\\lim_{dt \\to 0} \\frac{E[(dW)^2-dt]^2}{dt}=0\n\n\nf_t:=\\frac{\\partial f}{\\partial t} is the partial derivative of f with respect to t\nf_xx:=\\frac{\\partial^2 f}{\\partial x^2} is the second order partial derivative of f with respect to x\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nn_values = [5, 10, 25, 50, 100]\np = 0.5\n\n# Individual plots\nfor n in n_values:\n    k = np.arange(0, n + 1)\n    positions = 2 * k - n\n    probs = binom.pmf(k, n, p)\n    \n    plt.figure(figsize=(6, 4))\n    plt.bar(positions, probs, width=1.0, color='skyblue', edgecolor='black')\n    plt.title(f'n = {n}')\n    plt.xlabel('Position (# wins - # losses)')\n    plt.ylabel('Probability')\n    plt.ylim(0, max(probs) * 1.2)\n    plt.savefig(f'random_walk_n_{n}.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\n# Combined plot\nfig, axes = plt.subplots(5, 1, figsize=(8, 12), sharex=True)\nfor i, n in enumerate(n_values):\n    k = np.arange(0, n + 1)\n    positions = 2 * k - n\n    probs = binom.pmf(k, n, p)\n    axes[i].bar(positions, probs, width=1.0, color='skyblue', edgecolor='black')\n    axes[i].set_title(f'n = {n}')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_ylim(0, max(probs) * 1.2)\naxes[-1].set_xlabel('Position (# wins - # losses)')\nplt.tight_layout()\nplt.savefig('random_walk_combined.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate Brownian motion\nnp.random.seed(42)\nt = np.linspace(0, 1, 1000)  # Time from 0 to 1\ndt = t[1] - t[0]\ndW = np.sqrt(dt) * np.random.normal(0, 1, size=len(t)-1)  # Increments\nW = np.concatenate([[0], np.cumsum(dW)])  # Cumulative sum starts at 0\n\n# Plot\nplt.plot(t, W)\nplt.title(\"Sample Brownian Motion Path\")\nplt.xlabel(\"Time t\")\nplt.ylabel(\"W(t)\")\nplt.grid(True)\nplt.show()\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate simple SDE: dX = mu dt + sigma dW\nnp.random.seed(42)\nT = 1.0\nN = 1000\ndt = T / N\nt = np.linspace(0, T, N+1)\nmu, sigma = 1.0, 0.5\nX = np.zeros(N+1)\nfor i in range(N):\n    dW = np.sqrt(dt) * np.random.normal(0, 1)\n    X[i+1] = X[i] + mu * dt + sigma * dW\n\nplt.plot(t, X, label=f\"μ={mu}, σ={sigma}\")\nplt.title(\"Sample Path of dX = μ dt + σ dW\")\nplt.xlabel(\"Time t\")\nplt.ylabel(\"X(t)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate simple SDE: dX = mu dt + sigma dW\nnp.random.seed(42)\n\n# Simulate Geometric Brownian Motion (exact solution)\nT_gbm = 10.0  # Longer time to show exponential nature\nN_gbm = 1000\nt_gbm = np.linspace(0, T_gbm, N_gbm+1)\nS0 = 100.0  # Initial stock price\nmu, sigma = 0.15, 0.2  # Slightly larger for visibility\nS = S0 * np.exp((mu - 0.5 * sigma**2) * t_gbm + sigma * np.sqrt(t_gbm) * np.random.normal(0, 1, N_gbm+1))\n\nplt.figure(figsize=(8, 4))\nplt.plot(t_gbm, S, label=f\"μ={mu}, σ={sigma}\")\nplt.title(\"Sample Path: Geometric Brownian Motion\")\nplt.xlabel(\"Time t\")\nplt.ylabel(\"S(t)\")\nplt.legend()\nplt.grid(True)\nplt.savefig(\"gbm_path.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\begin{document}\n\n\\begin{tikzpicture}[scale=0.8]\n    % Add a white background rectangle\n  \\fill[white] (-12, 1) rectangle (10, -5);\n  \n  % Row labels (only once, to the left of the first diagram)\n  \\node[align=right] at (-11, 0) {Row 0};\n  \\node[align=right] at (-11, -1) {Row 1};\n  \\node[align=right] at (-11, -2) {Row 2};\n  \\node[align=right] at (-11, -3) {Row 3};\n\n  % Diagram 1: Path RRL\n  \\node at (-6, 0) {1}; % Row 0\n  \\node at (-7, -1) {1}; % Row 1\n  \\node at (-5, -1) {1};\n  \\node at (-8, -2) {1}; % Row 2\n  \\node at (-6, -2) {2};\n  \\node at (-4, -2) {1};\n  \\node at (-9, -3) {1}; % Row 3\n  \\node at (-7, -3) {3};\n  \\node at (-5, -3) {3};\n  \\node at (-3, -3) {1};\n  \\draw[-&gt;, red, thick] (-6, 0) -- (-5, -1) -- (-4, -2) -- (-5, -3); % RRL\n  \\node at (-6, -4) {Right-Right-Left};\n\n  % Diagram 2: Path RLR\n  \\node at (0, 0) {1}; % Row 0\n  \\node at (-1, -1) {1}; % Row 1\n  \\node at (1, -1) {1};\n  \\node at (-2, -2) {1}; % Row 2\n  \\node at (0, -2) {2};\n  \\node at (2, -2) {1};\n  \\node at (-3, -3) {1}; % Row 3\n  \\node at (-1, -3) {3};\n  \\node at (1, -3) {3};\n  \\node at (3, -3) {1};\n  \\draw[-&gt;, blue, thick] (0, 0) -- (1, -1) -- (0, -2) -- (1, -3); % RLR\n  \\node at (0, -4) {Right-Left-Right};\n\n  % Diagram 3: Path LRR\n  \\node at (6, 0) {1}; % Row 0\n  \\node at (5, -1) {1}; % Row 1\n  \\node at (7, -1) {1};\n  \\node at (4, -2) {1}; % Row 2\n  \\node at (6, -2) {2};\n  \\node at (8, -2) {1};\n  \\node at (3, -3) {1}; % Row 3\n  \\node at (5, -3) {3};\n  \\node at (7, -3) {3};\n  \\node at (9, -3) {1};\n  \\draw[-&gt;, green, thick] (6, 0) -- (5, -1) -- (6, -2) -- (7, -3); % LRR\n  \\node at (6, -4) {Left-Right-Right};\n\\end{tikzpicture}\n\n\\end{document}\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\nimport imageio.v3 as imageio  # using modern imageio v3 API\nimport os\nfrom scipy.special import comb\nfrom scipy.stats import norm\n\n# Create a directory for frames\nos.makedirs('gif_frames', exist_ok=True)\n\n##############################################\n# Part 1: Discrete Binomial Random Walk (N = 15)\n##############################################\n\nN = 15  # total number of steps (kept small for clear discreteness)\nnum_sample_paths = 5  # number of sample paths to overlay\n\n# Simulate a few discrete random walk sample paths\nsample_paths = []\nfor i in range(num_sample_paths):\n    steps = np.random.choice([-1, 1], size=N)\n    path = np.concatenate(([0], np.cumsum(steps)))\n    sample_paths.append(path)\nsample_paths = np.array(sample_paths)  # shape: (num_sample_paths, N+1)\n\nframes = []\nfor t_step in range(N + 1):\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # For each discrete time slice up to the current time, plot the PMF\n    for t in range(t_step + 1):\n        # For a random walk starting at 0, possible positions are -t, -t+2, ..., t\n        x_values = np.arange(-t, t + 1, 2)\n        if t == 0:\n            p_values = np.array([1.0])\n        else:\n            # k = (x + t)/2 gives the number of +1 steps\n            k = (x_values + t) // 2  \n            p_values = comb(t, k) * (0.5 ** t)\n        # Plot the discrete PMF as blue markers (and connect them with a line)\n        ax.scatter(x_values, [t]*len(x_values), p_values, color='blue', s=50)\n        ax.plot(x_values, [t]*len(x_values), p_values, color='blue', alpha=0.5)\n    \n    # Overlay the sample random walk paths (projected at z=0)\n    for sp in sample_paths:\n        ax.plot(sp[:t_step + 1], np.arange(t_step + 1), np.zeros(t_step + 1),\n                'r-o', markersize=5, label='Sample Path' if t_step == 0 else \"\")\n    \n    ax.set_xlabel('Position (x)')\n    ax.set_ylabel('Time (steps)')\n    ax.set_zlabel('Probability')\n    ax.set_title(f'Discrete Binomial Random Walk: Step {t_step}')\n    ax.set_zlim(0, 1.0)\n    ax.view_init(elev=30, azim=-60)\n    \n    frame_path = f'gif_frames/discrete_binomial_{t_step:02d}.png'\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\n# Compute per-frame durations: 0.25 sec for all frames except the last one (2 sec)\ndurations = [0.25] * (len(frames) - 1) + [2.0]\n\n# Write the GIF with variable durations and infinite looping\nimageio.imwrite('discrete_binomial.gif', frames, duration=durations, loop=0)\n\n##############################################\n# Part 2: Discrete Random Walk Normalizing (N = 50)\n##############################################\n\nN = 50  # total number of steps (increased to show gradual convergence)\nnum_sample_paths = 5  # number of sample paths to overlay\n\n# Simulate a few discrete random walk sample paths\nsample_paths = []\nfor i in range(num_sample_paths):\n    steps = np.random.choice([-1, 1], size=N)\n    path = np.concatenate(([0], np.cumsum(steps)))\n    sample_paths.append(path)\nsample_paths = np.array(sample_paths)  # shape: (num_sample_paths, N+1)\n\nframes = []\nfor t_step in range(N + 1):\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # Plot the PMFs for all time slices from 0 to the current step\n    for t in range(t_step + 1):\n        # For a random walk starting at 0, possible positions are -t, -t+2, ..., t\n        x_values = np.arange(-t, t + 1, 2)\n        if t == 0:\n            p_values = np.array([1.0])\n        else:\n            # For each x, number of +1 steps is (x+t)/2\n            k = (x_values + t) // 2\n            p_values = comb(t, k) * (0.5 ** t)\n        \n        # Plot the discrete PMF as blue markers and lines\n        ax.scatter(x_values, [t]*len(x_values), p_values, color='blue', s=50)\n        ax.plot(x_values, [t]*len(x_values), p_values, color='blue', alpha=0.5)\n        \n        # For the current time slice, overlay the normal approximation in red\n        if t == t_step and t &gt; 0:\n            x_cont = np.linspace(-t, t, 200)\n            normal_pdf = norm.pdf(x_cont, 0, np.sqrt(t))\n            ax.plot(x_cont, [t]*len(x_cont), normal_pdf, 'r-', linewidth=2, label='Normal Approx.')\n    \n    # Overlay the sample random walk paths (projected along the z=0 plane)\n    for sp in sample_paths:\n        ax.plot(sp[:t_step + 1], np.arange(t_step + 1), np.zeros(t_step + 1),\n                'g-o', markersize=5, label='Sample Path' if t_step == 0 else \"\")\n    \n    ax.set_xlabel('Position (x)')\n    ax.set_ylabel('Time (steps)')\n    ax.set_zlabel('Probability')\n    ax.set_title(f'Discrete Binomial Random Walk at Step {t_step}')\n    ax.set_zlim(0, 1.0)\n    ax.view_init(elev=30, azim=-60)\n    \n    frame_path = f'gif_frames/discrete_binomial_{t_step:02d}.png'\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\n# Compute per-frame durations: 0.25 sec for all frames except the last one (2 sec)\ndurations = [0.25] * (len(frames) - 1) + [2.0]\n\n# Write the GIF with variable durations and infinite looping\nimageio.imwrite('discrete_binomial_normalizing.gif', frames, duration=durations, loop=0)\n\n\n\n\nNormal distribution sweeping and evolving across time according Brownian motion\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\nfrom scipy.stats import norm\nimport imageio.v3 as imageio  # using modern API\nimport os\n\nos.makedirs('gif_frames', exist_ok=True)\n\n# Parameters for continuous Brownian motion\nnum_frames = 100  # more frames for smoother animation\nt_values = np.linspace(0.1, 5, num_frames)\nx = np.linspace(-5, 5, 200)  # increased resolution\n\nnum_sample_paths = 5\nsample_paths = np.zeros((num_sample_paths, len(t_values)))\ndt_cont = t_values[1] - t_values[0]\nfor i in range(num_sample_paths):\n    increments = np.random.normal(0, np.sqrt(dt_cont), size=len(t_values)-1)\n    sample_paths[i, 1:] = np.cumsum(increments)\n\nframes = []\nfor i, t in enumerate(t_values):\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    mask = t_values &lt;= t\n    T_sub, X_sub = np.meshgrid(t_values[mask], x)\n    P_sub = (1 / np.sqrt(2 * np.pi * T_sub)) * np.exp(-X_sub**2 / (2 * T_sub))\n    ax.plot_surface(X_sub, T_sub, P_sub, cmap='viridis', alpha=0.7, edgecolor='none')\n    \n    for sp in sample_paths:\n        ax.plot(sp[:i+1], t_values[:i+1], np.zeros(i+1), 'r-', marker='o', markersize=3)\n    \n    ax.set_xlabel('Position (x)')\n    ax.set_ylabel('Time (t)')\n    ax.set_zlabel('Density')\n    ax.set_title(f'Continuous Brownian Motion at t = {t:.2f}')\n    ax.view_init(elev=30, azim=-60)\n    \n    frame_path = f'gif_frames/continuous_3d_smooth_t_{t:.2f}.png'\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\nimageio.imwrite('continuous_brownian_3d_smooth.gif', frames, duration=0.1, loop=0)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\nimport imageio.v3 as imageio  # modern API\nimport os\n\nos.makedirs('gif_frames', exist_ok=True)\n\n# Parameters for geometric Brownian motion (GBM)\nS0 = 1.0    # initial stock price\nmu = 0.2    # drift rate (increased for noticeable drift)\nsigma = 0.2 # volatility\n\nnum_frames = 100\nt_values = np.linspace(0.1, 5, num_frames)  # avoid t=0 to prevent singularity in density\nS_range = np.linspace(0.01, 5, 200)         # price range\n\n# Simulate GBM sample paths\nnum_sample_paths = 5\nsample_paths = np.zeros((num_sample_paths, len(t_values)))\ndt = t_values[1] - t_values[0]\nfor i in range(num_sample_paths):\n    increments = np.random.normal(0, np.sqrt(dt), size=len(t_values)-1)\n    W = np.concatenate(([0], np.cumsum(increments)))\n    sample_paths[i] = S0 * np.exp((mu - 0.5 * sigma**2) * t_values + sigma * W)\n\nframes = []\nfor i, t in enumerate(t_values):\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    mask = t_values &lt;= t\n    T_sub, S_sub = np.meshgrid(t_values[mask], S_range)\n    P_sub = (1 / (S_sub * sigma * np.sqrt(2 * np.pi * T_sub))) * \\\n            np.exp(- (np.log(S_sub / S0) - (mu - 0.5 * sigma**2) * T_sub)**2 / (2 * sigma**2 * T_sub))\n    ax.plot_surface(S_sub, T_sub, P_sub, cmap='viridis', alpha=0.7, edgecolor='none')\n    \n    for sp in sample_paths:\n        ax.plot(sp[:i+1], t_values[:i+1], np.zeros(i+1), 'r-', marker='o', markersize=3)\n    \n    ax.set_xlabel('Stock Price S')\n    ax.set_ylabel('Time t')\n    ax.set_zlabel('Density')\n    ax.set_title(f'Geometric Brownian Motion at t = {t:.2f}')\n    ax.view_init(elev=30, azim=-60)\n    \n    frame_path = f'gif_frames/geometric_brownian_drifted_3d_t_{t:.2f}.png'\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\nimageio.imwrite('geometric_brownian_drifted_3d.gif', frames, duration=0.1)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport imageio.v3 as imageio  # modern ImageIO v3 API\nimport os\nfrom scipy.special import comb\n\n# Create a directory for frames\nos.makedirs('gif_frames', exist_ok=True)\n\n# 1. Continuous Brownian Motion with Sample Paths\n\n# Define time values and x range for density\nt_values = np.linspace(0.1, 5, 50)  # Times from 0.1 to 5\nx = np.linspace(-5, 5, 100)          # Range of x values\n\n# Simulate a few sample Brownian motion paths\nnum_sample_paths = 5\ndt_cont = t_values[1] - t_values[0]  # constant time step (~0.1)\nsample_paths = np.zeros((num_sample_paths, len(t_values)))\nsample_paths[:, 0] = 0\nincrements = np.random.normal(0, np.sqrt(dt_cont), size=(num_sample_paths, len(t_values)-1))\nsample_paths[:, 1:] = np.cumsum(increments, axis=1)\n\nframes = []\nfor i, t in enumerate(t_values):\n    p = (1 / np.sqrt(2 * np.pi * t)) * np.exp(-x**2 / (2 * t))\n    \n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, p, 'b-', label=f't = {t:.2f}')\n    plt.title('Brownian Motion Distribution')\n    plt.xlabel('x')\n    plt.ylabel('Density p(x,t)')\n    plt.ylim(0, 0.8)\n    plt.legend()\n    plt.grid(True)\n    \n    plt.subplot(1, 2, 2)\n    for sp in sample_paths:\n        plt.plot(t_values[:i+1], sp[:i+1], '-o', markersize=3)\n    plt.title('Sample Brownian Paths')\n    plt.xlabel('Time')\n    plt.ylabel('Position')\n    plt.xlim(0, 5)\n    plt.grid(True)\n    \n    frame_path = f'gif_frames/continuous_t_{t:.2f}.png'\n    plt.tight_layout()\n    plt.savefig(frame_path)\n    plt.close()\n    frames.append(imageio.imread(frame_path))\n\n# Save the continuous Brownian motion GIF\n# (duration in seconds per frame; adjust as desired)\nimageio.imwrite('continuous_brownian.gif', frames, duration=0.1)\n\n# 2. Discrete Random Walk with Sample Paths\n\ndef simulate_random_walk(dt, T, num_paths):\n    \"\"\"Simulate random walk paths with step size sqrt(dt).\"\"\"\n    n_steps = int(T / dt)\n    positions = np.zeros((num_paths, n_steps + 1))\n    for i in range(num_paths):\n        increments = np.random.choice([-1, 1], size=n_steps) * np.sqrt(dt)\n        positions[i, 1:] = np.cumsum(increments)\n    return positions\n\ndt = 0.01  # Step size\nT = 5.0    # Total time\nnum_paths = 10000  # For histogram\ntimes = np.arange(0, T + dt, dt)\npositions = simulate_random_walk(dt, T, num_paths)\nsample_indices = np.arange(5)\n\nframes = []\nfor i, t in enumerate(times):\n    if i % 10 == 0:  # Use every 10th frame for the GIF\n        current_positions = positions[:, i]\n        x_vals = np.linspace(-5, 5, 100)\n        p_theoretical = norm.pdf(x_vals, 0, np.sqrt(t) if t &gt; 0 else 1e-5)\n        \n        plt.figure(figsize=(12, 4))\n        plt.subplot(1, 2, 1)\n        plt.hist(current_positions, bins=50, density=True, alpha=0.6, label=f't = {t:.2f}')\n        plt.plot(x_vals, p_theoretical, 'r-', label='N(0,t)')\n        plt.title('Discrete Random Walk Distribution')\n        plt.xlabel('Position')\n        plt.ylabel('Density')\n        plt.ylim(0, 0.8)\n        plt.legend()\n        plt.grid(True)\n        \n        plt.subplot(1, 2, 2)\n        for idx in sample_indices:\n            plt.plot(times[:i+1], positions[idx, :i+1], '-o', markersize=3)\n        plt.title('Sample Random Walk Paths')\n        plt.xlabel('Time')\n        plt.ylabel('Position')\n        plt.xlim(0, T)\n        plt.grid(True)\n        \n        frame_path = f'gif_frames/discrete_t_{t:.2f}.png'\n        plt.tight_layout()\n        plt.savefig(frame_path)\n        plt.close()\n        frames.append(imageio.imread(frame_path))\n\n# Save the discrete random walk GIF with infinite looping\nimageio.imwrite('discrete_random_walk.gif', frames, duration=0.1, loop=0)"
  }
]