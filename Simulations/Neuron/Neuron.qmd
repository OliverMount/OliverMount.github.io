# NEURON simulator

[NEURON simulator](https://nrn.readthedocs.io/en/8.2.6/) excels at simulating complex, multi-compartmental models with detailed biophysics. The NEURON simulator is widely used in the simulation of detailed neural mechanisms and networks of neurons in the neuroscience literature. The original programming language used to code the NEURON is Higher Order Calculator (HOC). NEURON may also be programmed in Python, which we will explore more in this series of writings.

## Installation of NEURON with GPU support (CoreNEURON)

### 1. Clone the latest version of NEURON

```{bash}
git clone https://github.com/neuronsimulator/nrn
cd nrn
```

### 2. Build

```{bash}
mkdir build
cd build
```

### 3. Load the modules

In normal Linux systems, the modules are in the path `/usr/share/modules/modulefiles`. If the directory does not exists, then install using

```{bash}

sudo apt-get install environment-modules

# and configure the ~/.bashrc with 
source /etc/profile.d/modules.sh

```

Load the following modules

```{bash}
module load  openmpi python cmake nvidia-hpc-sdk/25.1 cuda  
```

If they are not loading make sure the following files are available in `/usr/share/modules/modulefiles`

1.  openmpi
2.  python
3.  cmake
4.  nvidia-hpc-sdk/25.1
5.  cuda

For example, the content of the file in nvidia-hpc-sdk/25.1 should read as

```{bash}
#%Module1.0 

proc ModulesHelp { } { puts stderr "NVIDIA HPC SDK 25.1" } 
module-whatis "NVIDIA HPC SDK 25.1"

set HPC_SDK /opt/nvidia/hpc_sdk/Linux_x86_64/25.1 
prepend-path PATH $HPC_SDK/compilers/bin 
prepend-path LD_LIBRARY_PATH $HPC_SDK/compilers/lib

```

#%Module1.0 proc ModulesHelp { } { puts stderr "NVIDIA HPC SDK 25.1" } module-whatis "NVIDIA HPC SDK 25.1"

set HPC_SDK /opt/nvidia/hpc_sdk/Linux_x86_64/25.1 prepend-path PATH \$HPC_SDK/compilers/bin prepend-path LD_LIBRARY_PATH \$HPC_SDK/compilers/lib

### 4. Compile

Following compilation is `for GPU architecture of 61`

```{bash}
cmake .. \
      -DNRN_ENABLE_CORENEURON=ON \
      -DCORENRN_ENABLE_GPU=ON \
      -DNRN_ENABLE_INTERVIEWS=OFF \
      -DNRN_ENABLE_RX3D=OFF \
      -DCMAKE_INSTALL_PREFIX=$HOME/install \
      -DCMAKE_C_COMPILER=nvc \
      -DCMAKE_CXX_COMPILER=nvc++ \
      -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu \
      -DCMAKE_CUDA_ARCHITECTURES="61" \  # need to know this
      -DCMAKE_EXE_LINKER_FLAGS: "-cuda -gpu=cuda12.6,lineinfo,cc61 -acc" # not necessary
      -DCMAKE_CXX_FLAGS="-O3 -g" \
      -DCMAKE_C_FLAGS="-O3 -g" \
      -DCMAKE_BUILD_TYPE=Custom
```

Check the CUDA_ARCHITECTURES via (nvidia-smi; assuming it is installed)

```{bash}
nvidia-smi --query-gpu=compute_cap --format=csv
```

### 7. Install (make)

```{bash}

make -j 
make install
```

### 8. Export Python PATH variables to \~/.bashrc

```{bash}
  
export PATH=$HOME/install/bin:$PATH  # assuming $HOME/install is the cmake installtion directory (see 5-th line of cmake in Step 4.)

export PYTHONPATH=$HOME/install/lib/python:$PYTHONPATH
```

### 9. Ringtest

Best way to check if the installation is working properly is by running the ring test as in

```{bash}
git clone https://github.com/nrnhines/ringtest.git
cd ringtest

nrnivmodl -coreneuron mod

# in any NEURON code add the following lines for CoreNEURON support (python files only)

h.cvode.cache_efficient(1)
if use_coreneuron:
    from neuron import coreneuron
    coreneuron.enable = True
    coreneuron.gpu = coreneuron_gpu
    
    
#run the three performance test 

# NEURON CPU Run
mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64

# CoreNEURON CPU Run
mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron

# CoreNEURON GPU Run
mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron -gpu
```

# 

## Batch files for running in cpu

```{bash}
#!/bin/bash
#SBATCH --job-name=rt_cpu       # Job name
#SBATCH --output=rt_cpu.out     # Standard output and error log
#SBATCH --error=rt_cpu.err      # Error log file                                                                                                     
#SBATCH --ntasks=1                # Number of MPI processes
#SBATCH --time=00:20:00           # Time limit (HH:MM:SS)
#SBATCH --partition=olaf_c_core       # Partition name (adjust as needed)

module purge
module load gcc/12.2.0 openmpi/4.1.1-Rocky python/.3.12.3

module load /opt/ibs_lib/modulefiles/libraries/.cuda/25.1
module load /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1

# Run the MPI program

#mpiexec -n 2 ./x86_64/special -mpi -python ringtest.py -tstop 100
#mpiexec -n 2 ./x86_64/special -mpi -python ringtest.py -tstop 100 -coreneuron

# NEURON CPU Run ( in partition olaf_c_core ) 
#mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64

# CoreNEURON CPU Run ( in partition olaf_c_core ) 
mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron
```

## Batch files for running in GPU

```{bash}
#!/bin/bash
#SBATCH --job-name=ringtest_gpu
#SBATCH --output=ringtest_gpu.out
#SBATCH --error=ringtest_gpu.err
#SBATCH --partition=AIP
#SBATCH --gpus-per-node=1
#SBATCH --ntasks=1
#SBATCH --time=01:00:00

# Load necessary modules (adjust as needed for your system)
module purge
module load gcc/12.2.0 openmpi/4.1.1-Rocky python/.3.12.3

module load /opt/ibs_lib/modulefiles/libraries/.cuda/25.1
module load /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1

# Run the CoreNEURON simulation
mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron -gpu

```

## Batch files for running in multi-node GPU

```{bash}
#!/bin/bash
#SBATCH --job-name=ringtest_multi_gpu
#SBATCH --output=ringtest_multi_gpu_%j.out
#SBATCH --error=ringtest_multi_gpu_%j.err
#SBATCH --partition=AIP
#SBATCH --nodes=2               # Number of nodes
#SBATCH --gpus-per-node=2       # GPUs per node (adjust based on node GPU capacity)
#SBATCH --ntasks-per-node=2     # MPI tasks per node (matches GPUs-per-node)
#SBATCH --time=01:00:00
#SBATCH --exclusive             # Request exclusive node access

# Load modules (confirm paths match your cluster's setup)
module purge
module load gcc/12.2.0 \
            openmpi/4.1.1-Rocky \
            python/3.12.3 \
            /opt/ibs_lib/modulefiles/libraries/cuda/25.1 \
            /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1

# Ensure CUDA and MPI are visible
export CUDA_VISIBLE_DEVICES=0,1  # Map GPUs per node (adjust if needed)
export OMP_NUM_THREADS=1         # Disable threading unless required

# Run with MPI across nodes
mpiexec --bind-to none --map-by node \
    -n $((SLURM_NNODES * SLURM_NTASKS_PER_NODE)) \
    ./x86_64/special -mpi -python ringtest.py \
    -tstop 10 -nring 128 -ncell 128 -branch 32 64 \
    -coreneuron -gpu

```

## Installation of DeepDendrite 

```{bash}

# Docker is here
# https://deepdendrite.readthedocs.io/
# docker run --gpus '"device=0"' -itd --user $(id -u):$(id -g) --volume $(pwd):/workdir deepdendrite_docker:1.0 

# Manual installation 

DIR="/home/olive/DeepDendrite"

# Check if the directory exists
if [ -d "$DIR" ]; then 
    rm -rf "$DIR"
    echo "Directory removed: $DIR"  
fi
    
echo "Cloing in to : $DIR" 
git clone https://github.com/pkuzyc/DeepDendrite.git
cd $DIR/src/nrn_modify 
 
#chmod +x configure


# you may set the variables before this command as well

# before configuration

# make sure that the 
# AM_INIT_AUTOMAKE([foreign]) in configure.ac to AM_INIT_AUTOMAKE([1.16 foreign])
# if your systerm automake version is 1.16
# check it via automake --version


echo "######################################################"
echo "Configure"
echo "######################################################" 
  
  
# Locate the python library flags
#python3-config --ldflags
#python3-config --libs

# the flags during configure should be inaccordance with your system ldflags and libs. Check them via the above command

./configure --prefix ~/DeepDendrite/install --without-iv --with-paranrn --with-nrnpython=`which python3` PYLIB="-L/usr/lib/python3.12/config-3.12-x86_64-linux-gnu -lpython3.12" PYLIBDIR="/usr/lib/x86_64-linux-gnu" PYLIBLINK="-L/usr/lib/python3.12/config-3.12-x86_64-linux-gnu -lpython3.12"

autoreconf -f -i  # this will correct for any version mismatch during the configure

# This will make 
make -j8  

export PYTHONPATH=$PYTHONPATH:/home/olive/DeepDendrite/install/lib/python

make install 

# After installtion start running the Figure 5 and Figure 4 files
```
