{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NEURON simulator\n",
        "\n",
        "[NEURON simulator](https://nrn.readthedocs.io/en/8.2.6/) excels at simulating complex, multi-compartmental models with detailed biophysics. The NEURON simulator is widely used in the simulation of detailed neural mechanisms and networks of neurons in the neuroscience literature. The original programming language used to code the NEURON is Higher Order Calculator (HOC). NEURON may also be programmed in Python, which we will explore more in this series of writings.\n",
        "\n",
        "## Installation of NEURON with GPU support (CoreNEURON)\n",
        "\n",
        "### 1. Clone the latest version of NEURON\n",
        "\n",
        "\n",
        "```{bash}\n",
        "git clone https://github.com/neuronsimulator/nrn\n",
        "cd nrn\n",
        "```\n",
        "\n",
        "\n",
        "### 2. Build\n",
        "\n",
        "\n",
        "```{bash}\n",
        "mkdir build\n",
        "cd build\n",
        "```\n",
        "\n",
        "\n",
        "### 3. Load the modules\n",
        "\n",
        "In normal Linux systems, the modules are in the path `/usr/share/modules/modulefiles`. If the directory does not exists, then install using\n",
        "\n",
        "\n",
        "```{bash}\n",
        "\n",
        "sudo apt-get install environment-modules\n",
        "\n",
        "# and configure the ~/.bashrc with \n",
        "source /etc/profile.d/modules.sh\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Load the following modules\n",
        "\n",
        "\n",
        "```{bash}\n",
        "module load  openmpi python cmake nvidia-hpc-sdk/25.1 cuda  \n",
        "```\n",
        "\n",
        "\n",
        "If they are not loading make sure the following files are available in `/usr/share/modules/modulefiles`\n",
        "\n",
        "1.  openmpi\n",
        "2.  python\n",
        "3.  cmake\n",
        "4.  nvidia-hpc-sdk/25.1\n",
        "5.  cuda\n",
        "\n",
        "For example, the content of the file in nvidia-hpc-sdk/25.1 should read as\n",
        "\n",
        "\n",
        "```{bash}\n",
        "#%Module1.0 \n",
        "\n",
        "proc ModulesHelp { } { puts stderr \"NVIDIA HPC SDK 25.1\" } \n",
        "module-whatis \"NVIDIA HPC SDK 25.1\"\n",
        "\n",
        "set HPC_SDK /opt/nvidia/hpc_sdk/Linux_x86_64/25.1 \n",
        "prepend-path PATH $HPC_SDK/compilers/bin \n",
        "prepend-path LD_LIBRARY_PATH $HPC_SDK/compilers/lib\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "#%Module1.0 proc ModulesHelp { } { puts stderr \"NVIDIA HPC SDK 25.1\" } module-whatis \"NVIDIA HPC SDK 25.1\"\n",
        "\n",
        "set HPC_SDK /opt/nvidia/hpc_sdk/Linux_x86_64/25.1 prepend-path PATH \\$HPC_SDK/compilers/bin prepend-path LD_LIBRARY_PATH \\$HPC_SDK/compilers/lib\n",
        "\n",
        "### 4. Compile\n",
        "\n",
        "Following compilation is `for GPU architecture of 61`\n",
        "\n",
        "\n",
        "```{bash}\n",
        "cmake .. \\\n",
        "      -DNRN_ENABLE_CORENEURON=ON \\\n",
        "      -DCORENRN_ENABLE_GPU=ON \\\n",
        "      -DNRN_ENABLE_INTERVIEWS=OFF \\\n",
        "      -DNRN_ENABLE_RX3D=OFF \\\n",
        "      -DCMAKE_INSTALL_PREFIX=$HOME/install \\\n",
        "      -DCMAKE_C_COMPILER=nvc \\\n",
        "      -DCMAKE_CXX_COMPILER=nvc++ \\\n",
        "      -DCMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu \\\n",
        "      -DCMAKE_CUDA_ARCHITECTURES=\"61\" \\  # need to know this\n",
        "      -DCMAKE_EXE_LINKER_FLAGS: \"-cuda -gpu=cuda12.6,lineinfo,cc61 -acc\" # not necessary\n",
        "      -DCMAKE_CXX_FLAGS=\"-O3 -g\" \\\n",
        "      -DCMAKE_C_FLAGS=\"-O3 -g\" \\\n",
        "      -DCMAKE_BUILD_TYPE=Custom\n",
        "```\n",
        "\n",
        "\n",
        "Check the CUDA_ARCHITECTURES via (nvidia-smi; assuming it is installed)\n",
        "\n",
        "\n",
        "```{bash}\n",
        "nvidia-smi --query-gpu=compute_cap --format=csv\n",
        "```\n",
        "\n",
        "\n",
        "### 7. Install (make)\n",
        "\n",
        "\n",
        "```{bash}\n",
        "\n",
        "make -j \n",
        "make install\n",
        "```\n",
        "\n",
        "\n",
        "### 8. Export Python PATH variables to \\~/.bashrc\n",
        "\n",
        "\n",
        "```{bash}\n",
        "  \n",
        "export PATH=$HOME/install/bin:$PATH  # assuming $HOME/install is the cmake installtion directory (see 5-th line of cmake in Step 4.)\n",
        "\n",
        "export PYTHONPATH=$HOME/install/lib/python:$PYTHONPATH\n",
        "```\n",
        "\n",
        "\n",
        "### 9. Ringtest\n",
        "\n",
        "Best way to check if the installation is working properly is by running the ring test as in\n",
        "\n",
        "\n",
        "```{bash}\n",
        "git clone https://github.com/nrnhines/ringtest.git\n",
        "cd ringtest\n",
        "\n",
        "nrnivmodl -coreneuron mod\n",
        "\n",
        "# in any NEURON code add the following lines for CoreNEURON support (python files only)\n",
        "\n",
        "h.cvode.cache_efficient(1)\n",
        "if use_coreneuron:\n",
        "    from neuron import coreneuron\n",
        "    coreneuron.enable = True\n",
        "    coreneuron.gpu = coreneuron_gpu\n",
        "    \n",
        "    \n",
        "#run the three performance test \n",
        "\n",
        "# NEURON CPU Run\n",
        "mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64\n",
        "\n",
        "# CoreNEURON CPU Run\n",
        "mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron\n",
        "\n",
        "# CoreNEURON GPU Run\n",
        "mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron -gpu\n",
        "```\n",
        "\n",
        "\n",
        "# \n",
        "\n",
        "## Batch files for running in cpu\n",
        "\n",
        "\n",
        "```{bash}\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=rt_cpu       # Job name\n",
        "#SBATCH --output=rt_cpu.out     # Standard output and error log\n",
        "#SBATCH --error=rt_cpu.err      # Error log file                                                                                                     \n",
        "#SBATCH --ntasks=1                # Number of MPI processes\n",
        "#SBATCH --time=00:20:00           # Time limit (HH:MM:SS)\n",
        "#SBATCH --partition=olaf_c_core       # Partition name (adjust as needed)\n",
        "\n",
        "module purge\n",
        "module load gcc/12.2.0 openmpi/4.1.1-Rocky python/.3.12.3\n",
        "\n",
        "module load /opt/ibs_lib/modulefiles/libraries/.cuda/25.1\n",
        "module load /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1\n",
        "\n",
        "# Run the MPI program\n",
        "\n",
        "#mpiexec -n 2 ./x86_64/special -mpi -python ringtest.py -tstop 100\n",
        "#mpiexec -n 2 ./x86_64/special -mpi -python ringtest.py -tstop 100 -coreneuron\n",
        "\n",
        "# NEURON CPU Run ( in partition olaf_c_core ) \n",
        "#mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64\n",
        "\n",
        "# CoreNEURON CPU Run ( in partition olaf_c_core ) \n",
        "mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron\n",
        "```\n",
        "\n",
        "\n",
        "## Batch files for running in GPU\n",
        "\n",
        "\n",
        "```{bash}\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=ringtest_gpu\n",
        "#SBATCH --output=ringtest_gpu.out\n",
        "#SBATCH --error=ringtest_gpu.err\n",
        "#SBATCH --partition=AIP\n",
        "#SBATCH --gpus-per-node=1\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --time=01:00:00\n",
        "\n",
        "# Load necessary modules (adjust as needed for your system)\n",
        "module purge\n",
        "module load gcc/12.2.0 openmpi/4.1.1-Rocky python/.3.12.3\n",
        "\n",
        "module load /opt/ibs_lib/modulefiles/libraries/.cuda/25.1\n",
        "module load /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1\n",
        "\n",
        "# Run the CoreNEURON simulation\n",
        "mpiexec -n 1 ./x86_64/special -mpi -python ringtest.py -tstop 10 -nring 128 -ncell 128 -branch 32 64 -coreneuron -gpu\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "## Batch files for running in multi-node GPU\n",
        "\n",
        "\n",
        "```{bash}\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=ringtest_multi_gpu\n",
        "#SBATCH --output=ringtest_multi_gpu_%j.out\n",
        "#SBATCH --error=ringtest_multi_gpu_%j.err\n",
        "#SBATCH --partition=AIP\n",
        "#SBATCH --nodes=2               # Number of nodes\n",
        "#SBATCH --gpus-per-node=2       # GPUs per node (adjust based on node GPU capacity)\n",
        "#SBATCH --ntasks-per-node=2     # MPI tasks per node (matches GPUs-per-node)\n",
        "#SBATCH --time=01:00:00\n",
        "#SBATCH --exclusive             # Request exclusive node access\n",
        "\n",
        "# Load modules (confirm paths match your cluster's setup)\n",
        "module purge\n",
        "module load gcc/12.2.0 \\\n",
        "            openmpi/4.1.1-Rocky \\\n",
        "            python/3.12.3 \\\n",
        "            /opt/ibs_lib/modulefiles/libraries/cuda/25.1 \\\n",
        "            /opt/ibs_lib/apps/nvhpc/25.1/modulefiles/nvhpc/25.1\n",
        "\n",
        "# Ensure CUDA and MPI are visible\n",
        "export CUDA_VISIBLE_DEVICES=0,1  # Map GPUs per node (adjust if needed)\n",
        "export OMP_NUM_THREADS=1         # Disable threading unless required\n",
        "\n",
        "# Run with MPI across nodes\n",
        "mpiexec --bind-to none --map-by node \\\n",
        "    -n $((SLURM_NNODES * SLURM_NTASKS_PER_NODE)) \\\n",
        "    ./x86_64/special -mpi -python ringtest.py \\\n",
        "    -tstop 10 -nring 128 -ncell 128 -branch 32 64 \\\n",
        "    -coreneuron -gpu\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "## Installation of DeepDendrite\n",
        "\n",
        "\n",
        "```{bash}\n",
        "\n",
        "# Docker is here\n",
        "# https://deepdendrite.readthedocs.io/\n",
        "# docker run --gpus '\"device=0\"' -itd --user $(id -u):$(id -g) --volume $(pwd):/workdir deepdendrite_docker:1.0 \n",
        "# If you use the Docker version, please change the settings inside the Docker file such as ARG HPCSDK_VER=\"25.1\", ARG CUDA_VER=\"12.6\", ARG UBUNTU_VER=\"24.04\" according to your sister version\n",
        "# coredat folder is the data folder (gen_data.py for Fig5)\n",
        "# If it is generated already, you can simply run the run.py inside the Fig5 folder\n",
        "\n",
        "\n",
        "\n",
        "# Manual installation \n",
        "\n",
        "DIR=\"/home/olive/DeepDendrite\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if [ -d \"$DIR\" ]; then \n",
        "    rm -rf \"$DIR\"\n",
        "    echo \"Directory removed: $DIR\"  \n",
        "fi\n",
        "    \n",
        "echo \"Cloing in to : $DIR\" \n",
        "git clone https://github.com/pkuzyc/DeepDendrite.git\n",
        "cd $DIR/src/nrn_modify \n",
        " \n",
        "#chmod +x configure\n",
        "\n",
        "\n",
        "# you may set the variables before this command as well\n",
        "\n",
        "# before configuration\n",
        "\n",
        "# make sure that the \n",
        "# AM_INIT_AUTOMAKE([foreign]) in configure.ac to AM_INIT_AUTOMAKE([1.16 foreign])\n",
        "# if your systerm automake version is 1.16\n",
        "# check it via automake --version\n",
        "\n",
        "\n",
        "echo \"######################################################\"\n",
        "echo \"Configure\"\n",
        "echo \"######################################################\" \n",
        "  \n",
        "  \n",
        "# Locate the python library flags\n",
        "#python3-config --ldflags\n",
        "#python3-config --libs\n",
        "\n",
        "# the flags during configure should be inaccordance with your system ldflags and libs. Check them via the above command\n",
        "\n",
        "./configure --prefix ~/DeepDendrite/install --without-iv --with-paranrn --with-nrnpython=`which python3` PYLIB=\"-L/usr/lib/python3.12/config-3.12-x86_64-linux-gnu -lpython3.12\" PYLIBDIR=\"/usr/lib/x86_64-linux-gnu\" PYLIBLINK=\"-L/usr/lib/python3.12/config-3.12-x86_64-linux-gnu -lpython3.12\"\n",
        "\n",
        "autoreconf -f -i  # this will correct for any version mismatch during the configure\n",
        "\n",
        "# This will make \n",
        "make -j8  \n",
        "\n",
        "export PYTHONPATH=$PYTHONPATH:/home/olive/DeepDendrite/install/lib/python\n",
        "\n",
        "make install \n",
        "\n",
        "# After installtion start running the Figure 5 and Figure 4 files\n",
        "```"
      ],
<<<<<<< HEAD
      "id": "18bb114f"
=======
      "id": "1896c8cf"
>>>>>>> 1df55c04ab661655d4e745b87d264ffe42d6adf9
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}