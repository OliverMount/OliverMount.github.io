# Auto-regressive moving average (ARMA) processess

Let $x(n)$ and $v(n)$ are stationary random processes. If they are related by the linear constant coefficient differential equations

$$
x(n) + \sum_{l=1}^{p} a(l) x(n-l)  = \sum_{l=0}^q b(l)v(n-l)
$$

then $x(n)$ is called an ARMA(p,q) process.

#### Autocorrelation

On both sides, multiplying by $x^*(n-k)$ and taking the expectation $\mathbb{E}$ leads to

$$
\begin{split}
\mathbb{E}[x(n)x^*(n-k)] + \sum_{l=1}^{p} a(l) \mathbb{E}[x(n-l) x^*(n-k)] & = \sum_{l=0}^q b(l)\mathbb{E}[v(n-l)x^*(n-k)] \\
r_x(k) + \sum_{l=1}^{p} a(l) r_x(k-l) & = \sum_{l=0}^q b(l)r_{vx}(k-l)
\end{split}
$$ {#eq-arma_original}

In the present form, @eq-arma_original is not useful. It would be nice to relate the cross-correlation term $r_{vx}(k)$ on the RHS of @eq-arma_original to the autocorrelation term $r_x(k)$ by invoking the linear filter theory (that is convolution). Since

$$
x(n) = \sum_{m={-\infty}}^{\infty} v(m) h^*(n-m) 
$$

where $h(n)$ is the impulse response of LTI system.

$$
\begin{split}
r_{vx}(k) &= \mathbb{E}[v(n)x^*(n-k)]  \\ 
& =  \sum_{m=-\infty}^{\infty} \mathbb{E}[v(n)v^*(m)  h^*(n-m-k)] \\
  & = \sigma_v^2 h^*(-k) 
\end{split}
$$

The above simplification is due to the assumption that $v(n)$ is white noise. Now,

$$ 
r_{vx}(k-l) =  \sigma_v^2 h^*(l-k)  
$$ {#eq-rvx}

Substituting @eq-rvx in to @eq-arma_original

$$
\begin{split}
 r_x(k) + \sum_{l=1}^{p} a(l) r_x(k-l) & = \sum_{l=0}^q b(l)r_{vx}(k-l) \\
& = \sigma_v^2\sum_{l=0}^q b(l)  h^*(l-k) \\
& = \sigma_v^2\sum_{l=0}^{q-k} b(l+k)  h^*(l)
\end{split}
$$

The last step is due to the assumption of causal $h(n)$.

The equation

$$ 
 r_x(k) + \sum_{l=1}^{p} a(l) r_x(k-l)  = \sigma_v^2\sum_{l=0}^{q-k} b(l+k)  h^*(l) 
$$

is called `Yule-Walker` equation. Let the RHS be

$$  
C_q(k)  = \sigma_v^2\sum_{l=0}^{q-k} b(l+k)  h^*(l)  
$$

$$
r_x(k) + \sum_{l=1}^{p} a(l) r_x(k-l) =\left\{ \begin{array}{ c l }
     C_q(k) & k  \leq q \\
    0                 &   k >q
  \end{array}\right.
$$

The above YW equations can be written (for $k=0,\cdots,p+q$) as

$$
\begin{split}
r_x(0) +   a(1) r_x(-1)  + a(2) r_x(-2) + \cdots + a(p) r_x(-p) &= C_q(0)\\ 
r_x(1) +   a(1) r_x(0)  + a(2) r_x(-1) + \cdots + a(p) r_x(-(p-1)) &= C_q(1)\\ 
\vdots ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\
r_x(q) +   a(1) r_x(q-1)  + a(2) r_x(q-2) + \cdots + a(p) r_x(-(p-q)) &= C_q(q)\\
---------------------------\\
r_x(q+1) +   a(1) r_x(q)  + a(2) r_x(q-1) + \cdots + a(p) r_x(-(p-(q+1)) &= 0\\
r_x(q+2) +   a(1) r_x(q+1)  + a(2) r_x(q) + \cdots + a(p) r_x(-(p-(q+2))) &= 0\\
\vdots ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\
r_x(q+p) +   a(1) r_x(q+p-1)  + a(2) r_x(q-1) + \cdots + a(p) r_x(q) &= 0\\
\end{split} 
$$

In matrix form,

$$
\begin{bmatrix}
r_x(0) &     r_x(-1)  &   r_x(-2) & \cdots &  r_x(-p)  \\ 
r_x(1) &     r_x(0)  &    r_x(-1) &  \cdots &  r_x(-(p-1)) \\  
\vdots &     \ddots  &    \ddots &  \vdots &  \vdots  \\   
r_x(q) &    r_x(q-1)  &  r_x(q-2) & \cdots & r_x(-(p-q)) \\ 
---- &     ----   &    ----  &  ----  & ----   \\   
r_x(q+1) &    r_x(q)  &  r_x(q-1) & \cdots &   r_x(-(p-(q+1)) \\
r_x(q+2) &  r_x(q+1)  & r_x(q) & \cdots &   r_x(-(p-(q+2)))  \\ 
\vdots &     \ddots  &    \ddots &  \vdots &  \vdots  \\   
r_x(q+p) &    r_x(q+p-1)  & r_x(q-1) & \cdots &  r_x(q)  
\end{bmatrix}
\begin{bmatrix}  1 \\ a(1) \\ a(2)  \\ \vdots \\ a(p) \\
\end{bmatrix}
= \sigma_v^2 \begin{bmatrix}  C_q(0) \\ C_q(1) \\ C_q(2)  \\ \vdots \\ C_q(q)\\ ---- \\ 0 \\ 0 \\ \vdots \\ 0 
\end{bmatrix}
$$

The YW equations can be used

1.  To find the (AR) filter coefficients $a(i)$ given autocorrelation values.

2.  Recurrence relationship for autocorrelation sequence for extrapolation.

    YW equations can be used to extrapolate the

### Recurrence for autocorrelation sequence for extrapolation

From

$$
r_x(k) + \sum_{l=1}^{p} a(l) r_x(k-l) =\left\{ \begin{array}{ c l }
     C_q(k) & k  \leq q \\
    0                 &   k >q
  \end{array}\right.
$$

for $k>q$,

$$
r_x(k) =- \sum_{l=1}^{p} a(l) r_x(k-l)   
$$

For example, in the case of ARMA(1,1) process where $p=q=1$, we have for $k>1$

$$
r_x(k) =-  a(1) r_x(k-1)   
$$

If $r_x(0)$ and $r_x(1)$ are known, then the $r_x(k), \forall k>1$ can be obtained from the above recurrence.

For example, in the case of ARMA(3,1) process where $p=3,q=1$, we have for $k>1$

$$
r_x(k) =-  a(1) r_x(k-1)   -  a(2) r_x(k-2) -  a(1) r_x(k-3) 
$$

If $r_x(0),r_x(1)$ are known, then the $r_x(k), \forall k>1$ can be obtained from the above recurrence.

For example, in the case of ARMA(1,3) process where $p=1,q=3$, we have for $k>3$

$$
r_x(k) =-  a(1) r_x(k-1)     
$$

If $r_x(0),r_x(1),r_x(2),r_x(3)$ are known, then the $r_x(k), \forall k>3$ can be obtained from the above recurrence.

Also, if $p \geq q$ and if $r_x(0), \cdots, r_x(p-1)$ are known, the for any $k \geq p$, one can obtain $$
r_x(k) =- \sum_{l=1}^{p} a(l) r_x(k-l)   
$$
