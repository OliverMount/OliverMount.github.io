# Auto-regressive moving average (ARMA) processess

Let $x(n)$ and $v(n)$ are stationary random processes. If they are related by the linear constant coefficient differential equations

$$
x(n) + \sum_{l=1}^{p} a(l) x(n-l)  = \sum_{l=0}^q b(l)v(n-l)
$$

then $x(n)$ is called an ARMA(p,q) process.

#### Autocorrelation

On both sides, multiplying by $x^*(n-k)$ and taking the expectation $\mathbb{E}$ leads to

$$
\begin{split}
\mathbb{E}[x(n)x^*(n-k)] + \sum_{l=1}^{p} a(l) \mathbb{E}[x(n-l) x^*(n-k)] & = \sum_{l=0}^q b(l)\mathbb{E}[v(n-l)x^*(n-k)] \\
r_x(k) + \sum_{l=1}^{p} a(l) r_x(k-l) & = \sum_{l=0}^q b(l)r_{vx}(k-l)
\end{split}
$$ {#eq-arma_original}

In the present form, @eq-arma_original is not useful. It would be nice to relate the cross-correlation term $r_{vx}(k)$ on the RHS of @eq-arma_original to the autocorrelation term $r_x(k)$ by invoking the linear filter theory. Since

$$
x(n) = \sum_{m={-\infty}}^{\infty} v(m) h^*(n-m) 
$$

$$
\begin{split}
r_{vx}(k) &= \mathbb{E}[v(n)x^*(n-k)]  \\ 
& =  \sum_{m=-\infty}^{\infty} \mathbb{E}[v(n)v^*(m)  h^*(n-m-k)] \\
  & = \sigma_v^2 h^*(-k) 
\end{split}
$$

The above simplification is due to the assumption that $v(n)$ is white noise. Now,

$$ 
r_{vx}(k-l) =  \sigma_v^2 h^*(l-k)  
$$ {#eq-rvx}

Substituting @eq-rvx in to @eq-arma_original

$$
\begin{split}
 r_x(k) + \sum_{l=1}^{p} a(l) r_x(k-l) & = \sum_{l=0}^q b(l)r_{vx}(k-l) \\
& = \sigma_v^2\sum_{l=0}^q b(l)  h^*(l-k) \\
& = \sigma_v^2\sum_{l=0}^{q-k} b(l+k)  h^*(l)
\end{split}
$$

The last step is due to the assumption of causal $h(n)$.

The equation

$$ 
 r_x(k) + \sum_{l=1}^{p} a(l) r_x(k-l)  = \sigma_v^2\sum_{l=0}^{q-k} b(l+k)  h^*(l) 
$$

is called `Yule-Walker` equation.
